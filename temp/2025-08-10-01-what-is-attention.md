---
layout: post 
title: 어텐션을 이해해보자
subtitle: 작업중
categories: AI
tags: AI LLM 학습 생각정리
thumb: /assets/images/posts/2025-08/2025-08-06-007.png
custom-excerpt: 작업중
banner:
  # video: https://vjs.zencdn.net/v/oceans.mp4
  video: https://cdn.pixabay.com/video/2019/10/04/27539-364430966_large.mp4
  loop: true
  volume: 0.8
  muted: true                 # For mobile device background music play 
  start_at: 8.5
  image: https://wallpapers.com/images/featured-full/running-wl9pg3zeygysq0ps.jpg
  opacity: 0.618
  background: "#000"
  height: "100vh"
  min_height: "38vh"
  heading_style: "font-size: 4.25em; font-weight: bold; text-decoration: underline"
  subheading_style: "color: gold"
---

## 영상 보기
[![비디오 제목](https://i.ytimg.com/vi/_Z3rXeJahMs/hq720.jpg)](https://www.youtube.com/watch?v=_Z3rXeJahMs)

## 요약
본 영상은 **트랜스포머(Transformer) 모델의 핵심인 어텐션(Attention) 메커니즘**이 어떻게 동작하는지에 대한 상세한 설명이다. 최대한 문돌이도 해결 가능하도록 만들어보고 싶었다.... 물론 불가능할 것 같지만..? 그럼에도 열심히 이해해보았다.

*   **트랜스포머의 목표**: 텍스트를 입력으로 받아 다음에 올 단어를 예측하는 모델이며, 입력된 텍스트는 **토큰이라는 조각들로 나뉘고, 이 토큰들은 고차원 벡터인 인베딩으로 변환된다**. 이 인베딩은 단어의 의미를 담고 있으며, 트랜스포머의 목표는 이러한 인베딩을 여러 단계를 거쳐 **단순한 단어 인코딩을 넘어선 풍부한 문맥적 의미를 알아내는 것**이다. 어텐션 메커니즘은 이 문제를 해결한다.
    *   예를 들어, 'mole'이라는 단어가 문맥에 따라 두더지, 화학 분자 단위, 점 등 다른 의미를 지닐 때, 어텐션은 이 단어의 인베딩이 **문맥에 맞게 정확한 의미 방향으로 업데이트되도록 돕는다**. 또한, 한 인베딩에 담긴 정보를 **멀리 떨어져 있는 다른 인베딩으로도 옮길 수 있게** 한다.

*   **어텐션의 세 가지 핵심 요소: 쿼리(Query), 키(Key), 밸류(Value)**: 어텐션은 명사가 주변 형용사에 의해 의미가 바뀌는 것과 같이 **단어 간의 관계를 파악하고 정보를 업데이트**하는 방식으로 작동한다.
    *   **쿼리(Query, Q)**: **"질문"의 역할을 하는 벡터**이다. 특정 단어(예: 'creature')를 설명해 주는 단어가 무엇인지 묻는 역할을 한다. 인베딩 벡터에 WQ라는 훈련 가능한 가중치 행렬을 곱하여 계산된다.
    *   **키(Key, K)**: **"질문에 대한 답을 찾는 도구" 역할을 하는 벡터**이다. 모든 인베딩 토큰에 WK라는 훈련 가능한 가중치 행렬을 곱하여 계산되며, 쿼리와 동일한 차원(예: 128차원)의 공간으로 매핑된다. 쿼리 벡터와 키 벡터가 **유사할 때 일치한다고 판단된다**.
    *   **내적 연산 및 어텐션 맵**: 각 **키와 쿼리의 쌍에 대해 내적을 수행하여 얼마나 잘 맞는지 측정한다**. 내적 값이 클수록 관련성이 높음을 의미하며, 해당 단어(예: 'fluffy'와 'blue'가 'creature'에 주목)가 다른 단어를 "주목(attend)"한다고 표현한다. 이 내적 값들은 음의 무한대에서 무한대까지의 실수값을 가질 수 있으며, 이를 **소프트맥스(Softmax) 함수를 통해 0에서 1 사이의 확률 분포처럼 정규화하여 '어텐션 맵' 또는 '어텐션 패턴'을 생성한다**. 이 어텐션 맵은 한 단어가 다른 단어에 대해 얼마나 업데이트될지(가중치)를 나타낸다.
    *   **밸류(Value, V)**: 세 번째 핵심 행렬이다. 인베딩에 WV라는 훈련 가능한 가중치 행렬을 곱하여 계산되며, 이는 **다른 인베딩이 현재 인베딩과 관련이 있을 때 어떤 값을 더해줘야 의미가 더해질지 알려주는 역할**을 한다. 어텐션 맵에서 계산된 가중치에 따라 **밸류 벡터들을 가중 합산하여 '변화량(델타 E)'을 만들고, 이 변화량을 원래 인베딩에 더하여 문맥적으로 업데이트된 새로운 인베딩을 생성한다**.

*   **마스킹(Masking)**: 모델 훈련 시, **뒤쪽에 오는 단어가 앞에 있는 단어에 영향을 주지 않도록 하는 기법**이다. 소프트맥스 적용 전 관련 없는 값들을 음의 무한대로 설정하여, 소프트맥스 이후 해당 값들이 0이 되도록 한다. GPT 훈련 시 항상 사용되는 중요한 개념이다.

*   **셀프 어텐션(Self-Attention) vs. 크로스 어텐션(Cross-Attention)**:
    *   위에서 설명된 것은 **셀프 어텐션**으로, 단일 데이터 내에서 단어 간의 관계를 파악한다.
    *   **크로스 어텐션**은 두 가지 다른 데이터 타입(예: 번역, 음성-텍스트)을 처리할 때 사용되며, 키와 쿼리를 두 개의 다른 데이터 사이에서 곱한다는 점에서 다르다. 이때는 마스킹을 사용하지 않는다.

*   **멀티 헤드 어텐션(Multi-Head Attention)**: 대부분의 트랜스포머 모델에서 사용된다. **여러 개의 어텐션 작업을 병렬적으로 동시에 수행하는 것**이다.
    *   예를 들어 GPT-3에는 **96개의 어텐션 헤드가 사용된다**. 이는 **96개의 서로 다른 쿼리, 키, 밸류 행렬이 존재하며, 각각 고유한 어텐션 패턴과 밸류 벡터 시퀀스를 생성한다는 의미**이다.
    *   각 헤드는 인베딩 벡터가 어떻게 변해야 하는지 **각각 다른 방식으로 제안하고, 이 제안된 방향들을 모두 더하여 원래 인베딩에 반영**한다. 이는 모델이 문맥에 따라 의미가 달라지는 다양한 방식들을 학습할 수 있게 한다.
    *   GPT-3의 경우, 96개의 멀티 헤드로 인해 어텐션 블록에만 **약 580억 개의 파라미터가 포함**된다.

*   **트랜스포머의 전체 구조**: 어텐션 블록 뒤에는 **멀티레이어 퍼셉트론(MLP)이 하나 더 존재**하며, 이 두 레이어가 반복적으로 실행된다. 이를 통해 단어 인베딩은 **주변 인베딩에 의해 계속해서 조정되고, 점점 더 많은 문맥적 의미를 담게 되며 고차원적이고 추상적인 의미를 인코딩**할 수 있게 된다.

*   **어텐션의 중요성**: 어텐션이 유명해진 가장 큰 이유는 **GPU를 사용하여 짧은 시간 안에 많은 계산을 병렬적으로 수행할 수 있었다는 점**이다. 이는 모델 사이즈를 키울수록 성능이 크게 향상되는 AI 분야의 흐름에 맞춰, **병렬 계산이 가능한 네트워크 구조가 필요했기 때문**이다.

## 내 생각 정리
